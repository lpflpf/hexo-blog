{"meta":{"title":null,"subtitle":null,"description":null,"author":"lpflpf","url":"http://blog.lpflpf.cn","root":"/"},"pages":[{"title":"搬砖程序员","date":"2020-04-06T01:49:47.000Z","updated":"2021-01-19T06:50:46.014Z","comments":true,"path":"about/index.html","permalink":"http://blog.lpflpf.cn/about/index.html","excerpt":"","text":"互联网界的小学生，保留着积极向上的心。乐于分享一些实践中遇到的小问题，小知识，算是学习也算是总结。不断搬砖，积极盖楼。 欢迎关注个人公众号："},{"title":"常用手册","date":"2021-01-19T01:49:47.000Z","updated":"2021-01-20T02:22:17.001Z","comments":true,"path":"about/manual.html","permalink":"http://blog.lpflpf.cn/about/manual.html","excerpt":"","text":"[golang]"}],"posts":[{"title":"Golang DB Query 的优化之旅","slug":"go-database-query-optimization","date":"2021-09-30T05:57:59.000Z","updated":"2021-10-11T01:21:06.767Z","comments":true,"path":"passages/go-database-query-optimization/","link":"","permalink":"http://blog.lpflpf.cn/passages/go-database-query-optimization/","excerpt":"记录一次 Golang 数据库查询组件的优化。","text":"记录一次 Golang 数据库查询组件的优化。 背景介绍线上有一块业务，需要做大量的数据库查询以及编码落盘的任务。数据库查询20分钟左右，大约有2kw条sql被执行。如果可以优化数据库查询的方法，可以节省一笔很大的开销。 由于代码比较久远，未能考证当时的数据查询选型为什么不适用orm，而是使用原生的方式自己构建。下面是核心的数据查询代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465func QueryHelperOne(db *sql.DB, result interface&#123;&#125;, query string, args ...interface&#123;&#125;) (err error) &#123; // 数据库查询 var rows *sql.Rows log.Debug(query, args) rows, err = db.Query(query, args...) if err != nil &#123; return err &#125; defer rows.Close() // 获取列名称，并转换首字母大写，用于和struct Field 匹配 var columns []string columns, err = rows.Columns() if err != nil &#123; return err &#125; fields := make([]string, len(columns)) for i, columnName := range columns &#123; fields[i] = server.firstCharToUpper(columnName) &#125; // 传参必须是数组 slice 指针 rv := reflect.ValueOf(result) if rv.Kind() == reflect.Ptr &#123; rv = rv.Elem() &#125; else &#123; return errors.New(\"Parameter result must be a slice pointer\") &#125; if rv.Kind() == reflect.Slice &#123; elemType := rv.Type().Elem() if elemType.Kind() == reflect.Struct &#123; ev := reflect.New(elemType) // 申请slice 数据，之后赋值给result nv := reflect.MakeSlice(rv.Type(), 0, 0) ignoreData := make([][]byte, len(columns)) for rows.Next() &#123; // for each rows // scanArgs 是扫描每行数据的参数 // scanArgs 中存储的是 struct 中field 的指针 scanArgs := make([]interface&#123;&#125;, len(fields)) for i, fieldName := range fields &#123; fv := ev.Elem().FieldByName(fieldName) if fv.Kind() != reflect.Invalid &#123; scanArgs[i] = fv.Addr().Interface() &#125; else &#123; ignoreData[i] = []byte&#123;&#125; scanArgs[i] = &amp;ignoreData[i] &#125; &#125; err = rows.Scan(scanArgs...) if err != nil &#123; return err &#125; nv = reflect.Append(nv, ev.Elem()) &#125; rv.Set(nv) &#125; &#125; else &#123; return errors.New(\"Parameter result must be a slice pointer\") &#125; return&#125; 方法通过如下方式调用： 123456789type TblUser struct &#123; Id int64 Name string Addr string UpdateTime string&#125;result := []TblUser&#123;&#125;QueryHelperOne(db, &amp;result, query, 10) 逐步优化直接看上面的代码，发现没有什么大的问题，但是从细节上不断调优，可以让性能压榨到极致。 网络优化golang 提供的db.Query(sql, args…) 方法，内部的实现，也是基于prepare 方法实现的。prepare 有三个好处： - 可以让 mysql 省去每次语法分析的过程 - 可以避免出现sql 注入 - 可以重复使用prepare 的结果，只发送参数即可做查询但是，也有不好的地方。一次 db.Query 会有三次网络请求。 prepare execute closing 而如果有多次相同SQL 查询的话，这种方式是非常占优的。因此，可以使用prepare 替换 db.Query 减少一次网络消耗。 1234567891011121314151617181920212223242526272829var stmts = sync.Map&#123;&#125;func QueryHelperOne(db *sql.DB, result interface&#123;&#125;, query string, args ...interface&#123;&#125;) (err error) &#123; // 使用sync.Map 缓存 query 对应的stmt // 减少不必要的prepare 请求 var stmt *sql.Stmt if v, ok := stmts.Load(query); ok &#123; stmt = v.(*sql.Stmt) &#125; else &#123; if stmt, err = db.Prepare(query); err != nil &#123; return err &#125; else &#123; stmts.Store(query, stmt) &#125; &#125; var rows *sql.Rows log.Debug(query, args) rows, err = stmt.Query(args...) if err != nil &#123; _ = stmt.Close() stmts.Delete(query) return err &#125; defer rows.Close() // 后面代码省略 ...&#125; 通过此番修改，作业的性能提升了17%，效果还是非常明显的。 gc 优化优化1在服务中，会预申请slice空间，因此无需每次构建的时候重新申请slice 内存。 12345// old code// nv := reflect.MakeSlice(rv.Type(), 0, 0) // new codenv := rv.Slice(0, 0) 优化2从代码56 行可以看到，每次会append 数据到数组中。由于 结构体切片在append 时，是做内存拷贝；scanArgs 的数据由于每次scan 都会覆盖，因此可以复用，不需要每次rows 的时候映射。 123456789101112131415161718192021222324ev := reflect.New(elemType) // 申请slice 数据，之后赋值给resultnv := reflect.MakeSlice(rv.Type(), 0, 0) ignoreData := make([][]byte, len(columns))// scanArgs 是扫描每行数据的参数// scanArgs 中存储的是 struct 中field 的指针scanArgs := make([]interface&#123;&#125;, len(fields))for i, fieldName := range fields &#123; fv := ev.Elem().FieldByName(fieldName) if fv.Kind() != reflect.Invalid &#123; scanArgs[i] = fv.Addr().Interface() &#125; else &#123; ignoreData[i] = []byte&#123;&#125; scanArgs[i] = &amp;ignoreData[i] &#125;&#125;for rows.Next() &#123; // for each rows err = rows.Scan(scanArgs...) if err != nil &#123; return err &#125; nv = reflect.Append(nv, ev.Elem())&#125;rv.Set(nv) 减少了每行扫描的时候，新申请scanArgs 优化 3对于不在field中的数据，需要使用一个空的值代替，上面代码使用的是一个[]byte 的切片，其实只需要一个[]byte 即可。代码如下： 123456789101112ignoreData := []byte&#123;&#125;// scanArgs 是扫描每行数据的参数// scanArgs 中存储的是 struct 中field 的指针scanArgs := make([]interface&#123;&#125;, len(fields))for i, fieldName := range fields &#123; fv := ev.Elem().FieldByName(fieldName) if fv.Kind() != reflect.Invalid &#123; scanArgs[i] = fv.Addr().Interface() &#125; else &#123; scanArgs[i] = &amp;ignoreData &#125;&#125; 优化 4由于相同的sql会查询次数在千万级；因此可以把每次扫描行所需要的行元素ev,以及对应的扫描参数列表 scanArgs 都缓存起来，再使用时从内存中加载即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// 定义数据池，用于存储每个sql 对应的扫描行item 以及扫描参数// 全局代码var datapools = sync.Map&#123;&#125;type ReflectItem struct &#123; Item reflect.Value scanArgs []interface&#123;&#125;&#125;///////// 方法调用内部// 从数据池中加载query 对应的 ReflectItemif v, ok := datapools.Load(query); ok &#123; pool = v.(*sync.Pool)&#125; else &#123; // 构建reflectItem var columns []string columns, err = rows.Columns() if err != nil &#123; return err &#125; pool = &amp;sync.Pool&#123; New: func() interface&#123;&#125; &#123; fields := make([]string, len(columns)) for i, columnName := range columns &#123; fields[i] = server.firstCharToUpper(columnName) &#125; ev := reflect.New(elemType) // New slice struct element // nv := reflect.MakeSlice(rv.Type(), 0, 0) // New slice for fill ignored := []byte&#123;&#125; scanArgs := make([]interface&#123;&#125;, len(fields)) for i, fieldName := range fields &#123; fv := ev.Elem().FieldByName(fieldName) if fv.Kind() != reflect.Invalid &#123; scanArgs[i] = fv.Addr().Interface() &#125; else &#123; scanArgs[i] = &amp;ignored &#125; &#125; return ReflectItem&#123; Item: ev, scanArgs: scanArgs, &#125; &#125;, &#125; datapools.Store(query, pool)&#125;ri = pool.Get().(ReflectItem)// 复用 ev 和 scanArgsev = ri.ItemscanArgs = ri.scanArgs// 开始扫描nv := rv.Slice(0, 0)for rows.Next() &#123; // for each rows err = rows.Scan(scanArgs...) if err != nil &#123; return err &#125; nv = reflect.Append(nv, ev.Elem())&#125;rv.Set(nv) // return rows data back to callerpool.Put(ri)// 结束扫描 经过几次优化，24分钟执行完的作业，成功减少到了18分钟。 总结 golang prepare 的实现，需要进一步了解，在使用prepare的情况下，连接是如何复用的，比较困惑。 对于相同query 的情况，但是扫描struct 类型不同的情况，会有问题。扫描参数的数据池，应该使用结构体类型做key。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"database query","slug":"database-query","permalink":"http://blog.lpflpf.cn/tags/database-query/"}]},{"title":"使用原子操作保证并发安全","slug":"use-unsafe-pointer-dynamic-load-data","date":"2021-09-24T01:34:45.000Z","updated":"2021-09-30T08:09:03.450Z","comments":true,"path":"passages/use-unsafe-pointer-dynamic-load-data/","link":"","permalink":"http://blog.lpflpf.cn/passages/use-unsafe-pointer-dynamic-load-data/","excerpt":"本文介绍使用unsafe.Pointer 来提升动态加载数据的效率问题。","text":"本文介绍使用unsafe.Pointer 来提升动态加载数据的效率问题。 场景在很多后台服务中，需要动态加载配置文件或者字典数据。因此在访问这些配置或者字典时，需要给这些数据添加锁，保证并发读写的安全性。正常情况下，需要使用读写锁。下面来看看读写锁的例子。 读写锁加载数据使用读写锁，可以保证访问data 不会出现竞态。 12345678910111213141516171819202122type Config struct &#123; sync.RWMutex data map[string]interface&#123;&#125;&#125;func (c *Config) Load() &#123; c.Lock() defer c.Unlock() c.data = c.load()&#125;func (c *Config) load() map[string]interface&#123;&#125; &#123; // 数据的加载 return make(map[string]interface&#123;&#125;)&#125;func (c *Config) Get() map[string]interface&#123;&#125; &#123; c.RLock() defer c.RUnlock() return c.data&#125; 使用原子操作动态替换数据此类业务需求有一个特点，就是读非常频繁，但是更新数据会比较少。我们可以用下面的方法替代读写锁。 123456789101112131415161718192021import \"sync/atomic\"import \"unsafe\"type Config struct &#123; data unsafe.Pointer&#125;func (c *Config) Load() &#123; v := c.load() atomic.StorePointer(&amp;c.data, unsafe.Pointer(&amp;v))&#125;func (c *Config) load() map[string]interface&#123;&#125; &#123; // 数据的加载 return make(map[string]interface&#123;&#125;)&#125;func (c *Config) Get() map[string]interface&#123;&#125; &#123; v := atomic.LoadPointer(&amp;c.data) return *(*map[string]interface&#123;&#125;)(v)&#125; 使用原子操作可以保证并发读写时，在更新数据时，保证新的map不会被之前的读操作获取，因此可以保证并发的安全性。 性能测试下面做个性能测试，其中 ConfigV2 是使用原子操作来替换的map数据。 1234567891011121314151617181920212223242526272829func BenchmarkConfig(b *testing.B) &#123; config := &amp;Config&#123;&#125; go func() &#123; for range time.Tick(time.Second) &#123; config.Load() &#125; &#125;() config.Load() b.ResetTimer() for i := 0; i &lt; b.N; i++ &#123; _ = config.Get() &#125;&#125;func BenchmarkConfigV2(b *testing.B) &#123; config := &amp;ConfigV2&#123;&#125; go func() &#123; for range time.Tick(time.Second) &#123; config.Load() &#125; &#125;() config.Load() b.ResetTimer() for i := 0; i &lt; b.N; i++ &#123; _ = config.Get() &#125;&#125; 二者差距有40倍,结果如下: 12345678goos: linuxgoarch: amd64pkg: lpflpf/loaddatacpu: Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHzBenchmarkConfig-32 551491118 21.79 ns/op 0 B/op 0 allocs/opBenchmarkConfigV2-32 1000000000 0.5858 ns/op 0 B/op 0 allocs/opPASSok lpflpf/loaddata 14.870s 技术总结 在做字典加载、配置加载的读多写少的业务中，可以使用原子操作代替读写锁来保证并发的安全。 原子操作性能比较高的原因可能是: 读写锁需要多增加一次原子操作。(有待考证)","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"}]},{"title":"Kafka 消费并行度提升","slug":"kafka","date":"2021-04-19T07:15:00.000Z","updated":"2021-04-19T07:36:03.895Z","comments":true,"path":"passages/kafka/","link":"","permalink":"http://blog.lpflpf.cn/passages/kafka/","excerpt":"本文介绍 Kafka 消费的一个例子，以及如何优化提升消费的并行度。","text":"本文介绍 Kafka 消费的一个例子，以及如何优化提升消费的并行度。 一个例子Kafka 消费一般使用 github.com/Shopify/sarama 包实现，现已支持消费组消费。下面是一个消费组消费的例子： 12345678910111213141516171819func consume()&#123; // 定义一个消费者，并开始消费 consumer := Consumer&#123;&#125; ConsumerHighLevel.Consume(ctx, []string&#123;Conf.topic&#125;, &amp;consumer); err != nil &#123; sarama.Logger.Printf(\"[ERROR] Error from Consumer: %s\", err.Error()) &#125;&#125;type Consumer struct &#123;&#125;func (consumer *Consumer) Setup(sarama.ConsumerGroupSession) error &#123; return nil &#125;func (consumer *Consumer) Cleanup(sarama.ConsumerGroupSession) error &#123; return nil &#125;func (consumer *Consumer) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) (err error) &#123; for &#123; message := &lt;-claim.Messages() println(message.Value) // 消费逻辑 session.MarkMessage(message, \"\") // 提交偏移 &#125; return nil&#125; Client 支持并行消费多个Topic。消费时，会针对各Partition分别启动一个ConsumeClaim 的goroutine，获取队列数据并消费。如下图所示：每个goroutine 会标记消息的偏移量，以方便提交偏移至远程。当服务关闭重启后，会从远端获取当前消费的偏移，并继续消费。 存在的问题为保证消费消息的顺序性，需一个队列由一个 goroutine 消费。此模式下并行度则由kafka的队列数量来控制。Kafka 队列数量多，并行度就越大，队列数越少，并行度就小。假设每条消息需要处理的时间平均为10ms，则一个队列的最大消费个数就是100/s；设置32分片，可以达到3200/s的消息处理量。当业务增长时，可能需要增加kakfa Topic的分片数量来提升消息处理量了。但Kafka的分片数量并不能无限增长。因设置太多的分片可能会造成 Broker 选举慢，客户端需要cache 的消息量过大等问题[1]。 下面看看提升并行度的另一种思路。 如何解决提升并行度的另一种方案，就是在本地做二次Sharding，使用本地队列做真实消费。下面是一个示意图： 每个goroutine通过分片规则重新分配到多个本地队列中。本地的队列个数(消费goroutine 数量)可以自由控制，使消费的并行度可控。 当然，使用本地队列会有如下几个问题： 如何保证偏移提交的顺序性？从本地队列中消费完成后，需要提交偏移到远程。如果提交顺序有问题，可能会出现消息漏掉的情况。举个例子： 消息M1，M2 均来自远程队列Q，且 M1 进入队列的时间早于 M2。在本地做分发时，M1 进入 LQ1, M2 进入 LQ2。若 M1 早于 M2 消费完成并提交偏移则没有问题；若 M1 晚于 M2 消费完成并提交了偏移，此时服务异常退出，当再次启动服务时M1 消息将不再被消费，造成M1 消息丢失。为此，需要在本地维护一个偏移提交的逻辑，保证提交偏移的有序性。逻辑如下： Kafka 的Offset 并不保证连续性[2]，需要对每个kafka Partition 提供一个逻辑队列waitCommitQueue 保存当前正在消费的消息， 提供一个hashmap waitCommitMap 标记某个偏移已完成消费（偏移较小的消息未消费完成。） 当消息从Kafka 队列中取出后，放入waitCommitQueue队列队尾（标记正在消费），并将消息分发到本地队列。 当消息消费完成后，判断该消息是否为 waitCommitQueue 队首: 若是，则说明该消息是最小的offset，直接提交。并循环判断队列后面的消息偏移是否已消费完成？ 如果消费完成，说明可以继续提交偏移 如果未消费完成，则需要等待最小消息消费完成。 若不是，则在waitCommitMap中标记已提交，等待最小偏移的消息出现。 下面是一段简要代码片段: 1234567891011121314151617181920212223242526272829303132333435363738394041424344type Consumer struct &#123; chMessage []chan *CMessage&#125;type None struct&#123;&#125;&#123;&#125;func (consumer *Consumer) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) (err error) &#123; waitCommitQueue := list.New() waitCommitMap := make(map[int64]None, 100000) var mutex sync.Mutex for &#123; message := &lt;-claim.Messages() mutex.Lock() waitCommitQueue.PushBack(message) mutex.Unlock() // 自定义的local 队列, 使用channel 实现 // sharding 是自定义的算法 consumer.chMessage[consumer.Sharding(message)] &lt;- &amp;CMessage&#123; Message: message, MarkMessage: func() &#123; mutex.Lock() defer mutex.Unlock() if waitCommitQueue.Front().Value.(*sarama.ConsumerMessage).Offset == message.Offset &#123; waitCommitQueue.Remove(waitCommitQueue.Front()) session.MarkMessage(message, \"\") for waitCommitQueue.Len() &gt; 0 &#123; item := waitCommitQueue.Front() offset := item.Value.(*sarama.ConsumerMessage).Offset if _, ok := waitCommitMap[offset]; !ok &#123; break &#125; delete(waitCommitMap, offset) session.MarkMessage(item.Value.(*sarama.ConsumerMessage), \"\") waitCommitQueue.Remove(item) &#125; &#125; else &#123; waitCommitMap[message.Offset] = None&#123;&#125; &#125; &#125;, &#125; &#125;&#125; 在做消费时，仅需要启动协程，分别消费各个channel中的数据即可： 1234567891011121314151617181920func (consumer *Consumer) consume()&#123; queues := consumer.chMessage for i := 0; i &lt; len(queues); i++ &#123; wg.Add(1) go func(queue chan *kafkautils.CMessage) &#123; defer wg.Done() for &#123; select &#123; case message := &lt;-queue: // time.Sleep(100 * time.Millisecond) time.Sleep(time.Duration(rand.Int() % 20) * time.Millisecond) message.MarkMessage() atomic.AddInt32(&amp;count, 1) case &lt;-closed: return &#125; &#125; &#125;(queues[i]) &#125;&#125; 有了本地队列的加持，决定并行度的不再是远端队列数量，而是本地的消费队列数量，只要多开点channel，所有问题将迎刃而解。 如何保证本地队列不会暴涨导致OOM？使用带缓冲的channel 作为本地队列，当队列满后将阻塞。避免了无限制的增长导致OOM. 应用场景 适合于消费端消费慢，并行度过低的情况。如果通过增加一些kafka 的partition 的方式解决，建议直接用kafka 的partition，避免在消费端增加逻辑复杂度。 消费的消息具有区分度，可以通过某些字段做分片。 当然此类消费逻辑也适合于消息聚合的场景，通过调整本地消费队列的个数，减少消费的并行度，一定程度上降低消费的速度和服务器的负载。 当服务宕机或者出现故障重启后，可能会出现重复消费的情况。因此消息需要保障最终一致性。 其他 还有个问题，为什么不是每个kafka 队列对应独立的协程池，而是公用同一个协程池？ 通过公用协程池，可以实现资源公用，针对消费写入速度相差甚远的队列时，可以取长补短。 本文相关代码转到github查看 引用[1] 如何设置kafka的分片数[2] Kafka Offset 不连续","categories":[{"name":"Message Queue","slug":"Message-Queue","permalink":"http://blog.lpflpf.cn/categories/Message-Queue/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"kafka","slug":"kafka","permalink":"http://blog.lpflpf.cn/tags/kafka/"}]},{"title":"Pika Hash 表 知识总结","slug":"pika-hash-table","date":"2021-01-29T05:32:22.000Z","updated":"2021-02-04T01:27:38.749Z","comments":true,"path":"passages/pika-hash-table/","link":"","permalink":"http://blog.lpflpf.cn/passages/pika-hash-table/","excerpt":"本文主要对 pika 中 Hash 数据结构的使用做一个小结。","text":"本文主要对 pika 中 Hash 数据结构的使用做一个小结。 Pika 是 360 开源的一个非关系型数据库，可以兼容 Redis 系统的大部分命令。支持主从同步。主要区别是 Pika 支持的数据量不受内存的限制，仅和硬盘大小有关。底层使用了 RocksDB 做 KV 数据的存储。 本文主要对Pika 的 Hash 数据结构做个小结。 命令支持 接口 状态 HDEL 支持 HEXISTS 支持 HGET 支持 HGETALL 支持 HINCRBY 支持 HINCRBYFLOAT 支持 HKEYS 支持 HLEN 支持 HMGET 支持 HMSET 支持 HSET 暂不支持单条命令设置多个field value，如有需求请用HMSET HSETNX 支持 HVALS 支持 HSCAN 支持 HSTRLEN 支持 存储引擎由于 Pika 数据最终会进入RocksDB,而RocksDB仅支持K-V数据结构, 因此 需要把两层结构的 Hash 数据转换为一层的KV存储结构。例如，执行如下的命令： 1HSET key field value Pika首先将创建 hash 的 meta k-v 值，用来保存 hash 结构的元数据, 其数据格式如下： 为了保存 field 和 value 值，将会再创建一个k-v，格式如下： 后创建的k-v 存储了field 和 value。 命令操作为了更好的了解hash 的操作，下面对几类命令逐个学习： 创建/更新操作例如 Hset 操作： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475Status RedisHashes::HSet(const Slice&amp; key, const Slice&amp; field, const Slice&amp; value, int32_t* res) &#123; rocksdb::WriteBatch batch; // 此操作需要加锁 // 函数结束，锁解除 ScopeRecordLock l(lock_mgr_, key); int32_t version = 0; uint32_t statistic = 0; std::string meta_value; // 获取meta 数据 Status s = db_-&gt;Get(default_read_options_, handles_[0], key, &amp;meta_value); if (s.ok()) &#123; ParsedHashesMetaValue parsed_hashes_meta_value(&amp;meta_value); if (parsed_hashes_meta_value.IsStale() || parsed_hashes_meta_value.count() == 0) &#123; // 如果meta 存在，但是没有用到 // 则直接更新meta &amp; field &amp; value version = parsed_hashes_meta_value.InitialMetaValue(); parsed_hashes_meta_value.set_count(1); batch.Put(handles_[0], key, meta_value); HashesDataKey data_key(key, version, field); batch.Put(handles_[1], data_key.Encode(), value); *res = 1; &#125; else &#123; // 如果存在，且时间未过期, 版本正确 version = parsed_hashes_meta_value.version(); std::string data_value; HashesDataKey hashes_data_key(key, version, field); // 获取field 数据 s = db_-&gt;Get(default_read_options_, handles_[1], hashes_data_key.Encode(), &amp;data_value); if (s.ok()) &#123; // 如果当前存的field 数据正确 *res = 0; if (data_value == value.ToString()) &#123; // 值也相等，则不操作 return Status::OK(); &#125; else &#123; // 修改kv batch.Put(handles_[1], hashes_data_key.Encode(), value); statistic++; &#125; &#125; else if (s.IsNotFound()) &#123; // 如果没有存在kv, 则添加，并更新meta parsed_hashes_meta_value.ModifyCount(1); batch.Put(handles_[0], key, meta_value); batch.Put(handles_[1], hashes_data_key.Encode(), value); *res = 1; &#125; else &#123; // 获取失败 return s; &#125; &#125; &#125; else if (s.IsNotFound()) &#123; // 若meta 未找到, 编码，写入 char str[4]; EncodeFixed32(str, 1); HashesMetaValue meta_value(std::string(str, sizeof(int32_t))); version = meta_value.UpdateVersion(); batch.Put(handles_[0], key, meta_value.Encode()); HashesDataKey data_key(key, version, field); batch.Put(handles_[1], data_key.Encode(), value); *res = 1; &#125; else &#123; return s; &#125; // 最后批量写 s = db_-&gt;Write(default_write_options_, &amp;batch); // 更新总的统计信息 UpdateSpecificKeyStatistics(key.ToString(), statistic); return s;&#125; 可以看出，在做 HSet 操作时，会对 metadata 和 field value 同时操作，并需要同时更新，而且由于Pika是多线程服务，需要加锁操作。在频繁访问同一个hash中的数据时，其锁粒度是一个hash的key，可能会有大量的锁冲突出现。 读操作 123456789101112131415161718192021222324252627Status RedisHashes::HGet(const Slice&amp; key, const Slice&amp; field, std::string* value) &#123; std::string meta_value; int32_t version = 0; rocksdb::ReadOptions read_options; const rocksdb::Snapshot* snapshot; ScopeSnapshot ss(db_, &amp;snapshot); read_options.snapshot = snapshot; // 获取meta 数据 Status s = db_-&gt;Get(read_options, handles_[0], key, &amp;meta_value); if (s.ok()) &#123; ParsedHashesMetaValue parsed_hashes_meta_value(&amp;meta_value); if (parsed_hashes_meta_value.IsStale()) &#123; // 如果存在meta，且生效 return Status::NotFound(\"Stale\"); &#125; else if (parsed_hashes_meta_value.count() == 0) &#123; return Status::NotFound(); &#125; else &#123; // 获取key 值 version = parsed_hashes_meta_value.version(); HashesDataKey data_key(key, version, field); s = db_-&gt;Get(read_options, handles_[1], data_key.Encode(), value); &#125; &#125; return s;&#125; 读操作比较简单，无需加锁。先获取metadata值，再通过metadata 计算出 field 存储key，返回结果即可。 删除操作删除操作有两种，一种是删除整个hash 表(DEL)，一种是删除一个field(HDEL)。首先看下删除整个hash 表的操作。 DEL 123456789101112131415161718192021222324Status RedisHashes::Del(const Slice&amp; key) &#123; std::string meta_value; ScopeRecordLock l(lock_mgr_, key); // 删除操作需要加锁 Status s = db_-&gt;Get(default_read_options_, handles_[0], key, &amp;meta_value); // 获取 metadata 值 if (s.ok()) &#123; ParsedHashesMetaValue parsed_hashes_meta_value(&amp;meta_value); if (parsed_hashes_meta_value.IsStale()) &#123; // 如果失效了 return Status::NotFound(\"Stale\"); &#125; else if (parsed_hashes_meta_value.count() == 0) &#123; // 无值 return Status::NotFound(); &#125; else &#123; // 更新统计值，更新meta_value 即可。 uint32_t statistic = parsed_hashes_meta_value.count(); parsed_hashes_meta_value.InitialMetaValue(); s = db_-&gt;Put(default_write_options_, handles_[0], key, meta_value); UpdateSpecificKeyStatistics(key.ToString(), statistic); &#125; &#125; return s;&#125; 这里有个需要注意的地方，hash 删表，并不是删除所有数据，只是把meta_value 值更新即可。(修改count值，时间戳，以及version值)由于field的key 是通过metadata 中的版本值计算出来的，由于meta_value 版本更新，所有 field value 均失效。这个是pika 的一个特性，叫秒删功能。顾名思义，可以做到快速删除hash值，由于其删除hash 只重置了meta值，而hash数据结构已经存在的kv在进行compact时进行 HDEL下面是删除一个field 的方法: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// 从参数可以看出 HDEL 是支持同时删除多个field 的Status RedisHashes::HDel(const Slice&amp; key, const std::vector&lt;std::string&gt;&amp; fields, int32_t* ret) &#123; uint32_t statistic = 0; std::vector&lt;std::string&gt; filtered_fields; std::unordered_set&lt;std::string&gt; field_set; // field 去重 for (auto iter = fields.begin(); iter != fields.end(); ++iter) &#123; std::string field = *iter; if (field_set.find(field) == field_set.end()) &#123; field_set.insert(field); filtered_fields.push_back(*iter); &#125; &#125; rocksdb::WriteBatch batch; rocksdb::ReadOptions read_options; const rocksdb::Snapshot* snapshot; std::string meta_value; int32_t del_cnt = 0; int32_t version = 0; // 加锁 ScopeRecordLock l(lock_mgr_, key); ScopeSnapshot ss(db_, &amp;snapshot); read_options.snapshot = snapshot; Status s = db_-&gt;Get(read_options, handles_[0], key, &amp;meta_value); if (s.ok()) &#123; ParsedHashesMetaValue parsed_hashes_meta_value(&amp;meta_value); if (parsed_hashes_meta_value.IsStale() || parsed_hashes_meta_value.count() == 0) &#123; *ret = 0; return Status::OK(); &#125; else &#123; std::string data_value; version = parsed_hashes_meta_value.version(); // 遍历所有数据，并删除 for (const auto&amp; field : filtered_fields) &#123; HashesDataKey hashes_data_key(key, version, field); s = db_-&gt;Get(read_options, handles_[1], hashes_data_key.Encode(), &amp;data_value); if (s.ok()) &#123; del_cnt++; statistic++; batch.Delete(handles_[1], hashes_data_key.Encode()); &#125; else if (s.IsNotFound()) &#123; continue; &#125; else &#123; return s; &#125; &#125; *ret = del_cnt; parsed_hashes_meta_value.ModifyCount(-del_cnt); batch.Put(handles_[0], key, meta_value); &#125; &#125; else if (s.IsNotFound()) &#123; *ret = 0; return Status::OK(); &#125; else &#123; return s; &#125; s = db_-&gt;Write(default_write_options_, &amp;batch); UpdateSpecificKeyStatistics(key.ToString(), statistic); return s;&#125; HDEL 操作支持批量操作。 数据的清理上面提到了，Pika 对于 hash 做了秒删的功能，那秒删之后field中的数据，如何做清理工作呢？经过研究，发现其实pika没有做主动删除的逻辑，只是通过RocksDB 在做compaction（数据压缩）时调用filter 来实现的。RocksDB 的compaction, 主要是为了压缩内存和硬盘的使用空间，提升查找速度(LSM 树便是不断的把树结构做merge，做内存落盘和数据压缩）。在copact操作时，提供了可定制的filter 接口。在pika 中，就是通过实现该接口来做秒删功能的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// 针对存储数据的k-v 结构的过滤 （还有一种meta 数据过滤的方法）class BaseDataFilter : public rocksdb::CompactionFilter &#123; public: BaseDataFilter(rocksdb::DB* db, std::vector&lt;rocksdb::ColumnFamilyHandle*&gt;* cf_handles_ptr) : db_(db), cf_handles_ptr_(cf_handles_ptr), cur_key_(\"\"), meta_not_found_(false), cur_meta_version_(0), cur_meta_timestamp_(0) &#123;&#125; bool Filter(int level, const Slice&amp; key, const rocksdb::Slice&amp; value, std::string* new_value, bool* value_changed) const override &#123; ParsedBaseDataKey parsed_base_data_key(key); Trace(\"==========================START==========================\"); Trace(\"[DataFilter], key: %s, data = %s, version = %d\", parsed_base_data_key.key().ToString().c_str(), parsed_base_data_key.data().ToString().c_str(), parsed_base_data_key.version()); // 如果是复杂数据结构的key, 两个值不相等，需要取meta中的版本和时间戳 if (parsed_base_data_key.key().ToString() != cur_key_) &#123; cur_key_ = parsed_base_data_key.key().ToString(); std::string meta_value; // destroyed when close the database, Reserve Current key value if (cf_handles_ptr_-&gt;size() == 0) &#123; return false; &#125; // 基于datakey，算出metakey // 查看meta 的状态 Status s = db_-&gt;Get(default_read_options_, (*cf_handles_ptr_)[0], cur_key_, &amp;meta_value); if (s.ok()) &#123; meta_not_found_ = false; ParsedBaseMetaValue parsed_base_meta_value(&amp;meta_value); cur_meta_version_ = parsed_base_meta_value.version(); cur_meta_timestamp_ = parsed_base_meta_value.timestamp(); &#125; else if (s.IsNotFound()) &#123; meta_not_found_ = true; &#125; else &#123; cur_key_ = \"\"; Trace(\"Reserve[Get meta_key faild]\"); return false; &#125; &#125; if (meta_not_found_) &#123; Trace(\"Drop[Meta key not exist]\"); return true; &#125; //判断版本和过期时间 int64_t unix_time; rocksdb::Env::Default()-&gt;GetCurrentTime(&amp;unix_time); if (cur_meta_timestamp_ != 0 &amp;&amp; cur_meta_timestamp_ &lt; static_cast&lt;int32_t&gt;(unix_time)) &#123; Trace(\"Drop[Timeout]\"); return true; &#125; if (cur_meta_version_ &gt; parsed_base_data_key.version()) &#123; Trace(\"Drop[data_key_version &lt; cur_meta_version]\"); return true; &#125; else &#123; Trace(\"Reserve[data_key_version == cur_meta_version]\"); return false; &#125; &#125; const char* Name() const override &#123; return \"BaseDataFilter\"; &#125; private: rocksdb::DB* db_; std::vector&lt;rocksdb::ColumnFamilyHandle*&gt;* cf_handles_ptr_; rocksdb::ReadOptions default_read_options_; mutable std::string cur_key_; mutable bool meta_not_found_; mutable int32_t cur_meta_version_; mutable int32_t cur_meta_timestamp_;&#125;; 从上述代码中可以看出，其实在pika中，对于大批量的数据(比如list，hash，set 等数据结构)均是具有秒删功能的，使用秒删功能比直接删除一方面可以节省执行时间,另一方面可以减少内存碎片，算是以空间换时间的典型例子了。 数据扫描hash 结构除了需要做kv操作外，还有类似扫描key 的操作(HKEYS, HVALS, HGETALL, HSCAN)。例如 HKEYS 命令会返回所有hash 结构中的 fields. 在 pika 中是如何解决该问题的? 问题的解决需要我们从pika 依赖的RocksDB 中找到答案。 由于 RocksDB 是 基于 LSM 树实现的存储引擎，其 KEY 是有序的，因此，可以通过 RocksDB 的区间查询操作做数据查询。 对于 HKEYS, HVALS, HGETALL 操作，会扫描 HASH 中的所有值，因此其扫描的数据，是 (keySize + key + version) 为前缀做索引前缀; 对于 HSCAN 则在 (keySize + key + version) 的基础上，增加 HSCAN 提供的前缀信息做前缀搜索即可。 学习小结 pika 中，可以存在相同的key 不同存储类型的数据。 hash 存储值不超过 2^32 , 由于 hash size 存储在 4bytes 的空间中。 从Hset中，可以看到，在设计数据结构时，尽量减小 hash 中key 值的数量，减少锁meta的时间。 pika hash 结构具有秒删功能，对于大批量数据的hash 结果，删除操作和正常命令一样会快速执行。（这个和redis有一定区别） pika 中异步删除策略是依赖于RocksDB 的compaction 提供的filter 接口实现的。 令人惊喜的是，也有go版本LSM树的实现。(moss) 除了pika外，最近比较火的TiDB的底层存储也是使用的 RocksDB 实现的。 备注：本文源码来自 github.com/Qihoo360/blackwidow 中。","categories":[],"tags":[{"name":"pika","slug":"pika","permalink":"http://blog.lpflpf.cn/tags/pika/"},{"name":"cache","slug":"cache","permalink":"http://blog.lpflpf.cn/tags/cache/"}],"author":"李朋飞"},{"title":"Golang Singleflight 使用和原理","slug":"golang-singleflight-go","date":"2021-01-18T09:16:18.000Z","updated":"2021-01-19T06:02:18.952Z","comments":true,"path":"passages/golang-singleflight-go/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-singleflight-go/","excerpt":"本文介绍 golang.org/x/sync/singleflight 包的使用和原理。","text":"本文介绍 golang.org/x/sync/singleflight 包的使用和原理。 建议结合源码阅读本文 缓存击穿在做高并发的服务时，不可避免的会遇到缓冲击穿的问题。缓冲击穿一般是说，当高并发流量缓存过期的情况下，出现大量请求从数据库读取相同数据的情况。这种情况下数据库的压力将瞬间增大。为了避免这种情况，一般有几种解决方案： 1. 缓存永不过期，缓存做主动更新。 2. 在使用缓存时，先检查缓存的过期时间，如果将要过期时，将过期时间延长到指定时间(避免其他服务也主动更新)，再主动做缓存更新(更新后，设置新的超时时间)。 3. 加互斥锁，在db查询结束后，统一返回数据。(本文主要使用介绍用singleflight 来实现该方法) 这种方法的弊端是，只是单进程限制同时只能有一个请求。 如何使用 singleflight 解决缓存击穿在singleflight 包中，提供了一个同时只运行一次方法(fn)的接口。这个接口和我们需要解决的缓存击穿问题异曲同工，下面简单介绍包中的几个方法： 123456789101112type Result struct &#123; Val interface&#123;&#125; Err error Shared bool &#125; // 同步返回结果func (g *Group) Do(key string, fn func() (interface&#123;&#125;, error)) (v interface&#123;&#125;, err error, shared bool) &#123;&#125;// 返回channel，异步返回结果func (g *Group) DoChan(key string, fn func() (interface&#123;&#125;, error)) &lt;-chan Result &#123;&#125;// 取新结果，不使用正在请求的结果func (g *Group) Forget(key string)&#123;&#125; 一个简单的例子包中提供了同步访问和异步访问两种调用。我们需要用一个简单的例子来做说明: 123456789101112131415161718192021func main() &#123; var count int32 g := &amp;singleflight.Group&#123;&#125; res := []&lt;-chan singleflight.Result&#123;&#125; for i := 0; i &lt; 10; i++ &#123; key := \"hello\" res = append(res, g.DoChan(\"getdata\", func() (interface&#123;&#125;, error) &#123; // mock db query iter := atomic.AddInt32(&amp;count, 1) time.Sleep(time.Duration(time.Microsecond)) // return val return key + strconv.Itoa(int(iter)), nil &#125;)) &#125; for i := 0; i &lt; 10; i++ &#123; dat := &lt;-res[i] fmt.Println(dat.Val) &#125;&#125; 上面例子中，我们 mock 了一个db读取的匿名方法，数据查询使用了1ms 的时间。返回值为 key + count 的值。如果不用singleflight，我们取到的值，一定是key + 1…10，但是使用了之后，取到的结果都是 hello1在查询db时。仅查询了一次，将相同的查询归并为一条。结果可以证明singleflight 符合我们的预期，确实可以防止缓存击穿问题的发生。 singleflight 的实现通过对singleflight包的使用，推测signleflight的实现只需要在执行fn时，判断当前是否有正在进行的fn，如果存在则等待查询结果；如果没有，则记录并执行fn。抽象的来说，就是希望同一个 key 指定的fn，同时仅执行一次，减少fn的调用次数。 纸上得来终觉浅，绝知此事要躬行。下面看看这个包是怎么实现的： 错误类型的定义首先是错误类型的定义， 除了正常的调用失败，一个方法的调用还可能包括 panic 错误 和 runtime.Goexit 调用，为了标记此类错误，因此定义了如下错误类型。 12345var errGoexit = errors.New(\"runtime.Goexit was called\")type panicError struct &#123; value interface&#123;&#125; stack []byte&#125; 执行中程序的调用对于每一次执行fn，会构造一个call 结构体，用于将结果返回给等待的协程。doCall 则为 fn 的调用执行方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566type call struct &#123; wg sync.WaitGroup val interface&#123;&#125; // 调用的返回值 err error // 调用执行失败后的错误 forgotten bool // 标记是否下次调用时不使用正在调用的fn的结果 dups int // 标记有多少调用方在等待fn 的结果 chans []chan&lt;- Result // 等待结果的channal&#125;func (g *Group) doCall(c *call, key string, fn func() (interface&#123;&#125;, error)) &#123; normalReturn := false recovered := false // 为了能捕获到 goexit, 需要使用defer 来判断 （与panic 错误区分） // 实际上 goexit 无法捕获，只能通过标记 panic 和正常退出来排除 // 第一个defer 是对执行结果的处理 defer func() &#123; // 非正常退出和panic， 则为 goexit 退出 if !normalReturn &amp;&amp; !recovered &#123; c.err = errGoexit &#125; c.wg.Done() g.mu.Lock() defer g.mu.Unlock() if !c.forgotten &#123; delete(g.m, key) &#125; if e, ok := c.err.(*panicError); ok &#123; // Panic 错误, 这种panic 无法捕获 if len(c.chans) &gt; 0 &#123; //对于 DoChan 的调用方式 go panic(e) select &#123;&#125; // 保证 `go panic(e)` 的执行，并且 panic 无法被捕获。 &#125; else &#123; panic(e) &#125; &#125; else if c.err == errGoexit &#123; // errGoexit 已经用排除法处理 &#125; else &#123; // 正常返回, 分发返回结果 for _, ch := range c.chans &#123; ch &lt;- Result&#123;c.val, c.err, c.dups &gt; 0&#125; &#125; &#125; &#125;() // 执行 fn 方法，捕获 panic func() &#123; defer func() &#123; if !normalReturn &#123; // 捕获 recover 错误 if r := recover(); r != nil &#123; c.err = newPanicError(r) &#125; &#125; &#125;() c.val, c.err = fn() normalReturn = true &#125;() if !normalReturn &#123; // 如果非正常返回，则是通过 recover 的方式执行的 | 因为 goexit 方式不会走到这里 recovered = true &#125;&#125; 接口的实现同步方式的调用： 123456789101112131415161718192021222324252627func (g *Group) Do(key string, fn func() (interface&#123;&#125;, error)) (v interface&#123;&#125;, err error, shared bool) &#123; g.mu.Lock() if g.m == nil &#123; g.m = make(map[string]*call) &#125; if c, ok := g.m[key]; ok &#123; // 执行中，则加入等待 c.dups++ g.mu.Unlock() c.wg.Wait() if e, ok := c.err.(*panicError); ok &#123; panic(e) &#125; else if c.err == errGoexit &#123; runtime.Goexit() &#125; return c.val, c.err, true &#125; c := new(call) c.wg.Add(1) g.m[key] = c g.mu.Unlock() // 调用执行 g.doCall(c, key, fn) return c.val, c.err, c.dups &gt; 0&#125; 异步方式的调用： 12345678910111213141516171819202122232425func (g *Group) DoChan(key string, fn func() (interface&#123;&#125;, error)) &lt;-chan Result &#123; ch := make(chan Result, 1) g.mu.Lock() if g.m == nil &#123; g.m = make(map[string]*call) &#125; if c, ok := g.m[key]; ok &#123; // 如果正在执行，则加入到chan中，等待 c.dups++ c.chans = append(c.chans, ch) g.mu.Unlock() return ch &#125; // 构造call c := &amp;call&#123;chans: []chan&lt;- Result&#123;ch&#125;&#125; c.wg.Add(1) g.m[key] = c g.mu.Unlock() // 异步执行 go g.doCall(c, key, fn) return ch&#125; 设置下次调用不适用正在执行的结果 12345678func (g *Group) Forget(key string) &#123; g.mu.Lock() if c, ok := g.m[key]; ok &#123; c.forgotten = true &#125; delete(g.m, key) g.mu.Unlock()&#125; 总结从上述代码可以看出，做一个防止穿透的小功能，简单而不简约。需要考量的地方还是挺多(如何截获panic，如何判断goexit 等)。如下是内容总结： runtime.Goexit 的特性, 以及如何捕获。(https://golang.org/cl/134395) goexit 用于退出某个协程，但是之前注册的defer 方法仍然将会被执行。 返回的error，不仅可能是正常逻辑错误，或者goexit 错误, 还有可能直接panic。 DoChan 调用方式，如果fn出现panic，该panic将无法被捕获，程序将退出。(Do 方式可以捕获panic), 而 Do 调用则可以捕获。例子如下： 12345678910111213141516go func() &#123; defer wg.Done() key := \"hello\" defer func() &#123; if r := recover(); r != nil &#123; fmt.Println(\"[[[\", r, \"]]]\") // 此处可以捕获到panic. 由 doCall 方法中捕获后再次抛出的异常 &#125; &#125;() _, _, _ = g.Do(\"getdata\", func() (interface&#123;&#125;, error) &#123; iter := atomic.AddInt32(&amp;count, 1) time.Sleep(time.Duration(time.Microsecond)) panic(\"panic\") return key + strconv.Itoa(int(iter)), nil &#125;)&#125;()","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"singleflight","slug":"singleflight","permalink":"http://blog.lpflpf.cn/tags/singleflight/"}]},{"title":"Golang Database 中 Scanner/ Valuer 接口学习","slug":"golang-db-scanner-valuer-interface","date":"2021-01-11T03:47:35.000Z","updated":"2021-01-11T07:44:25.861Z","comments":true,"path":"passages/golang-db-scanner-valuer-interface/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-db-scanner-valuer-interface/","excerpt":"自定义类型在做数据库查询和插入操作时，可以通过实现 Scanner / Valuer 接口，使我们在做db的增删查改时更加顺畅。","text":"自定义类型在做数据库查询和插入操作时，可以通过实现 Scanner / Valuer 接口，使我们在做db的增删查改时更加顺畅。 现实中遇到的问题在做后台系统时，有些表中的字段是定制化的，例如: 12type Day time.Time // 以天为单位标记type LocaleTime time.Time // 本地格式化的时间, 存在 0000-00-00 00:00:00 值 为什么需要给这些类型重定义? 我这里的原因是为了在给下游提供JSON接口时输出标准化的值。例如，对于Day 类型，需要输出 “2021-01-11”，对于 LocaleTime 需要输出的是 “2021-01-11 12:41:01”。 对于JSON 的格式化，使用的是如下接口： 12345678910111213141516171819202122232425262728293031323334353637383940414243const dayFormat = \"2006-01-02\"// 格式化JSON 解析func (t *Day) UnmarshalJSON(data []byte) (err error) &#123; now, err := time.ParseInLocation(`\"`+dayFormat+`\"`, string(data), time.Local) *t = Day(now) return&#125;// 格式化JSON 编码func (t Day) MarshalJSON() ([]byte, error) &#123; b := make([]byte, 0, len(dayFormat)+2) b = append(b, '\"') b = time.Time(t).AppendFormat(b, dayFormat) b = append(b, '\"') return b, nil&#125;const localTimeFormat = \"2006-01-02 15:04:05\"func (t *LocalTime) UnmarshalJSON(data []byte) (err error) &#123; now, err := time.ParseInLocation(`\"`+localTimeFormat+`\"`, string(data), time.Local) *t = LocalTime(now) return&#125;func (t LocalTime) MarshalJSON() ([]byte, error) &#123; b := make([]byte, 0, len(localTimeFormat)+2) b = append(b, '\"') b = append(b, []byte(t.String())...) //b = time.Time(t).AppendFormat(b, localTimeFormat) b = append(b, '\"') return b, nil&#125;func (t LocalTime) String() string &#123; if time.Time(t).IsZero() &#123; return \"0000-00-00 00:00:00\" &#125; return time.Time(t).Format(localTimeFormat)&#125; 解决了JSON 编解码的问题，但是对于DB插入和查询却总是有问题。 如何解决经过翻看接口文档，其实解决方式和 json.UnmarshalJSON / json.MarshalJSON 接口类似。只要实现该类型的Valuer/Scanner接口即可。 1234567891011121314151617181920212223242526272829303132333435func (t Day) Value() (driver.Value, error) &#123; tTime := time.Time(t) return tTime.Format(\"2006/01/02 15:04:05\"), nil&#125;func (t *Day) Scan(v interface&#123;&#125;) error &#123; switch vt := v.(type) &#123; case time.Time: *t = Day(vt) case string: tTime, _ := time.Parse(\"2006/01/02 15:04:05\", vt) *t = Day(tTime) &#125; return nil&#125;func (t LocalTime) Value() (driver.Value, error) &#123; if time.Time(t).IsZero() &#123; return \"0000-00-00 00:00:00\", nil &#125; return time.Time(t), nil&#125;func (t *LocalTime) Scan(v interface&#123;&#125;) error &#123; switch vt := v.(type) &#123; case time.Time: *t = LocalTime(vt) case string: tTime, _ := time.Parse(\"2006/01/02 15:04:05\", vt) *t = LocalTime(tTime) default: return nil &#125; return nil&#125; 接口学习下面，具体学习下两个接口。 Scanner 接口 12345678910111213141516type Value interface&#123;&#125;type Valuer interface &#123; // Value returns a driver Value. Value() (Value, error)&#125;func IsValue(v interface&#123;&#125;) bool &#123; if v == nil &#123; return true &#125; switch v.(type) &#123; case []byte, bool, float64, int64, string, time.Time: return true &#125; return false&#125; 在sql的Exec 和 Query 时，需要将入参转换为各db驱动包支持的数据类型。而Valuer 是将值转换为 driver.Value 类型的接口定义。 因此，对于自定义的时间转义，可以转义为一个 time.Time 类型，或者一个字符串类型。 Valuer 接口 123type Scanner interface &#123; Scan(src interface&#123;&#125;) error&#125; 在数据查询结果中，需要将查询结果映射为go支持的数据类型。在实现时，首先会把所有的数据都转换为 int64, float64, bool, []byte, string, time.Time, nil 几种类型，然后调用目标类型的Scan方法赋值。实现Scanner 接口时，入参即为这些类型中的一种，仅需把入参转为我们的变量即可。","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"database/sql","slug":"database-sql","permalink":"http://blog.lpflpf.cn/tags/database-sql/"}]},{"title":"Golang反射学习：手写一个RPC","slug":"golang-rpc","date":"2021-01-08T08:03:00.000Z","updated":"2021-01-11T06:41:06.577Z","comments":true,"path":"passages/golang-rpc/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-rpc/","excerpt":"本文主要为了在对golang反射学习后做一个小练习，使用100行代码实现一个通用的RPC服务。","text":"本文主要为了在对golang反射学习后做一个小练习，使用100行代码实现一个通用的RPC服务。 简要说明golang 的RPC框架还是非常丰富的，比如 gRPC，go-zero, go-dubbo 等都是使用非常普遍的rpc框架。在go语言实现的RPC客户端中，大部分RPC框架采用的是使用生成代码的方式来构建RPC服务。即：定义好相应的接口后，需要通过命令生成相应的代码。采用这种方式的优点在于可以减少不必要的类型转换；而麻烦之处也显而易见，需要在每次结构发生改变时，重新生成对应的代码。那么，如果不采用命令行生成的方式来调用RPC该怎么做呢？经过对golang反射的学习后，让我们用100行代码来小试牛刀，实现一个极简版的RPC。 协议的定义由于极简，我们采用HTTP协议，数据传输采用最常见的json结构。服务请求，通过http 请求路径判断调用哪个方法。 输入参数定义如下：[&quot;参数1&quot;, &quot;参数2&quot;]其中参数1，参数2 采用Json编码，最终的请求参数在做一次编码。例如：Do(&quot;abc&quot;, 123) ,其post请求body为 [&quot;\\&quot;abc\\&quot;&quot;, 123] 输出参数定义与输入参数定义格式相同 如何使用服务端：服务端需要实现每一个接口，并把接口绑定到对应的路由上。 123456789101112package mainimport \"github.com/lpflpf/rpc\"import \"strconv\"func main() &#123; serv := rpc.NewRpcServ(\"127.0.0.1:18080\") serv.Impl(\"/conv/int2str\", strconv.Itoa) // 路由绑定到方法 serv.Impl(\"/conv/str2int\", strconv.Atoi) serv.Impl(\"/math/add\", func(a, b int) int &#123; return a + b &#125;) serv.Start()&#125; 客户端调用客户端仅需要定义对应的rpc服务的方法，并通过struct tag的方式指定路由即可 123456789101112131415161718192021222324package mainimport \"fmt\"import \"github.com/lpflpf/rpc\"type Conv struct &#123; Int2Str func(int) string `rpc:\"conv/int2str\"` Str2Int func(input string) (int, error) `rpc:\"conv/str2int\"`&#125;type Math struct &#123; Add func(int, int) int `rpc:\"math/add\"`&#125;func main() &#123; conv := &amp;Conv&#123;&#125; rpc.Connect(\"http://127.0.0.1:18080\", conv) // 连接RPC 服务 fmt.Println(conv.Int2Str(123), conv.Int2Str(456)) // 123 456 fmt.Println(conv.Str2Int(\"1234\")) // 1234 &lt;nil&gt; math := &amp;Math&#123;&#125; rpc.Connect(\"http://127.0.0.1:18080\", math) // 连接 RPC 服务 fmt.Println(math.Add(1, 2)) // 3&#125; Server 端的实现服务端主要是将注册路由。在处理请求时，需要将请求的数据转化为注册句柄的参数，并将句柄的处理结果编码，并返回给客户端。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package rpcimport \"net/http\"import \"reflect\"import \"encoding/json\"import \"io/ioutil\"type RpcServ struct &#123; serv *http.Server mux *http.ServeMux&#125;func (rs *RpcServ) Impl(router string, f interface&#123;&#125;) &#123; rs.mux.HandleFunc(router, func(rw http.ResponseWriter, request *http.Request) &#123; rt := reflect.TypeOf(f) requestBody, _ := ioutil.ReadAll(request.Body) requestData := []string&#123;&#125; _ = json.Unmarshal(requestBody, &amp;requestData) params := []reflect.Value&#123;&#125; num := rt.NumIn() if rt.IsVariadic() &#123; num = num - 1 &#125; for i := 0; i &lt; num; i++ &#123; val := reflect.New(rt.In(i)) json.Unmarshal([]byte(requestData[i]), val.Interface()) params = append(params, val.Elem()) &#125; call := reflect.ValueOf(f) result := []reflect.Value&#123;&#125; if rt.IsVariadic() &#123; val := reflect.MakeSlice(rt.In(num), 0, 0).Interface() json.Unmarshal([]byte(requestData[num]), &amp;val) params = append(params, reflect.ValueOf(val)) result = call.CallSlice(params) &#125; else &#123; result = call.Call(params) &#125; response := []string&#123;&#125; for _, res := range result &#123; val, _ := json.Marshal(res.Interface()) response = append(response, string(val)) &#125; data, _ := json.Marshal(response) rw.Write(data) &#125;)&#125;func (rs *RpcServ) Start() &#123; rs.serv.Handler = rs.mux rs.serv.ListenAndServe()&#125;func NewRpcServ(addr string) *RpcServ &#123; return &amp;RpcServ&#123; serv: &amp;http.Server&#123;Addr: addr&#125;, mux: http.NewServeMux(), &#125;&#125; 客户端实现客户端需要在Connect时，针对定义的每个句柄（即客户端调用时内部的方法）均需要绑定一个RPC 请求的实现。 RPC 请求的实现，即获取方法调用的各个参数，并编码后发送请求至 server 端，读取请求结果并解码，将解码后的数据填充为函数的返回值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package rpcimport \"io/ioutil\"import \"bytes\"import \"strings\"import \"reflect\"import \"errors\"import \"encoding/json\"import \"net/http\"type RpcClient struct &#123; serv *http.Server mux *http.ServeMux&#125;// struct BIND RPCfunc Connect(addr string, iface interface&#123;&#125;) error &#123; rv := reflect.ValueOf(iface).Elem() rt := reflect.TypeOf(iface).Elem() if rt.Kind() != reflect.Struct &#123; return errors.New(\"\") &#125; for i := 0; i &lt; rt.NumField(); i++ &#123; if requestPath := rt.Field(i).Tag.Get(\"rpc\"); requestPath == \"\" &#123; continue &#125; else &#123; fieldType := rt.Field(i).Type rv.Field(i).Set(reflect.MakeFunc(fieldType, func(params []reflect.Value) []reflect.Value &#123; requestBody := []string&#123;&#125; for _, param := range params &#123; raw, _ := json.Marshal(param.Interface()) requestBody = append(requestBody, string(raw)) &#125; body, _ := json.Marshal(requestBody) // 拼接请求Uri requestUri := strings.Trim(addr, \"/\") + \"/\" + strings.Trim(requestPath, \"/\") resp, _ := http.Post(requestUri, \"application/json\", bytes.NewReader(body)) defer resp.Body.Close() data, _ := ioutil.ReadAll(resp.Body) // 组装返回结果 ret := []reflect.Value&#123;&#125; responseStr := []string&#123;&#125; _ = json.Unmarshal(data, &amp;responseStr) for i := 0; i &lt; fieldType.NumOut(); i++ &#123; val := reflect.New(fieldType.Out(i)) _ = json.Unmarshal([]byte(responseStr[i]), val.Interface()) ret = append(ret, val.Elem()) &#125; return ret &#125;)) &#125; &#125; return nil&#125; 小记在看reflect.MakeFunc 时，源码中给出的例子是一个抽象的Swap 方法，联想到可以通过抽象的方法来实现一个RPC的调用。因此有了本文中的代码。代码中未做异常处理，仅是对reflect.MakeFunc, reflect.Call, reflect.CallSlice 理解的一个实践。 golang 版本: go1.12.5 linux/amd64 源码地址: github.com/lpflpf/rpc","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"reflect","slug":"reflect","permalink":"http://blog.lpflpf.cn/tags/reflect/"}]},{"title":"Golang sync.Map 实现","slug":"golang-sync-map","date":"2020-11-21T05:55:40.000Z","updated":"2021-01-11T06:41:06.579Z","comments":true,"path":"passages/golang-sync-map/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-sync-map/","excerpt":"本文主要简单介绍 sync.Map 的使用和实现。","text":"本文主要简单介绍 sync.Map 的使用和实现。 众所周知，Golang 的map是非协程安全的（并发读写数据是不允许的，但是可以并发读）。因此，在 Golang1.9 的版本中加入了 sync.Map 包，用于并发的访问 map。 下面我们简单学习sync.Map 的使用和实现。 如何使用sync.Map 和map 在使用上有较大区别。map 为内置类型，sync.Map 实质是实现了一个带有一些操作方法的Struct 对象。因此，在使用sync.Map包做数据存取时，其实是调用了对象的一些方法来实现的。下面是sync.Map使用的一个简单例子，包含了大部分日常所需方法。 123456789101112131415161718192021222324252627var cache sync.Map var k1 = \"key1\" var v1 = []int&#123;1, 2, 3&#125; println(\"==&gt;Store, Load&lt;==\") cache.Store(k1, v1) if val, ok := cache.Load(k1); ok &#123; fmt.Println(val.([]int)) &#125; println(\"==&gt;LoadOrStore&lt;==\") k2 := \"key2\" v2 := \"val2\" fmt.Println(cache.LoadOrStore(k2, v2)) fmt.Println(cache.LoadOrStore(k2, v2)) println(\"==&gt;Range&lt;==\") cache.Range(func(k, v interface&#123;&#125;) bool &#123; fmt.Println(k, v) return true &#125;) println(\"==&gt;Delete k2&lt;==\") cache.Delete(k2) fmt.Println(cache.Load(k2)) 这里需要注意的是，在sync.Map中，没有提供 len 方法。 sync.Map 实现为了更好的理解sync.Map，有必要学习 sync.Map 是如何实现的。数据结构如下： 1234567891011121314151617type Map struct &#123; mu Mutex // 锁map的互斥锁 read atomic.Value // 读结构 dirty map[interface&#123;&#125;]*entry misses int // 统计&#125;type readOnly struct &#123; m map[interface&#123;&#125;]*entry amended bool // 标记 dirty 中存在 read 里不存在的key&#125;type entry struct &#123; p unsafe.Pointer&#125;var expunged = unsafe.Pointer(new(interface&#123;&#125;)) mu 在数据访问上，通过互斥锁mu来保证 dirty 做读写操作的互不冲突。 read 对象通过原子访问的方式保存,保存对象为 readOnly 类型，包含了存储数据的 m, 和一个标识 amended。在大部分情况下，读 read 不需要加锁访问。这里可以减少抢锁带来的消耗。amended 标识是否在 dirty 中有 read 中没有的数据。 dirty 也保存了一个 map 数据。当用户写操作时，如果 read 中没有对应的 key，就会加锁把数据写入 dirty 中。dirty 如果不为 nil 的情况下，read 的数据应该是 dirty 数据的子集。 misses 为一个统计参数，在数据访问时，如果 key 在 read 中不存在, 且 amended 标识为 true， dirty 增加 1， 当 misses 值大于 dirty 中元素的大小时，将 dirty 中的数据替换为 read 的 map。 *entry.p 保存了value值的指针。p 存在多种情况: p 为 expunged, 标记在 read 中将删除的数据。在下一次 dirty 转为 read 数据时将被删除。 p 为 nil，标识删除，但是 key 位置还保留，在 dirtyLocked 中，被转为 expunged 数据存在，在调用 Delete 方法时，将被置为 nil 通过简单的描述，可以理解为读操作，尽量访问 read。写操作大部分情况下会访问 dirty （除非是做覆盖操作）。 下面，我们对比较常用的几个操作做细致分析。 LoadLoad 操作从Map中获取 key 对应的value 值。 123456789101112131415161718192021222324252627282930313233func (m *Map) Load(key interface&#123;&#125;) (value interface&#123;&#125;, ok bool) &#123; // 读操作首先从read中读取，如果读到数据，则返回 read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended &#123; // 如果没有读到数据，并且存在dirty中有，read中没有的数据 m.mu.Lock() read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; e, ok = m.dirty[key] // 出现了从read取失败的情况，miss += 1, 并考虑是否需要将dirty数据搬迁至read m.missLocked() &#125; m.mu.Unlock() &#125; if !ok &#123; return nil, false &#125; return e.load()&#125;func (m *Map) missLocked() &#123; m.misses++ // misses 过多的话，就做一次copy if m.misses &lt; len(m.dirty) &#123; return &#125; m.read.Store(readOnly&#123;m: m.dirty&#125;) m.dirty = nil m.misses = 0&#125; Load 方法比较好理解。从read中取数据。在未取到并且存在脏数据的情况下，到dirty中取。如果miss过多的话，就把dirty放到read中。Load中，如果在read中出现了，就不需要加锁。反之需要加锁。这里也可以看出，在读的频次远远大于写时，大部分情况下是不需要加锁的，这也是sync.Map 的优势。 Store 123456789101112131415161718192021222324252627282930313233343536373839404142434445func (m *Map) Store(key, value interface&#123;&#125;) &#123; // 首先看read中是否存在对应的key，如果存在，直接替换即可 read, _ := m.read.Load().(readOnly) if e, ok := read.m[key]; ok &amp;&amp; e.tryStore(&amp;value) &#123; return &#125; m.mu.Lock() read, _ = m.read.Load().(readOnly) if e, ok := read.m[key]; ok &#123; // 如果之前是删除状态 if e.unexpungeLocked() &#123; m.dirty[key] = e &#125; e.storeLocked(&amp;value) &#125; else if e, ok := m.dirty[key]; ok &#123; // 如果 read 中不存在， dirty 中存在，那和store 一样，保存即可 e.storeLocked(&amp;value) &#125; else &#123; // read/dirty 中都不存在 if !read.amended &#123; // 构造一个dirty 数据集 （包含目前 read 中的非删除的数据） O(n) m.dirtyLocked() // 设置 amended 与 dirty 不一致 m.read.Store(readOnly&#123;m: read.m, amended: true&#125;) &#125; m.dirty[key] = newEntry(value) // 数据保存至dirty &#125; m.mu.Unlock()&#125;// 清理read.m中的数据func (m *Map) dirtyLocked() &#123; if m.dirty != nil &#123; return &#125; read, _ := m.read.Load().(readOnly) m.dirty = make(map[interface&#123;&#125;]*entry, len(read.m)) for k, e := range read.m &#123; // 真正删除数据 if !e.tryExpungeLocked() &#123; m.dirty[k] = e &#125; &#125;&#125; Store 方法略微复杂，在read中存在对应key时，直接替换即可，此处也不需要加锁。如果不存在，就需要加锁新增key 了。 如果dirty中存在，则直接保存。如果都不存在，则首先判断是否有修正过的数据，如果没有，需要调用dirtyLocked 方法，将read 方法中的未删除的数据copy 到新创建的map中，并标记read中nil值为 expunged（如果被标记过，那该value在read中不能被修改了）。重新设置amended 为 被修改的数据，并将新增的kv赋值到dirty中。 需要注意的是，在调用tryStore 更新 read 中的value值时，需要判断是否p 被标记为 unexpunged. 如果被标记为 unexpunged，则不能被更新。原因是: 在标记 unexpunged 后，在 dirty 中将不存在该值。如果做了更新，read中的数据将在dirty中不存在，导致在未来dirty迁移为read.m 时出现数据丢失。 Delete 12345678910111213141516171819202122232425func (m *Map) LoadAndDelete(key interface&#123;&#125;) (value interface&#123;&#125;, loaded bool) &#123; read, _ := m.read.Load().(readOnly) e, ok := read.m[key] // 删除操作大部分情况下也只是做nil标记，并不是直接删除 if !ok &amp;&amp; read.amended &#123; // 如果read 里面没有，并且有脏数据的时候，就需要检查 dirty 的数据 m.mu.Lock() read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; e, ok = m.dirty[key] m.missLocked() // 可能需要将 dirty -&gt; read &#125; m.mu.Unlock() &#125; if ok &#123; return e.delete() // 标记 e.p = nil &#125; return nil, false&#125;// Delete deletes the value for a key.func (m *Map) Delete(key interface&#123;&#125;) &#123; m.LoadAndDelete(key)&#125; 对于在read中存在的数据，删除操作，只是通过标记value值为nil，并不会实际删除对应key. 这种情况下，不需要加锁。 技术总结 为什么在store中，read 可以不加锁修改map值。正常情况下，对map的赋值是需要加锁的(不然可能会出现panic)，但是，为了减少加锁的消耗，map中存储的是entry对象，对象中保存的entry。赋值只与entry.p 相关，与map的赋值没有关系了。 理论上，在read不存在key 的情况下，删除dirty中的key，只需要直接删除key 即可。（看github.com中源码已修改） 为了保证操作的原子性，在做entry中数据的赋值时，均采用 atomic.StorePointer, atomic.LoadPointer, atomic.CompareAndSwapPointer 保证了赋值的有序和原子性。 为了保证无锁状态下读取 read.m，read.m 对象是只读的，无法增加和删除key。通过标记p的值来删除，以及通过使用 dirty 替换 read.m 做数据的更新。 从源码角度看来性能： 不存在删除的情况，且key值比较固定，大部分情况是不需要加锁的。 对于读多写少的情况，大部分情况也是从read中取值，不需要加锁的。 对于存取的kv数据量非常大（百万级别）的情况下，一次 Store 可能需要对所有的read做遍历，并将未标记删除的entry赋值给dirty,这时可能出现卡顿现象。 对于协程安全的map，除了可以使用sync.Map 对象外，还可以用map +读写锁的方式简单暴力的实现协程安全的map。另外github.com/orcaman/concurrent-map 包也是一个不错的实现。concurrent-map 采用分段锁的方式实现了一个协程安全的map，在某些场景下比 sync.Map 有很大优势。具体如何选取，我们在接下来的文章中做具体分析。","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"sync","slug":"sync","permalink":"http://blog.lpflpf.cn/tags/sync/"}]},{"title":"Golang 限流器","slug":"time-limiter","date":"2020-06-20T06:17:00.000Z","updated":"2021-01-11T06:41:06.598Z","comments":true,"path":"passages/time-limiter/","link":"","permalink":"http://blog.lpflpf.cn/passages/time-limiter/","excerpt":"本文从源码角度学习不同限流器的实现方式。","text":"本文从源码角度学习不同限流器的实现方式。 限流器是服务中非常重要的一个组件，在网关设计、微服务、以及普通的后台应用中都比较常见。它可以限制访问服务的频次和速率，防止服务过载，被刷爆。 限流器的算法比较多，常见的比如令牌桶算法、漏斗算法、信号量等。本文主要介绍基于漏斗算法的一个限流器的实现。文本也提供了其他几种开源的实现方法。 基于令牌桶的限流器实现在golang 的官方扩展包 time 中（github/go/time），提供了一个基于令牌桶算法的限流器的实现。 原理令牌桶限流器，有两个概念： 令牌：每次都需要拿到令牌后，才可以访问 桶：有一定大小的桶，桶中最多可以放一定数量的令牌 放入频率：按照一定的频率向通里面放入令牌，但是令牌数量不能超过桶的容量 因此，一个令牌桶的限流器，可以限制一个时间间隔内，最多可以承载桶容量的访问频次。下面我们看看官方的实现。 实现限流器的定义下面是对一个限流器的定义： 123456789type Limiter struct &#123; limit Limit // 放入桶的频率 （Limit 为 float64类型） burst int // 桶的大小 mu sync.Mutex tokens float64 // 当前桶内剩余令牌个数 last time.Time // 最近取走token的时间 lastEvent time.Time // 最近限流事件的时间&#125; 其中，核心参数是 limit，burst。 burst 代表了桶的大小，从实际意义上来讲，可以理解为服务可以承载的并发量大小；limit 代表了 放入桶的频率，可以理解为正常情况下，1s内我们的服务可以处理的请求个数。 在令牌发放后，会被保留在Reservation 对象中，定义如下： 1234567type Reservation struct &#123; ok bool // 是否满足条件分配到了tokens lim *Limiter // 发送令牌的限流器 tokens int // tokens 的数量 timeToAct time.Time // 满足令牌发放的时间 limit Limit // 令牌发放速度&#125; Reservation 对象，描述了一个在达到 timeToAct 时间后，可以获取到的令牌的数量tokens。 （因为有些需求会做预留的功能，所以timeToAct 并不一定就是当前的时间。 限流器如何限流官方提供的限流器有阻塞等待式的，也有直接判断方式的，还有提供了自己维护预留式的，但核心的实现都是下面的reserveN 方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 在 now 时间需要拿到n个令牌，最多可以等待的时间为maxFutureResrve// 结果将返回一个预留令牌的对象func (lim *Limiter) reserveN(now time.Time, n int, maxFutureReserve time.Duration) Reservation &#123; lim.mu.Lock() // 首先判断是否放入频次是否为无穷大，如果为无穷大，说明暂时不限流 if lim.limit == Inf &#123; // ... &#125; // 拿到截至now 时间时，可以获取的令牌tokens数量，上一次拿走令牌的时间last now, last, tokens := lim.advance(now) // 然后更新 tokens 的数量，把需要拿走的去掉 tokens -= float64(n) // 如果tokens 为负数，说明需要等待，计算等待的时间 var waitDuration time.Duration if tokens &lt; 0 &#123; waitDuration = lim.limit.durationFromTokens(-tokens) &#125; // 计算是否满足分配条件 // ① 需要分配的大小不超过桶容量 // ② 等待时间不超过设定的等待时常 ok := n &lt;= lim.burst &amp;&amp; waitDuration &lt;= maxFutureReserve // 最后构造一个Reservation对象 r := Reservation&#123; ok: ok, lim: lim, limit: lim.limit, &#125; if ok &#123; r.tokens = n r.timeToAct = now.Add(waitDuration) &#125; // 并更新当前limiter 的值 if ok &#123; lim.last = now lim.tokens = tokens lim.lastEvent = r.timeToAct &#125; else &#123; lim.last = last &#125; lim.mu.Unlock() return r&#125; 从实现上看，limiter 并不是每隔一段时间更新当前桶中令牌的数量，而是记录了上次访问时间和当前桶中令牌的数量。当再次访问时，通过上次访问时间计算出当前桶中的令牌的数量，决定是否可以发放令牌。 使用下面我们通过一个简单的例子，学习上面介绍的限流器的使用。 1234567limiter := rate.NewLimiter(rate.Every(100*time.Millisecond), 10)http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) &#123; if limiter.Allow() &#123;// do something log.Println(\"say hello\") &#125;&#125;)_ = http.ListenAndServe(\":13100\", nil) 上面，每100 ms 放入令牌桶中1个令牌，所以当批量访问该接口时，可以看到如下结果： 12342020/06/26 14:34:16 say hello 有18 条记录2020/06/26 14:34:17 say hello 有10 条记录2020/06/26 14:34:18 say hello 有10 条记录 ... 一开始漏斗满着，可以缓解部分突发的流量。当漏斗未空时，访问的频次和令牌放入的频次变为一致。 其他限流器的实现 uber 开源库中基于漏斗算法实现了一个限流器。漏斗算法可以限制流量的请求速度，并起到削峰填谷的作用。 滴滴开源实现了一个对http请求的限流器中间件。可以基于以下模式限流。 基于IP，路径，方法，header，授权用户等限流 通过自定义方法限流 还支持基于 http header 设置限流数据 实现方式是基于 github/go/time 实现的，不同类别的数据都存储在一个带超时时间的数据池中。 golang 网络包中还有基于信号量实现的限流器,也值得我们去学习下。源码地址。 总结令牌桶实现的限流器算法，相较于漏斗算法可以在一定程度上允许突发的流量进入我们的应用中，所以在web应用中最为广泛。 在实际使用时，一般不会做全局的限流，而是针对某些特征去做精细化的限流。例如：通过header、x-forward-for 等限制爬虫的访问，通过对 ip,session 等用户信息限制单个用户的访问等。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"rate limiter","slug":"rate-limiter","permalink":"http://blog.lpflpf.cn/tags/rate-limiter/"}]},{"title":"Golang 熔断器","slug":"circuit-breaker","date":"2020-06-19T06:17:00.000Z","updated":"2021-01-11T06:41:06.544Z","comments":true,"path":"passages/circuit-breaker/","link":"","permalink":"http://blog.lpflpf.cn/passages/circuit-breaker/","excerpt":"本文主要从源码角度介绍golang 熔断器的一种实现。","text":"本文主要从源码角度介绍golang 熔断器的一种实现。 熔断器像是一个保险丝。当我们依赖的服务出现问题时，可以及时容错。一方面可以减少依赖服务对自身访问的依赖，防止出现雪崩效应；另一方面降低请求频率以方便上游尽快恢复服务。 熔断器的应用也非常广泛。除了在我们应用中，为了请求服务时使用熔断器外，在 web 网关、微服务中，也有非常广泛的应用。本文将从源码角度学习sony 开源的一个熔断器实现 github/sony/gobreaker。（代码注释可以从github/lpflpf/gobreaker 查看) 熔断器的模式gobreaker是基于《微软云设计模式》一书中的熔断器模式的Golang实现。下面是模式定义的一个状态机： 熔断器有三种状态，四种状态转移的情况： 三种状态： 熔断器关闭状态, 服务正常访问 熔断器开启状态，服务异常 熔断器半开状态，部分请求，验证是否可以访问 四种状态转移： 在熔断器关闭状态下，当失败后并满足一定条件后，将直接转移为熔断器开启状态。 在熔断器开启状态下，如果过了规定的时间，将进入半开启状态，验证目前服务是否可用。 在熔断器半开启状态下，如果出现失败，则再次进入关闭状态。 在熔断器半开启后，所有请求（有限额）都是成功的，则熔断器关闭。所有请求将正常访问。 gobreaker 的实现gobreaker 是在上述状态机的基础上，实现的一个熔断器。 熔断器的定义 1234567891011121314type CircuitBreaker struct &#123; name string maxRequests uint32 // 最大请求数 （半开启状态会限流） interval time.Duration // 统计周期 timeout time.Duration // 进入熔断后的超时时间 readyToTrip func(counts Counts) bool // 通过Counts 判断是否开启熔断。需要自定义 onStateChange func(name string, from State, to State) // 状态修改时的钩子函数 mutex sync.Mutex // 互斥锁，下面数据的更新都需要加锁 state State // 记录了当前的状态 generation uint64 // 标记属于哪个周期 counts Counts // 计数器，统计了 成功、失败、连续成功、连续失败等，用于决策是否进入熔断 expiry time.Time // 进入下个周期的时间&#125; 其中，如下参数是我们可以自定义的： MaxRequests：最大请求数。当在最大请求数下，均请求正常的情况下，会关闭熔断器 interval：一个正常的统计周期。如果为0，那每次都会将计数清零 timeout: 进入熔断后，可以再次请求的时间 readyToTrip：判断熔断生效的钩子函数 onStateChagne：状态变更的钩子函数 请求的执行熔断器的执行操作，主要包括三个阶段；①请求之前的判定；②服务的请求执行；③请求后的状态和计数的更新 1234567891011121314151617181920212223242526// 熔断器的调用func (cb *CircuitBreaker) Execute(req func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) &#123; // ①请求之前的判断 generation, err := cb.beforeRequest() if err != nil &#123; return nil, err &#125; defer func() &#123; e := recover() if e != nil &#123; // ③ panic 的捕获 cb.afterRequest(generation, false) panic(e) &#125; &#125;() // ② 请求和执行 result, err := req() // ③ 更新计数 cb.afterRequest(generation, err == nil) return result, err&#125; 请求之前的判定操作请求之前，会判断当前熔断器的状态。如果熔断器以开启，则不会继续请求。如果熔断器半开，并且已达到最大请求阈值，也不会继续请求。 12345678910111213141516func (cb *CircuitBreaker) beforeRequest() (uint64, error) &#123; cb.mutex.Lock() defer cb.mutex.Unlock() now := time.Now() state, generation := cb.currentState(now) if state == StateOpen &#123; // 熔断器开启，直接返回 return generation, ErrOpenState &#125; else if state == StateHalfOpen &amp;&amp; cb.counts.Requests &gt;= cb.maxRequests &#123; // 如果是半打开的状态，并且请求次数过多了，则直接返回 return generation, ErrTooManyRequests &#125; cb.counts.onRequest() return generation, nil&#125; 其中当前状态的计算，是依据当前状态来的。如果当前状态为已开启，则判断是否已经超时，超时就可以变更状态到半开；如果当前状态为关闭状态，则通过周期判断是否进入下一个周期。 1234567891011121314func (cb *CircuitBreaker) currentState(now time.Time) (State, uint64) &#123; switch cb.state &#123; case StateClosed: if !cb.expiry.IsZero() &amp;&amp; cb.expiry.Before(now) &#123; // 是否需要进入下一个计数周期 cb.toNewGeneration(now) &#125; case StateOpen: if cb.expiry.Before(now) &#123; // 熔断器由开启变更为半开 cb.setState(StateHalfOpen, now) &#125; &#125; return cb.state, cb.generation&#125; 周期长度的设定，也是以据当前状态来的。如果当前正常（熔断器关闭），则设置为一个interval 的周期；如果当前熔断器是开启状态，则设置为超时时间（超时后，才能变更为半开状态）。 请求之后的处理操作每次请求之后，会通过请求结果是否成功，对熔断器做计数。 123456789101112131415161718func (cb *CircuitBreaker) afterRequest(before uint64, success bool) &#123; cb.mutex.Lock() defer cb.mutex.Unlock() now := time.Now() // 如果不在一个周期，就不再计数 state, generation := cb.currentState(now) if generation != before &#123; return &#125; if success &#123; cb.onSuccess(state, now) &#125; else &#123; cb.onFailure(state, now) &#125;&#125; 如果在半开的状态下： 如果请求成功，则会判断当前连续成功的请求数 大于等于 maxRequests， 则可以把状态由半开状态转移为关闭状态 如果在半开状态下，请求失败，则会直接将半开状态转移为开启状态 如果在关闭状态下： 如果请求成功，则计数更新 如果请求失败，则调用readyToTrip 判断是否需要将状态关闭状态转移为开启状态 总结 对于频繁请求一些远程或者第三方的不可靠的服务，存在失败的概率还是非常大的。使用熔断器的好处就是可以是我们自身的服务不被这些不可靠的服务拖垮，造成雪崩。 由于熔断器里面，不仅会维护不少的统计数据，还有互斥锁做资源隔离，成本也会不少。 在半开状态下，可能出现请求过多的情况。这是由于半开状态下，连续请求成功的数量未达到最大请求值。所以，熔断器对于请求时间过长（但是比较频繁）的服务可能会造成大量的 too many requests 错误 微软云设计模式(https://www.microsoft.com/en-us/download/details.aspx?id=42026)","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"circuit-breaker","slug":"circuit-breaker","permalink":"http://blog.lpflpf.cn/tags/circuit-breaker/"}]},{"title":"Go Web 框架 Gin 路由的学习","slug":"gin-router","date":"2020-06-05T09:42:56.000Z","updated":"2021-01-11T06:41:06.545Z","comments":true,"path":"passages/gin-router/","link":"","permalink":"http://blog.lpflpf.cn/passages/gin-router/","excerpt":"本文主要从源码角度介绍 Gin 框架路由的实现。","text":"本文主要从源码角度介绍 Gin 框架路由的实现。 Gin 是目前应用比较广泛的Golang web 框架。 目前，Github Star 数已经达到了3.8w. 框架的实现非常简单，可定制性非常强，性能也比较好，深受golang开发者的喜爱。Gin 提供了web开发的一些基本功能。如路由，中间件，日志，参数获取等，本文主要从源码的角度分析Gin的路由实现。 Gin 的路由功能是基于 https://github.com/julienschmidt/httprouter 这个项目实现的。目前也有很多其他Web框架也基于该路由框架做了二次开发。 http 路由的接口在 Gin 中，为了兼容不同路由的引擎，定义了 IRoutes 和 IRouter 接口，便于替换其他的路由实现。（目前默认是httprouter) 下面是一个路由的接口定义 12345678910111213141516171819type IRoutes interface &#123; Use(...HandlerFunc) IRoutes Handle(string, string, ...HandlerFunc) IRoutes Any(string, ...HandlerFunc) IRoutes GET(string, ...HandlerFunc) IRoutes POST(string, ...HandlerFunc) IRoutes DELETE(string, ...HandlerFunc) IRoutes PATCH(string, ...HandlerFunc) IRoutes PUT(string, ...HandlerFunc) IRoutes OPTIONS(string, ...HandlerFunc) IRoutes HEAD(string, ...HandlerFunc) IRoutes StaticFile(string, string) IRoutes Static(string, string) IRoutes StaticFS(string, http.FileSystem) IRoutes&#125;type HandlerFunc func(*Context) HandlerFunc 是一个方法类型的定义，我们定义的路由其实就是一个路径与HandlerFunc 的映射关系。从上面的定义可以看出，IRoutes 主要定义了一些基于http方法、静态方法的路径和一组方法的映射。 Use 方法是针对此路由的所有路径映射一组方法，在使用上是为了给这些路由添加中间件。 除了上面的定义外，Gin 还有路由组的抽象。 1234type IRouter interface &#123; IRoutes Group(string, ...HandlerFunc) *RouterGroup&#125; 路由组是在IRoutes 的基础上，有了组的概念，组下面还可以挂在不同的组。组的概念可以很好的管理一组路由，路由组可以自己定义一套Handler方法（即一组中间件）。 个人认为IRouter的定义Group 应该返回 IRouter，这样可以把路由组更加抽象，也不会改变现有服务的使用。期待看下Gin源码什么时候会按照这种定义方法修改过来。 在Gin框架中，路由由 RouterGroup 实现。我们从构造和路由查找两个方面分析路由的实现。 路由实现路由的本质就是在给定 路径与Handler映射关系 的前提下，当提供新的url时，给出对应func 的过程。其中可能需要从url中提取参数，或者按照 * 匹配 url 的情况。 首先，我们看下Gin中路由结构的定义。 12345678910111213141516171819202122// gin enginetype Engine struct &#123; RouterGroup // ... 其他字段 trees methodTrees&#125;// 每个 http 方法定义一个森林type methodTrees []methodTreetype methodTree struct &#123; method string root *node&#125;// 路由组的定义type RouterGroup struct &#123; Handlers HandlersChain basePath string engine *Engine root bool&#125; 从定义中可以看出，其实Gin 的 Engine 是复用了 RouterGroup。对于不同的 http method，都通过一个森林来存储路由数据。下面是森林上每个节点的定义： 12345678910type node struct &#123; path string // 当前路径 indices string // 对应children 的前缀 wildChild bool // 可能是带参数的，或者是 * 的，所以是野节点 nType nodeType // 参数节点，静态节点 priority uint32 // 优先级 ，优先级高的放在children 放在前面。 children []*node // 子节点 handlers HandlersChain // 调用链 fullPath string // 全路径&#125; 从代码实现上得知，这个森林其实是一个压缩版本的Trie树，每个节点会存储前缀相同的路径数据。下面，我们通过代码来学习下路由的添加和删除。 路由的添加路由的添加，就是将path路径添加到定义的Trie树种，将handlers 添加到对应的node 节点。 123456789101112131415161718192021222324func (n *node) addRoute(path string, handlers HandlersChain) &#123; // 初始化和维护优先级 for &#123; // 查找前缀 i := longestCommonPrefix(path, n.path) // 原有路径长的情况下 // 节点n 的 path 变为了公共前缀 // 原有n 的path 路径变为了现有n 的子节点 // 当添加的path长的情况 // 需要分情况讨论： // 1. 如果是一个带参数的路径，校验是否后续路径不同，如果不同则继续扫描下一段路径 // 2. 如果是带 * 的路径， 则直接报错 // 3. 如果已经有对应的首字母，修改当前node节点，并继续扫描，并扫描下一段路径 // 4. 如果非参数或者 * 匹配的方法，则插入一个子节点路径，并完成扫描 // 最后注册handlers，添加fullPath n.handlers = handlers n.fullPath = fullPath return &#125;&#125; 从上面的代码注释可以看出，路由的添加，主要是通过不断对比当前节点的path和添加的path，做添加节点或者节点变更的操作，达到添加path的目的。 路径查找在服务请求时，路由的责任就是给定一个url请求，拿到节点保存的handlers，以及url中包含的参数值。下面是对一个url 的解析实现。 1234567891011121314151617181920type nodeValue struct &#123; handlers HandlersChain params *Params tsr bool fullPath string&#125;func (n *node) getValue(path string, params *Params, unescape bool) (value nodeValue) &#123;walk: // Outer loop for walking the tree for &#123; prefix := n.path// 如果比当前节点路径要长：// - 非参数类型或模糊匹配的URL，如果和当前节点前缀匹配，直接查看 node 的子节点// - 参数化的node, 按照 / 分割提取参数，如果未结束，则继续匹配剩下的路径，否则返回结果。// - * 匹配的node，将剩余的路径添加到 param 中直接返回。// 如果和当前节点相等，那就直接返回即可。// 这里还做了非本方法的路径匹配，用户返回http 方法错误的异常报告。 &#125;&#125; 一个例子下面通过一个例子，方便我们快速理解router的实现。 加入下面的一个路径： /search/ /support/ /blog/:post/ /about-us/team/ /contact/ 在树中,我们看到的样子如下： 1234567891011Path\\├s|├earch\\|└upport\\├blog\\| └:post| └\\├about-us\\| └team\\└contact\\ 在做路由查找时，通过路径不断匹配，找到对应的子节点。拿到对应子节点下的handler。完成路由的匹配。 总结 httprouter 没有实现了routergroup功能，只是实现了router 的功能，在gin中做了实现 通过Trie树实现路由是比较基础的一种实现方法，除了这种方法外，还可以考虑通过正则的方式提取路由。 Gin http 服务是基于 Go 的 net/http 库的， net/http 库中handler 的实现是针对不同的 http method 的，所以需要在engine 中针对不同的method 提供不同的trie 树。 在添加路由时，如果使用了 any 方法，则在每个http method 下都会添加一样的路径。 middleware 本质上只是一个 HandlerFunc.","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"gin","slug":"gin","permalink":"http://blog.lpflpf.cn/tags/gin/"},{"name":"http router","slug":"http-router","permalink":"http://blog.lpflpf.cn/tags/http-router/"}]},{"title":"Golang Net/rpc 包的学习和使用","slug":"net-rpc","date":"2020-06-01T01:56:41.000Z","updated":"2021-01-11T06:41:06.580Z","comments":true,"path":"passages/net-rpc/","link":"","permalink":"http://blog.lpflpf.cn/passages/net-rpc/","excerpt":"从 rpc 包的 Server 端 和 Client 端入手，学习 Server/Client 的源码实现。并以一个例子作为总结。最后总结了rpc实现的几个学习的要点。","text":"从 rpc 包的 Server 端 和 Client 端入手，学习 Server/Client 的源码实现。并以一个例子作为总结。最后总结了rpc实现的几个学习的要点。 Golang net/rpc 包学习golang 提供了一个开箱即用的RPC服务，实现方式简约而不简单。 RPC 简单介绍远程过程调用 (Remote Procedure Call，RPC) 是一种计算机通信协议。允许运行再一台计算机的程序调用另一个地址空间的子程序（一般是开放网络种的一台计算机），而程序员就像调用调用本地程序一样，无需额外的做交互编程。RPC 是一种 CS (Client-Server) 架构的模式，通过发送请求-接收响应的方式进行信息的交互。 有很多广泛使用的RPC框架，例如 gRPC, Thrift, Dubbo, brpc 等。这里的RPC 框架有的实现了跨语言调用，有的实现了服务注册发现等。比我们今天介绍的官方提供的 rpc 包要使用广泛的多。但是，通过对net/rpc的学习，可以使我们对一个rpc框架做一个最基本的了解。 Golang 的实现rpc 是 cs 架构，所以既有客户端，又有服务端。下面，我们先分析通信的编码，之后从服务端、客户端角度分析RPC的实现。 通信编码golang 在rpc 实现中，抽象了协议层，我们可以自定义协议实现我们自己的接口。如下是协议的接口： 1234567891011121314151617// 服务端type ServerCodec interface &#123; ReadRequestHeader(*Request) error ReadRequestBody(interface&#123;&#125;) error WriteResponse(*Response, interface&#123;&#125;) error // Close can be called multiple times and must be idempotent. Close() error&#125;// 客户端type ClientCodec interface &#123; WriteRequest(*Request, interface&#123;&#125;) error ReadResponseHeader(*Response) error ReadResponseBody(interface&#123;&#125;) error Close() error&#125; 而包中提供了基于gob 二进制编码的编解码实现。当然我们也可以实现自己想要的编解码方式。 Server 端实现结构定义 1234567type Server struct &#123; serviceMap sync.Map // 保存Service reqLock sync.Mutex // 读请求的锁 freeReq *Request respLock sync.Mutex // 写响应的锁 freeResp *Response&#125; server端通过互斥锁的方式支持了并发执行。由于每个请求和响应都需要定义Request/Response 对象，为了减少内存的分配，这里使用了一个freeReq/freeResp 链表实现了两个对象池。当需要Request 对象时，从 freeReq 链表中获取，当使用完毕后，再放回链表中。 服务的注册service保存在 Server 的 serviceMap 中，每个Service 的信息如下： 123456type service struct &#123; name string // 服务名 rcvr reflect.Value // 服务对象 typ reflect.Type // 服务类型 method map[string]*methodType // 注册方法&#125; 从上面可以看到，一个类型以及该类型的多个方法可以被注册为一个Service。在注册服务时，通过下面的方法将服务保存在serviceMap 中。 1234// 默认使用对象方法名func (server *Server) Register(rcvr interface&#123;&#125;) error &#123;&#125;// 指定方法名func (server *Server) RegisterName(name string, rcvr interface&#123;&#125;) error &#123;&#125; 服务的调用首先，是rpc 服务的启动。和大部分的网络应用一致，在accept一个连接后，会启动一个协程做消息处理，代码如下： 12345678for &#123; conn, err := lis.Accept() if err != nil &#123; log.Print(\"rpc.Serve: accept:\", err.Error()) return &#125; go server.ServeConn(conn)&#125; 其次，对于每一个连接，服务端会不断获取请求，并异步发送响应。代码如下： 123456789101112131415161718192021for &#123; // 读取请求 service, mtype, req, argv, replyv, keepReading, err := server.readRequest(codec) if err != nil &#123; if debugLog &amp;&amp; err != io.EOF &#123; log.Println(\"rpc:\", err) &#125; if !keepReading &#123; break &#125; if req != nil &#123; // 发送请求 server.sendResponse(sending, req, invalidRequest, codec, err.Error()) server.freeRequest(req) // 释放 req 对象 &#125; continue &#125; wg.Add(1) // 并发处理每个请求 go service.call(server, sending, wg, mtype, req, argv, replyv, codec)&#125; 最后，由于异步发送请求，所以请求的顺序和响应顺序不一定一致。所以，在响应报文中，会携带请求报文的seq （序列号），保证消息的一致性。除此之外，为了兼容http 服务，net/rpc 包还通过http包实现的 Hijack 方式，将 http 协议转换为 rpc 协议。代码如下： 123456789101112131415func (server *Server) ServeHTTP(w http.ResponseWriter, req *http.Request) &#123; // 客户端通过 CONNECT 方法连接 // 通过Hijack 拿到tcp 连接 conn, _, err := w.(http.Hijacker).Hijack() if err != nil &#123; log.Print(\"rpc hijacking \", req.RemoteAddr, \": \", err.Error()) return &#125; // 发送客户端，支持 RPC 协议 io.WriteString(conn, \"HTTP/1.0 \"+connected+\"\\n\\n\") // 开始 RPC 的请求响应 server.ServeConn(conn)&#125; Client 端实现客户端的连接相较于服务端是比较简单的。我们从发起连接、发送请求、读取响应三个角度学习。 RPC 的连接由于该RPC支持HTTP协议做连接升级，因此，有几种连接方式。 直接使用 tcp 协议。 1func Dial(network, address string) (*Client, error) &#123;&#125; 使用 http 协议。 http 协议可以指定路径，或者使用默认的rpc 路径。 1234// 默认路径 \"/_goRPC_\"func DialHTTP(network, address string) (*Client, error) &#123;&#125;// 使用默认的路径func DialHTTPPath(network, address, path string) (*Client, error) &#123;&#125; 请求的发送RPC 请求的发送，提供了同步和异步的接口调用，方式如下： 1234// 异步func (client *Client) Go(serviceMethod string, args interface&#123;&#125;, reply interface&#123;&#125;, done chan *Call) *Call &#123;&#125;// 同步func (client *Client) Call(serviceMethod string, args interface&#123;&#125;, reply interface&#123;&#125;) error&#123;&#125; 从内部实现可以知道，都是通过Go 异步的方式拿到返回数据。 下面，我们看内部如何实现请求的发送： 12345678910111213141516171819202122232425func (client *Client) send(call *Call) &#123; // 客户端正常的情况下 seq := client.seq client.seq++ // 请求的序列号 client.pending[seq] = call // 对请求进行编码，包括请求方法、参数。 // Encode and send the request. client.request.Seq = seq client.request.ServiceMethod = call.ServiceMethod // client 可以并发 发起 Request, 然后异步等待 Done err := client.codec.WriteRequest(&amp;client.request, call.Args) // 是否有发送失败，如果发送成功，则保存在pending map中，等待请求结果。 if err != nil &#123; client.mutex.Lock() call = client.pending[seq] delete(client.pending, seq) client.mutex.Unlock() if call != nil &#123; call.Error = err call.done() &#125; &#125;&#125; 从上面可以看出，对于一个客户端，可以同时发送多条请求，然后异步等待响应。 读取响应在rpc 连接成功后，会建立一个连接，专门用于做响应的读取。 1234567891011121314151617for err == nil &#123; response = Response&#123;&#125; err = client.codec.ReadResponseHeader(&amp;response) if err != nil &#123; break &#125; seq := response.Seq client.mutex.Lock() call := client.pending[seq] // 从 pending 列表中删除 delete(client.pending, seq) client.mutex.Unlock() // 解码body // 此处有多种判断，判断是否有异常 client.codec.ReadResponseBody(nil) // 最后通知异步等待的请求，调用完成 call.done()&#125; 通过循环读取响应头，响应body，并将读取结果通知调用rpc 的异步请求，完成一次响应的读取。 简单例子下面我们官方提供的一个简单例子，对rpc包学习做个总结。 服务端 123456789101112131415161718192021222324252627282930313233type Args struct &#123; // 请求参数 A, B int&#125;type Quotient struct &#123; // 一个响应的类型 Quo, Rem int&#125;type Arith int// 定义了乘法和除法func (t *Arith) Multiply(args *Args, reply *int) error &#123; *reply = args.A * args.B return nil&#125;func (t *Arith) Divide(args *Args, quo *Quotient) error &#123; if args.B == 0 &#123; return errors.New(\"divide by zero\") &#125; quo.Quo = args.A / args.B quo.Rem = args.A % args.B return nil&#125;func main() &#123; serv := rpc.NewServer() arith := new(Arith) serv.Register(arith) // 服务注册 // 通过http 监听，到时做协议转换 http.ListenAndServe(\"0.0.0.0:3000\", serv)&#125; 客户端 12345678910111213141516171819202122232425262728func main() &#123; client, err := rpc.DialHTTP(\"tcp\", \"127.0.0.1:3000\") if err != nil &#123; log.Fatal(\"dialing:\", err) &#125; dones := make([]chan *rpc.Call, 0, 10) // 先同步发起请求 for i := 0; i &lt; 10; i++ &#123; quotient := new(Quotient) args := &amp;Args&#123;i + 10, i&#125; divCall := client.Go(\"Arith.Divide\", args, quotient, nil) dones = append(dones, divCall.Done) log.Print(\"send\", i) &#125; log.Print(\"---------------\") // 之后异步读取 for idx, done := range dones &#123; replyCall := &lt;-done // will be equal to divCall args := replyCall.Args.(*Args) reply := replyCall.Reply.(*Quotient) log.Printf(\"%d / %d = %d, %d %% %d = %d\\n\", args.A, args.B, reply.Quo, args.A, args.B, reply.Rem) log.Print(\"recv\", idx) &#125;&#125; 我们可以学到什么最后，做一个学习的总结。 对统一连接上的不同请求实现异步操作，通过请求、响应需要保证数据的一致性。 链表方式实现一个对象池 对 http 包中实现的Hijack 方式的一次简单实践，通过http协议升级为rpc协议。劫持了原有http协议的tcp连接，转为rpc使用。 rpc 的实现，通过gob编码，应该是不支持与其他语言通信的。需要自己实现编解码方式。 rpc 的实现，也不支持服务的注册和发现，需要我们自己去维护服务方。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"net/rpc","slug":"net-rpc","permalink":"http://blog.lpflpf.cn/tags/net-rpc/"}]},{"title":"Golang Http 学习（二） Http Client 的实现","slug":"golang-http-2","date":"2020-06-01T01:50:58.000Z","updated":"2021-01-11T06:41:06.550Z","comments":true,"path":"passages/golang-http-2/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-http-2/","excerpt":"golang http Client 的实现, 从 源码入手， 总结Client 的实现方式。","text":"golang http Client 的实现, 从 源码入手， 总结Client 的实现方式。 众所周知，在golang 中实现的 http client 是自带连接池的。当我们做 http 请求时，极有可能就是复用了之前建立的 tcp 连接。那这个连接池是如何实现的，今天我们一起来探究。 请求操作一个http 的请求操作，核心操作是通过构造一个 Request 对象，然后返回一个 Response 对象。在 http 包中，http 的server 实现与client 的实现共用了Request/Response 对象。在 http client 中，我们通过构造Request，发起请求，并通过读取的数据构造Response 对象，返回给客户端的使用者；而在Server端，通过读取网络数据，通过数据头构造 Request 对象，并将响应数据放入 Response 对象中；通过将 Response 对象写入网络连接中，实现一次HTTP的交互。 在http client 的实现时，所有类型的http请求，均来自于如下方法： 1func (c *Client) do(req *Request) (retres *Response, reterr error) &#123;&#125; do 方法中，考虑了重定向问题，以及请求cookie携带的相关问题。而最终发送 request 到获取 response ，来自于 RoundTriper 接口。该接口中仅有一个方法，是用来实现 Request 到 Response 转换的: 123type RoundTripper interface &#123; RoundTrip(*Request) (*Response, error)&#125; Transport 是我们最常用的 RoundTripper 接口的实现，它实现了http连接池的管理，连接的请求复用，并且是协程安全的。如果我们不指定，默认情况下 http 请求是使用 Transport 的实例 DefaultTransport 作为我们的 RoundTripper. 在 RoundTrip 中，抽象看来，主要有几个阶段： 拿到一个连接 发送Request，读取Response 将连接返回给连接池 因此，下面我们从连接的管理维护、请求和响应的读写操作两个方面学习。 连接管理维护连接的管理，Transport 中主要用到了如下的几个容器： 12345678910// 保存连接池， 按照Key 区分连接池idleConn map[connectMethodKey][]*persistConn// 等待连接的队列idleConnWait map[connectMethodKey]wantConnQueue// 空闲连接的LRU，用于删除最近未使用的连接idleLRU connLRU// 保存每个host 目前的连接数connsPerHost map[connectMethodKey]int// 当前等待 Dial 的连接数connsPerHostWait map[connectMethodKey]wantConnQueue 从上述的几个容器可以看到，主要保存了当前正在使用的连接池，当前正在等待连接的队列，以及当前通过Dial 请求连接的池子等。这些容器使用的维度为connectMethodKey. 这个结构的定义如下： 123456type connectMethodKey struct &#123; // 代理，scheme，地址， proxy, scheme, addr string // 是否仅为 http1 onlyH1 bool&#125; 可以看出，对于同一个connectMethodKey, 才会使用同一个连接池。下面我们从获取一个连接开始，学习如何维护这个连接池。下面是获取连接的流程图： 对于非Keeplive 的请求，则直接发起 Dial，不会复用连接。 从上面的流程图中可以看到，我们的 wantConn 会放入两个队列 idleConnWait, connsPerHostWait。 当阻塞拿去连接时，如果有连接释放或者有新的连接成功连接，都会使我们拿到一个空闲连接。 如果 Response 的 Body 关闭后，连接的读通道关闭，正常情况下会放入idleConn 连接池中。 如果中间出现异常情况。例如：读操作失败，或者请求操作失败，该连接将不再被复用。 如果在返回连接后，我们已经从idleConn 中拿到了一个连接，则返回后的连接将顺理成章的放入到空闲队列中。 在创建一个新的连接后，会启用两个新的goroutine： readLoop, writeLoop，用于连接的读操作和写操作。下面我们看看请求的读和写。 读写操作http 请求，在同一个时刻是半双工的，要么是请求数据，要么是读取访问。在实现时，将读操作和写操作分别放在了不同的goroutine中，下面是一个从请求到Response 读取完成的时序图： 从图中可以看出，整体操作分为如下几个步骤： 传递一个 WriteRequest 对象至WriteLoop中，将请求通过连接发送到远端。 同时会发送一个读 Response 的消息至readLoop，readLoop 开始阻塞读取远程数据 读取成功数据后，readLoop 协程中将Response 返回至调用方。 当关闭了Response 的Body 后，将通知readLoop。 写成功后，会发送写成功的消息至readLoop, 告知该连接是正常的，可以继续复用。 此时开始连接将复用，继续等待开始读的事件。（即 将连接收回至空闲连接池中，等待被重新触发请求） 总结http 的连接池的实现就简单的介绍到这里。从上面连接池的实现，对我们使用时也有很多的启发： 尽可能早的关闭 Response 的Body， 方便做连接的回收。 连接池使用时，可以充分使用同一个Transport，使我们可以充分连接池。 结合使用场景，在Transport 中设置空闲连接的超时时间，最大空闲连接数量，每个连接的最大连接数等值。 由于代码较多，这里不从代码角度分析。可以参考 “https://github.com/lpflpf/go” 中的注释。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"}]},{"title":"Golang Http 学习（一） Http Server 的实现","slug":"golang-http-1","date":"2020-05-11T03:06:40.000Z","updated":"2021-01-11T06:41:06.550Z","comments":true,"path":"passages/golang-http-1/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-http-1/","excerpt":"Golong Http 包中，对Http Server 实现的学习和理解。","text":"Golong Http 包中，对Http Server 实现的学习和理解。 Http 服务是基于 Tcp 的应用层的实现，也是我们常见的网络协议之一。go 语言提供了较为丰富的http协议的实现包 net/http 包。http 是典型的C/S 架构（也是B/S架构），我们先从Server端入手，看看Http Server 是如何实现的。 请求连接的管理golang 中， 连接的管理采用的是 Reactor 模式。每个请求到达服务器之后，都会分配一个 goroutine 做任务处理。 12345678910111213141516171819202122func (srv *Server) Serve(l net.Listener) error &#123; // ... 初始化和验证listener // ... 构造 context for &#123; rw, e := l.Accept() if e != nil &#123; select &#123; case &lt;-srv.getDoneChan(): return ErrServerClosed default: &#125; // ... 若为临时错误，启动重试机制 // 否则退出 &#125; tempDelay = 0 c := srv.newConn(rw) c.setState(c.rwc, StateNew) // 创建goroutine, 单独处理连接 go c.serve(ctx) &#125;&#125; 我们在处理 http 请求时，不同请求在不同goroutine中，需要注意并发请求数据共享的问题。 连接的状态Server 在Accept 后创建连接（conn)，连接可能有多种状态。通过连接的状态转移，可以方便我们了解一个conn 的处理流程。下面是状态的转移图： 当Accept后，构建了新的连接，状态将标记为New。如果可以读取数据，连接将标记为Active（即，活动的Conn）。作为一个活动的Conn，可能在处理完毕后变为Idle状态用于请求复用；也有可能因为请求协议故障，变为Close状态；也有可能被服务调用方直接管理Conn，状态变更为Hijacked 状态。 Hijacked 状态下，Conn 被使用方自行管理，一般用于协议升级的情况。例如：通过http 请求后，协议升级为websocket 请求，或者Rpc 请求等。 连接的处理做http 的连接处理，重点有几个方面：① 通过连接读取数据，并做协议分析和处理；②对http请求做处理（我们正常需要做的业务处理）；③ 连接的复用和升级。 首先，我们看看整体的处理流程： 123456789101112131415161718192021222324252627282930313233343536373839404142// Serve a new connection.func (c *conn) serve(ctx context.Context) &#123; // ... defer 处理异常退出 和连接关闭 // ... tls 握手 // 初始化conn 的读写 // 对于keeplive 循环处理请求 for &#123; // ①② 读取/处理请求头 构造了Request，Response w, err := c.readRequest(ctx) // ... 连接状态变更 &amp;&amp; 异常处理 // 对 Except 100-continue 的特殊处理。 // 原子包裹 response 对象 c.curReq.Store(w) // 异步读取Body （此处也有对 except 100 的处理) // ③ 传入 request 和 response 处理 handler serverHandler&#123;c.server&#125;.ServeHTTP(w, w.req) // 连接复用判断， 复用则退出 // 请求结束，写header 和resp，并关闭req w.finishRequest() if !w.shouldReuseConnection() &#123; if w.requestBodyLimitHit || w.closedRequestBodyEarly() &#123; c.closeWriteAndWait() &#125; return &#125; // 更改状态，释放 response // 如果不需要keeplive, 连接将被关闭 if !w.conn.server.doKeepAlives() &#123; return &#125; // 判断是否超时，连接是否可用， 若不可用则关闭 // 重新设置超时 c.rwc.SetReadDeadline(time.Time&#123;&#125;) &#125;&#125; 从代码中可以看出，除了需要做Http 的解析外，还需要不断判断Conn 的状态。当进入Hijack状态后，不再控制Conn；当连接异常后，不再处理请求；当keeplive后，需要复用连接；超时之后，对连接的关闭等。此外，还需要对http 协议做适配处理，例如 对 Except: 100-continue的支持等。 对于每个请求，我们都会有一个 Request 和 Response 对象，分别标识一个请求和响应。从Request 中读取请求Body，将我们的响应写入Response对象中。下面我们来看看Server端是如何构造这两个对象的。 Request 的构造 首先是对协议头的解析,获取请求的方法、请求Url，协议等，如果是代理模式，还会做Url的替换。 然后会解析Header，在Server 中，Golang 的Header 数据是存储在 map[string][]string 结构中，Key 采用大驼峰和连字符描述。 对于Pragma：no-cache 的请求，标识 Cache-control：No-cache 对于Connection: close 的请求，不再keeplive 构造 Request 传输控制的数据： Transfer-Encoding 的修正 Content-Length 的修正 chunk 模式下的Trailer修正 Body 的构造 PRI header 对Http2的支持。（需要通过HiJack 支持） Response 的构造Response 作为服务的响应节点，比较简单，初始化后，创建一个写缓冲区即可： 12345678910111213141516171819w = &amp;response&#123; conn: c, cancelCtx: cancelCtx, req: req, reqBody: req.Body, handlerHeader: make(Header), contentLength: -1, closeNotifyCh: make(chan bool, 1), // We populate these ahead of time so we're not // reading from req.Header after their Handler starts // and maybe mutates it (Issue 14940) wants10KeepAlive: req.wantsHttp10KeepAlive(), wantsClose: req.wantsClose(),&#125;w.cw.res = w // chunkWriter//创建 一个写的缓冲区,(Writer 从 sync.Pool 中共享)w.w = newBufioWriterSize(&amp;w.cw, bufferBeforeChunkingSize) Handler 的处理Http Server 是为了我们的业务处理服务的。在构造了Request 和 Response 对象后，最终的目的就是为了处理我们的业务逻辑。 在 http Server 中， 构造了 serverHandler 对象完成我们的业务逻辑， serverHandler 中，调用handler.ServerHTTP 方法，我们业务逻辑需要定义一个Handler，handler实现 ServerHTTP 方法即可。 12345678910111213141516171819type Handler interface &#123; ServeHTTP(ResponseWriter, *Request)&#125;type serverHandler struct &#123; srv *Server&#125;func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) &#123; handler := sh.srv.Handler if handler == nil &#123; handler = DefaultServeMux &#125; if req.RequestURI == \"*\" &amp;&amp; req.Method == \"OPTIONS\" &#123; handler = globalOptionsHandler&#123;&#125; &#125; handler.ServeHTTP(rw, req)&#125; http 包中也通过实现 Handler 接口 提供了一些基础的结构体方便我们使用。例如： ServeMux 结构体。这个ServerMux 实现了可以定义路径和Handler映射（简单路由）功能的 Handler，方便我们定义路由。内部还定义了一个默认的DefaultServeMux，我们可以通过如下方法做默认路由的映射(可以从上面方法看到，如果没有定义handler，将使用DefaultServeMux)： Handle(pattern string, handler Handler){} HandleFunc(pattern string, handler func(ResponseWrite, *Request)){} timeoutHandler 结构体。通过 NewTestTimeoutHandler 方法可以构造一个带超时功能的handler。 redirectHandler 结构体。通过 RedirectHandler 方法可以实现对连接的重定向。 fileHandler 结构体。通过 FileServer(root FileSystem) 方法可以构造一个fileHandler 结构体，从而实现一个文件服务器。 总结 一个 Http 请求，至少会启动两个goroutine。一个groutine用来处理请求，另一个goroutine 用来异步读取body 数据。 Server 实现中，对协议升级做了充分的考虑。可以通过 Hijack 手段, 将我们的协议从 Http 升级为 WebSocket, RPC，或者其他TCP协议。 几个比较特殊的 Http 协议规则还有一些http 协议规则的实现，我们在后续的文章做仔细的分析。例如： Http Except: 100-continue 协议 Http CONNECT METHOD, 不仅会用在代理模式的Http Server中，还有可能用在RPC中。 Chunk 模式， Trailer 设置。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"http","slug":"http","permalink":"http://blog.lpflpf.cn/tags/http/"}]},{"title":"Golang Net 包学习","slug":"golang-net","date":"2020-05-06T06:15:48.000Z","updated":"2021-01-11T06:41:06.576Z","comments":true,"path":"passages/golang-net/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-net/","excerpt":"golang 的 net 包，相关接口和结构比较多，今天做个简单的梳理。","text":"golang 的 net 包，相关接口和结构比较多，今天做个简单的梳理。 网络模型在总结 net 包之前，还需要温习模糊的网络模型知识。下图是大学课本上的网络模型图： 模型图中可以看到，OSI 的七层模型，每一层实现的是与对端相应层的通信接口。但是实际应用中，我们把会话层、表示层、应用层统称为应用层。因此，就变成了TCP/IP 的五层模型。 其中网络层包含了 ip,arp,icmp 等协议，传输层包含了 TCP， UDP 等协议，应用层，比如 SMTP，DNS，HTTP 等协议。在 net 包中，主要涉及网络层和传输层的协议。支持如下：网络层： ICMP IGMP IVP6-ICMP 传输层： TCP UDP Socket 编程在讲代码结构前，还需要回忆（学习）几个 Socket 编程(套接字编程)的知识点。 在 Linux 上一切皆文件。所以各端口的读写服务可以认为是读取/写入文件, 一般使用文件描述符 fd (file descriptor) 表示。在Windows上，各端口的读写服务是一个通信链的句柄操作，通过句柄实现网络发出请求和读取数据。在 go 中为了统一，采用 linux 的 fd 代表一个链接节点。 TCP 是面向连接的、可靠的流协议，可以理解为不断从文件中读取数据（STREAM）。UDP 是无链接的、面向报文的协议，是无序，不可靠的（DGRAM）（目前很多可靠的协议都是基于UDP 开发的）。 UNIXDomain Socket 是一种 进程间通信的协议，之前仅在*nix上使用，17年 17063 版本后支持了该协议。虽然是一个 IPC 协议，但是在实现上是基于套接字 (socket) 实现的。因此，UNIXDomain Socket 也放在了net 包中。 unixDomain Socket 也可以选择采用比特流的方式，或者无序的，不可靠的通讯方式，有序数据包的方式（SEQPACKET, Linux 2.6 内核才支持） 代码结构下面我们看看 net 包中一些接口，以及一些接口的实现。 从图中可以看出，基于 TCP、UDP、IP、Unix （Stream 方式）的链接抽象出来都是 Conn 接口。基于包传递的 UDP、IP、UnixConn （DGRAM 包方式） 都实现了 PacketConn 接口。对于面向流的监听器，比如： TCPListener、 UnixListener 都实现了 Listener 接口。 整体上可以看出，net 包对网络链接是基于我们复习的网络知识实现的。对于代码的底层实现，也是比较简单的。正对不同的平台，调用不同平台套接字的系统调用即可。直观上看，对于不同的链接，我们都是可以通过Conn 的接口来做网络io的交互。 如何使用在了解了包的构成后，我们基于不同的网络协议分两类来学习如何调用网络包提供的方法。 基于流的协议基于流的协议，net 包中支持了常见的 TCP，Unix （Stream 方式） 两种。基于流的协议需要先于对端建立链接，然后再发送消息。下面是 Unix 套接字编程的一个流程： 首先，服务端需要绑定并监听端口，然后等待客户端与其建立链接，通过 Accept 接收到客户端的连接后，开始读写消息。最后，当服务端收到EOF标识后，关闭链接即可。 HTTP, SMTP 等应用层协议都是使用的 TCP 传输层协议。 基于包的协议基于包的协议，net 包中支持了常见的 UDP，Unix （DGRAM 包方式，PacketConn 方式），Ip (网络层协议，支持了icmp, igmp) 几种。基于包的协议在bind 端口后，无需建立连接，是一种即发即收的模式。 基于包的协议，例如基于UDP 的 DNS解析， 文件传输（TFTP协议）等协议，在网络层应该都是基于包的协议。 下面是基于包请求的Server 端和Client端： 可以看到，在Socket 编程里， 基于包的协议是不需要 Listen 和 Accept 的。在 net 包中，使用ListenPacket，实际上仅是构造了一个UDP连接，做了端口绑定而已。端口绑定后，Server 端开始阻塞读取包数据，之后二者开始通信。由于基于包协议，因此，我们也可以采用PacketConn 接口（看第一个实现接口的图）构造UDP包。 一个简单的例子下面，我们构造一个简单的 Redis Server （支持多线程），实现了支持Redis协议的简易Key-Value操作（可以使用Redis-cli直接验证）: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package mainimport ( \"bufio\" \"fmt\" \"io\" \"net\" \"strconv\" \"strings\" \"sync\")var KVMap sync.Mapfunc main() &#123; // 构造一个listener listener, _ := net.Listen(\"tcp\", \"127.0.0.1:6379\") defer func() &#123; _ = listener.Close() &#125;() for &#123; // 接收请求 conn, _ := listener.Accept() // 连接的处理 go FakeRedis(conn) &#125;&#125;// 这里做了io 读写操作，并解析了 Redis 的协议func FakeRedis(conn net.Conn) &#123; defer conn.Close() reader := bufio.NewReader(conn) for &#123; data, _, err := reader.ReadLine() if err == io.EOF &#123; return &#125; paramCount, _ := strconv.Atoi(string(data[1:])) var params []string for i := 0; i &lt; paramCount; i++ &#123; _, _, _ = reader.ReadLine() // 每个参数的长度，这里忽略了 sParam, _, _ := reader.ReadLine() params = append(params, string(sParam)) &#125; switch strings.ToUpper(params[0]) &#123; case \"GET\": if v, ok := KVMap.Load(params[1]); !ok &#123; conn.Write([]byte(\"$-1\\r\\n\")) &#125; else &#123; conn.Write([]byte(fmt.Sprintf(\"$%d\\r\\n%v\\r\\n\", len(v.(string)), v))) &#125; case \"SET\": KVMap.Store(params[1], params[2]) conn.Write([]byte(\"+OK\\r\\n\")) case \"COMMAND\": conn.Write([]byte(\"+OK\\r\\n\")) &#125; &#125;&#125; 上述代码没有任何的异常处理，仅作为网络连接的一个简单例子。从代码中可以看出，我们的数据流式的网络协议，在建立连接后，可以和文件IO服务一样，可以任意的读写操作。正常情况下，流处理的请求，都会开启一个协程来做连接处理，主协程仅用来接收连接请求。(基于包的网络协议则可以不用开启协程处理) 总结 基于 Conn 的消息都是有三种过期时间，这其实是在底层epoll_wait中设置的超时时间。 Deadline 设置了Dail中建立连接的超时时间， ReadDeadline 是 Read 操作的超时时间， WriteDeadline 为 Write 操作的超时时间。 net 包作为基础包，基于net开发应用层协议比较多，例如 net/http, net/rpc/smtp 等。 网络的io操作底层是基于epoll来实现的, unixDomain 基于文件来实现的。 net 包实现的套接字编程仅是我们日常生活中用的比较多的一些方法，还有很多未实现的配置待我们去探索。 网络模型比较简单，实际用起来，还是需要分门别类的。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"net","slug":"net","permalink":"http://blog.lpflpf.cn/tags/net/"}]},{"title":"Golang 容器的学习与实践","slug":"golang-container","date":"2020-05-03T02:42:54.000Z","updated":"2021-01-11T06:41:06.547Z","comments":true,"path":"passages/golang-container/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-container/","excerpt":"golang 提供了几个简单的容器供我们使用，本文在介绍几种Golang 容器的基础上，实现一个基于Golang 容器的LRU算法。","text":"golang 提供了几个简单的容器供我们使用，本文在介绍几种Golang 容器的基础上，实现一个基于Golang 容器的LRU算法。 容器介绍Golang 容器位于 container 包下，提供了三种包供我们使用，heap、list、ring. 下面我们分别学习。 heapheap 是一个堆的实现。一个堆正常保证了获取/弹出最大（最小）元素的时间为log n、插入元素的时间为log n.golang的堆实现接口如下： 123456// src/container/heap.gotype Interface interface &#123;sort.InterfacePush(x interface&#123;&#125;) // add x as element Len()Pop() interface&#123;&#125; // remove and return element Len() - 1.&#125; heap 是基于 sort.Interface 实现的。 12345678910// src/sort/type Interface interface &#123;// Len is the number of elements in the collection.Len() int// Less reports whether the element with// index i should sort before the element with index j.Less(i, j int) bool// Swap swaps the elements with indexes i and j.Swap(i, j int)&#125; 因此，如果要使用官方提供的heap，需要我们实现如下几个接口： 12345Len() int &#123;&#125; // 获取元素个数Less(i, j int) bool &#123;&#125; // 比较方法Swap(i, j int) // 元素交换方法Push(x interface&#123;&#125;)&#123;&#125; // 在末尾追加元素Pop() interface&#123;&#125; // 返回末尾元素 然后在使用时，我们可以使用如下几种方法： 12345678910// 初始化一个堆func Init(h Interface)&#123;&#125;// push一个元素倒堆中func Push(h Interface, x interface&#123;&#125;)&#123;&#125;// pop 堆顶元素func Pop(h Interface) interface&#123;&#125; &#123;&#125;// 删除堆中某个元素，时间复杂度 log nfunc Remove(h Interface, i int) interface&#123;&#125; &#123;&#125;// 调整i位置的元素位置（位置I的数据变更后）func Fix(h Interface, i int)&#123;&#125; list 链表list 实现了一个双向链表，链表不需要实现heap 类似的接口，可以直接使用。 链表的构造和使用： 123456789101112131415161718192021222324252627282930// 返回一个链表对象func New() *List &#123;&#125;// 返回链表的长度func (l *List) Len() int &#123;&#125;// 返回链表中的第一个元素func (l *List) Front() *Element &#123;&#125;// 返回链表中的末尾元素func (l *List) Back() *Element &#123;&#125;// 移除链表中的某个元素func (l *List) Remove(e *Element) interface&#123;&#125; &#123;&#125;// 在表头插入值为 v 的元素func (l *List) PushFront(v interface&#123;&#125;) *Element &#123;&#125;// 在表尾插入值为 v 的元素func (l *List) PushBack(v interface&#123;&#125;) *Element &#123;&#125;// 在mark之前插入值为v 的元素func (l *List) InsertBefore(v interface&#123;&#125;, mark *Element) *Element &#123;&#125;// 在mark 之后插入值为 v 的元素func (l *List) InsertAfter(v interface&#123;&#125;, mark *Element) *Element &#123;&#125;// 移动e某个元素到表头func (l *List) MoveToFront(e *Element) &#123;&#125;// 移动e到队尾func (l *List) MoveToBack(e *Element) &#123;&#125;// 移动e到mark之前func (l *List) MoveBefore(e, mark *Element) &#123;&#125;// 移动e 到mark 之后func (l *List) MoveAfter(e, mark *Element) &#123;&#125;// 追加到队尾func (l *List) PushBackList(other *List) &#123;&#125;// 将链表list放在队列前func (l *List) PushFrontList(other *List) &#123;&#125; 我们可以通过 Value 方法访问 Element 中的元素。除此之外，我们还可以用下面方法做链表遍历： 1234// 返回下一个元素func (e *Element) Next() *Element &#123;&#125;// 返回上一个元素func (e *Element) Prev() *Element &#123;&#125; 队列的遍历： 1234// l 为队列，for e := l.Front(); e != nil; e = e.Next() &#123; //通过 e.Value 做数据访问&#125; ring 循环列表container 中的循环列表是采用链表实现的。 12345678910111213141516// 构造一个包含N个元素的循环列表func New(n int) *Ring &#123;&#125;// 返回列表下一个元素func (r *Ring) Next() *Ring &#123;&#125;// 返回列表上一个元素func (r *Ring) Prev() *Ring &#123;&#125;// 移动n个元素 （可以前移，可以后移）func (r *Ring) Move(n int) *Ring &#123;&#125;// 把 s 链接到 r 后面。如果s 和r 在一个ring 里面，会把r到s的元素从ring 中删掉func (r *Ring) Link(s *Ring) *Ring &#123;&#125;// 删除n个元素 （内部就是ring 移动n个元素，然后调用Link)func (r *Ring) Unlink(n int) *Ring &#123;&#125;// 返回Ring 的长度，时间复杂度 nfunc (r *Ring) Len() int &#123;&#125;// 遍历Ring，执行 f 方法 （不建议内部修改ring）func (r *Ring) Do(f func(interface&#123;&#125;)) &#123;&#125; 访问Ring 中元素，直接 Ring.Value 即可。 容器的使用LRU 算法 (Least Recently Used)，在做缓存置换时用的比较多。逐步淘汰最近未使用的cache，而使我们的缓存中持续保持着最近使用的数据。下面，我们通过map 和 官方包中的双向链表实现一个简单的lru 算法，用来熟悉golang 容器的使用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package mainimport \"fmt\"import \"container/list\"// lru 中的数据type Node struct &#123; K, V interface&#123;&#125;&#125;// 链表 + maptype LRU struct &#123; list *list.List cacheMap map[interface&#123;&#125;]*list.Element Size int&#125;// 初始化一个LRUfunc NewLRU(cap int) *LRU &#123; return &amp;LRU&#123; Size: cap, list: list.New(), cacheMap: make(map[interface&#123;&#125;]*list.Element, cap), &#125;&#125;// 获取LRU中数据func (lru *LRU) Get(k interface&#123;&#125;) (v interface&#123;&#125;, ret bool) &#123; // 如果存在，则把数据放到链表最前面 if ele, ok := lru.cacheMap[k]; ok &#123; lru.list.MoveToFront(ele) return ele.Value.(*Node).V, true &#125; return nil, false&#125;// 设置LRU中数据func (lru *LRU) Set(k, v interface&#123;&#125;) &#123; // 如果存在，则把数据放到最前面 if ele, ok := lru.cacheMap[k]; ok &#123; lru.list.MoveToFront(ele) ele.Value.(*Node).V = v // 更新数据值 return &#125; // 如果数据是满的，先删除数据，后插入 if lru.list.Len() == lru.Size &#123; last := lru.list.Back() node := last.Value.(*Node) delete(lru.cacheMap, node.K) lru.list.Remove(last) &#125; ele := lru.list.PushFront(&amp;Node&#123;K: k, V: v&#125;) lru.cacheMap[k] = ele&#125; 其他 上述的容器都不是goroutines 安全的 上面的lr 也不是goroutines 安全的 Ring 中不建议在Do 方法中修改Ring 的指针，行为是未定义的","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"container","slug":"container","permalink":"http://blog.lpflpf.cn/tags/container/"}]},{"title":"Golang Context","slug":"golang-context","date":"2020-04-30T03:15:09.000Z","updated":"2021-09-30T08:02:45.850Z","comments":true,"path":"passages/golang-context/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-context/","excerpt":"本文让我们一起来学习 golang Context 的使用和标准库中的Context的实现。","text":"本文让我们一起来学习 golang Context 的使用和标准库中的Context的实现。 golang context 包 一开始只是 Google 内部使用的一个 Golang 包，在 Golang 1.7的版本中正式被引入标准库。下面开始学习。 简单介绍在学习 context 包之前，先看几种日常开发中经常会碰到的业务场景： 业务需要对访问的数据库，RPC ，或API接口，为了防止这些依赖导致我们的服务超时，需要针对性的做超时控制。 为了详细了解服务性能，记录详细的调用链Log。 上面两种场景在web中是比较常见的，context 包就是为了方便我们应对此类场景而使用的。 接下来, 我们首先学习 context 包有哪些方法供我们使用；接着举一些例子，使用 context 包应用在我们上述场景中去解决我们遇到的问题；最后从源码角度学习 context 内部实现，了解 context 的实现原理。 Context 包Context 定义context 包中实现了多种 Context 对象。Context 是一个接口，用来描述一个程序的上下文。接口中提供了四个抽象的方法，定义如下： 123456type Context interface &#123; Deadline() (deadline time.Time, ok bool) Done() &lt;-chan struct&#123;&#125; Err() error Value(key interface&#123;&#125;) interface&#123;&#125;&#125; Deadline() 返回的是上下文的截至时间，如果没有设定，ok 为 false Done() 当执行的上下文被取消后，Done返回的chan就会被close。如果这个上下文不会被取消，返回nil Err() 有几种情况: 如果Done() 返回 chan 没有关闭，返回nil 如果Done() 返回的chan 关闭了， Err 返回一个非nil的值，解释为什么会Done() 如果Canceled，返回 “Canceled” 如果超过了 Deadline，返回 “DeadlineEsceeded” Value(key) 返回上下文中 key 对应的 value 值 Context 构造为了使用 Context，我们需要了解 Context 是怎么构造的。 Context 提供了两个方法做初始化： 12func Background() Context&#123;&#125;func TODO() Context &#123;&#125; 上面方法均会返回空的 Context，但是 Background 一般是所有 Context 的基础，所有 Context 的源头都应该是它。TODO 方法一般用于当传入的方法不确定是哪种类型的 Context 时，为了避免 Context 的参数为nil而初始化的 Context。 其他的 Context 都是基于已经构造好的 Context 来实现的。一个 Context 可以派生多个子 context。基于 Context 派生新Context 的方法如下： 123func WithCancel(parent Context) (ctx Context, cancel CancelFunc)&#123;&#125;func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) &#123;&#125;func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) &#123;&#125; 上面三种方法比较类似，均会基于 parent Context 生成一个子 ctx，以及一个 Cancel 方法。如果调用了cancel 方法，ctx 以及基于 ctx 构造的子 context 都会被取消。不同点在于 WithCancel 必需要手动调用 cancel 方法，WithDeadline 可以设置一个时间点，WithTimeout 是设置调用的持续时间，到指定时间后，会调用 cancel 做取消操作。 除了上面的构造方式，还有一类是用来创建传递 traceId， token 等重要数据的 Context。 1func WithValue(parent Context, key, val interface&#123;&#125;) Context &#123;&#125; withValue 会构造一个新的context，新的context 会包含一对 Key-Value 数据，可以通过Context.Value(Key) 获取存在 ctx 中的 Value 值。 通过上面的理解可以直到，Context 是一个树状结构，一个 Context 可以派生出多个不一样的Context。我们大概可以画一个如下的树状图： 一个background，衍生出一个带有traceId的valueCtx，然后valueCtx衍生出一个带有cancelCtx 的context。最终在一些db查询，http查询，rpc沙逊等异步调用中体现。如果出现超时，直接把这些异步调用取消，减少消耗的资源，我们也可以在调用时，通过Value 方法拿到traceId，并记录下对应请求的数据。 当然，除了上面的几种 Context 外，我们也可以基于上述的 Context 接口实现新的Context. 使用方法下面我们举几个例子，学习上面讲到的方法。 超时查询的例子在做数据库查询时，需要对数据的查询做超时控制，例如： 12ctx = context.WithTimeout(context.Background(), time.Second)rows, err := pool.QueryContext(ctx, \"select * from products where id = ?\", 100) 上面的代码基于 Background 派生出一个带有超时取消功能的ctx，传入带有context查询的方法中，如果超过1s未返回结果，则取消本次的查询。使用起来非常方便。为了了解查询内部是如何做到超时取消的，我们看看DB内部是如何使用传入的ctx的。 在查询时，需要先从pool中获取一个db的链接，代码大概如下： 12345678910111213// src/database/sql/sql.go// func (db *DB) conn(ctx context.Context, strategy connReuseStrategy) *driverConn, error)// 阻塞从req中获取链接，如果超时，直接返回select &#123;case &lt;-ctx.Done(): // 获取链接超时了，直接返回错误 // do something return nil, ctx.Err()case ret, ok := &lt;-req: // 拿到链接，校验并返回 return ret.conn, ret.err&#125; req 也是一个chan，是等待链接返回的chan，如果Done() 返回的chan 关闭后，则不再关心req的返回了，我们的查询就超时了。 在做SQL Prepare、SQL Query 等操作时，也会有类似方法： 12345678select &#123;default:// 校验是否已经超时，如果超时直接返回case &lt;-ctx.Done(): return nil, ctx.Err()&#125;// 如果还没有超时，调用驱动做查询return queryer.Query(query, dargs) 上面在做查询时，首先判断是否已经超时了，如果超时，则直接返回错误，否则才进行查询。 可以看出，在派生出的带有超时取消功能的 Context 时，内部方法在做异步操作（比如获取链接，查询等）时会先查看是否已经 Done了，如果Done，说明请求已超时，直接返回错误；否则继续等待，或者做下一步工作。这里也可以看出，要做到超时控制，需要不断判断 Done() 是否已关闭。 链路追踪的例子在做链路追踪时，Context 也是非常重要的。（所谓链路追踪，是说可以追踪某一个请求所依赖的模块，比如db，redis，rpc下游，接口下游等服务，从这些依赖服务中找到请求中的时间消耗） 下面举一个链路追踪的例子： 1234567891011121314151617181920212223242526272829303132333435// 建议把key 类型不导出，防止被覆盖type traceIdKey struct&#123;&#125;&#123;&#125;// 定义固定的Keyvar TraceIdKey = traceIdKey&#123;&#125;func ServeHTTP(w http.ResponseWriter, req *http.Request)&#123; // 首先从请求中拿到traceId // 可以把traceId 放在header里，也可以放在body中 // 还可以自己建立一个 （如果自己是请求源头的话） traceId := getTraceIdFromRequest(req) // Key 存入 ctx 中 ctx := context.WithValue(req.Context(), TraceIdKey, traceId) // 设置接口1s 超时 ctx = context.WithTimeout(ctx, time.Second) // query RPC 时可以携带 traceId repResp := RequestRPC(ctx, ...) // query DB 时可以携带 traceId dbResp := RequestDB(ctx, ...) // ...&#125;func RequestRPC(ctx context.Context, ...) interface&#123;&#125; &#123; // 获取traceid，在调用rpc时记录日志 traceId, _ := ctx.Value(TraceIdKey) // request // do log return&#125; 上述代码中，当拿到请求后，我们通过req 获取traceId， 并记录在ctx中，在调用RPC，DB等时，传入我们构造的ctx，在后续代码中，我们可以通过ctx拿到我们存入的traceId，使用traceId 记录请求的日志，方便后续做问题定位。 当然，一般情况下，context 不会单纯的仅仅是用于 traceId 的记录，或者超时的控制。很有可能二者兼有之。 如何实现知其然也需知其所以然。想要充分利用好 Context，我们还需要学习 Context 的实现。下面我们一起学习不同的 Context 是如何实现 Context 接口的， 空上下文Background(), Empty() 均会返回一个空的 Context emptyCtx。emptyCtx 对象在方法 Deadline(), Done(), Err(), Value(interface{}) 中均会返回nil，String() 方法会返回对应的字符串。这个实现比较简单，我们这里暂时不讨论。 有取消功能的上下文WithCancel 构造的context 是一个cancelCtx实例，代码如下。 1234567891011type cancelCtx struct &#123; Context // 互斥锁，保证context协程安全 mu sync.Mutex // cancel 的时候，close 这个chan done chan struct&#123;&#125; // 派生的context children map[canceler]struct&#123;&#125; err error&#125; WithCancel 方法首先会基于 parent 构建一个新的 Context，代码如下： 12345func WithCancel(parent Context) (ctx Context, cancel CancelFunc) &#123; c := newCancelCtx(parent) // 新的上下文 propagateCancel(parent, &amp;c) // 挂到parent 上 return &amp;c, func() &#123; c.cancel(true, Canceled) &#125;&#125; 其中，propagateCancel 方法会判断 parent 是否已经取消，如果取消，则直接调用方法取消；如果没有取消，会在parent的children 追加一个child。这里就可以看出，context 树状结构的实现。 下面是propateCancel 的实现： 12345678910111213141516171819202122232425262728293031// 把child 挂在到parent 下func propagateCancel(parent Context, child canceler) &#123; // 如果parent 为空，则直接返回 if parent.Done() == nil &#123; return // parent is never canceled &#125; // 获取parent类型 if p, ok := parentCancelCtx(parent); ok &#123; p.mu.Lock() if p.err != nil &#123; // parent has already been canceled child.cancel(false, p.err) &#125; else &#123; if p.children == nil &#123; p.children = make(map[canceler]struct&#123;&#125;) &#125; p.children[child] = struct&#123;&#125;&#123;&#125; &#125; p.mu.Unlock() &#125; else &#123; // 启动goroutine，等待parent/child Done go func() &#123; select &#123; case &lt;-parent.Done(): child.cancel(false, parent.Err()) case &lt;-child.Done(): &#125; &#125;() &#125;&#125; Done() 实现比较简单，就是返回一个chan，等待chan 关闭。可以看出 Done 操作是在调用时才会构造 chan done，done 变量是延时初始化的。 12345678910111213141516171819202122232425262728293031323334func (c *cancelCtx) Done() &lt;-chan struct&#123;&#125; &#123; c.mu.Lock() if c.done == nil &#123; c.done = make(chan struct&#123;&#125;) &#125; d := c.done c.mu.Unlock() return d&#125;在手动取消 Context 时，会调用 cancelCtx 的 cancel 方法，代码如下：func (c *cancelCtx) cancel(removeFromParent bool, err error) &#123; // 一些判断,关闭 ctx.done chan // ... if c.done == nil &#123; c.done = closedchan &#125; else &#123; close(c.done) &#125; // 广播到所有的child，需要cancel goroutine 了 for child := range c.children &#123; // NOTE: acquiring the child's lock while holding parent's lock. child.cancel(false, err) &#125; c.children = nil c.mu.Unlock() // 然后从父context 中，删除当前的context if removeFromParent &#123; removeChild(c.Context, c) &#125;&#125; 这里可以看到，当执行cancel时，除了会关闭当前的cancel外，还做了两件事，① 所有的child 都调用cancel方法，② 由于该上下文已经关闭，需要从父上下文中移除当前的上下文。 定时取消功能的上下文WithDeadline, WithTimeout 提供了实现定时功能的 Context 方法，返回一个timerCtx结构体。WithDeadline 是给定了执行截至时间，WithTimeout 是倒计时时间，WithTImeout 是基于WithDeadline实现的，因此我们仅看其中的WithDeadline 即可。WithDeadline 内部实现是基于cancelCtx 的。相对于 cancelCtx 增加了一个计时器，并记录了 Deadline 时间点。下面是timerCtx 结构体： 1234567type timerCtx struct &#123; cancelCtx // 计时器 timer *time.Timer // 截止时间 deadline time.Time&#125; WithDeadline 的实现： 1234567891011121314151617181920212223242526272829303132func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) &#123; // 若父上下文结束时间早于child， // 则child直接挂载在parent上下文下即可 if cur, ok := parent.Deadline(); ok &amp;&amp; cur.Before(d) &#123; return WithCancel(parent) &#125; // 创建个timerCtx, 设置deadline c := &amp;timerCtx&#123; cancelCtx: newCancelCtx(parent), deadline: d, &#125; // 将context挂在parent 之下 propagateCancel(parent, c) // 计算倒计时时间 dur := time.Until(d) if dur &lt;= 0 &#123; c.cancel(true, DeadlineExceeded) // deadline has already passed return c, func() &#123; c.cancel(false, Canceled) &#125; &#125; c.mu.Lock() defer c.mu.Unlock() if c.err == nil &#123; // 设定一个计时器，到时调用cancel c.timer = time.AfterFunc(dur, func() &#123; c.cancel(true, DeadlineExceeded) &#125;) &#125; return c, func() &#123; c.cancel(true, Canceled) &#125;&#125; 构造方法中，将新的context 挂在到parent下，并创建了倒计时器定期触发cancel。 timerCtx 的cancel 操作，和cancelCtx 的cancel 操作是非常类似的。在cancelCtx 的基础上，做了关闭定时器的操作 123456789101112131415func (c *timerCtx) cancel(removeFromParent bool, err error) &#123; // 调用cancelCtx 的cancel 方法 关闭chan，并通知子context。 c.cancelCtx.cancel(false, err) // 从parent 中移除 if removeFromParent &#123; removeChild(c.cancelCtx.Context, c) &#125; c.mu.Lock() // 关掉定时器 if c.timer != nil &#123; c.timer.Stop() c.timer = nil &#125; c.mu.Unlock()&#125; timeCtx 的 Done 操作直接复用了cancelCtx 的 Done 操作，直接关闭 chan done 成员。 传递值的上下文WithValue 构造的上下文与上面几种有区别，其构造的context 原型如下： 12345type valueCtx struct &#123; // 保留了父节点的context Context key, val interface&#123;&#125;&#125; 每个context 包含了一个Key-Value组合。valueCtx 保留了父节点的Context，但没有像cancelCtx 一样保留子节点的Context. 下面是valueCtx的构造方法： 12345678910func WithValue(parent Context, key, val interface&#123;&#125;) Context &#123; if key == nil &#123; panic(\"nil key\") &#125; // key 必须是课比较的，不然无法获取Value if !reflect.TypeOf(key).Comparable() &#123; panic(\"key is not comparable\") &#125; return &amp;valueCtx&#123;parent, key, val&#125;&#125; 直接将Key-Value赋值给struct 即可完成构造。下面是获取Value 的方法： 1234567func (c *valueCtx) Value(key interface&#123;&#125;) interface&#123;&#125; &#123; if c.key == key &#123; return c.val &#125; // 从父context 中获取 return c.Context.Value(key)&#125; Value 的获取是采用链式获取的方法。如果当前 Context 中找不到，则从父Context中获取。如果我们希望一个context 多放几条数据时，可以保存一个map 数据到 context 中。这里不建议多次构造context来存放数据。毕竟取数据的成本也是比较高的。 注意事项最后，在使用中应该注意如下几点： context.Background 用在请求进来的时候，所有其他context 来源于它。 在传入的conttext 不确定使用的是那种类型的时候，传入TODO context （不应该传入一个nil 的context) context.Value 不应该传入可选的参数，应该是每个请求都一定会自带的一些数据。（比如说traceId，授权token 之类的）。在Value 使用时，建议把Key 定义为全局const 变量，并且key 的类型不可导出，防止数据存在冲突。 context goroutines 安全。","categories":[],"tags":[]},{"title":"Golang Map 实现 （四）","slug":"golang-map-4","date":"2020-04-28T10:20:30.000Z","updated":"2021-01-11T06:41:06.575Z","comments":true,"path":"passages/golang-map-4/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-map-4/","excerpt":"golang map 操作，是map 实现中较复杂的逻辑。因为当赋值时，为了减少hash 冲突链的长度过长问题，会做map 的扩容以及数据的迁移。而map 的扩容以及数据的迁移也是关注的重点。","text":"golang map 操作，是map 实现中较复杂的逻辑。因为当赋值时，为了减少hash 冲突链的长度过长问题，会做map 的扩容以及数据的迁移。而map 的扩容以及数据的迁移也是关注的重点。 数据结构首先，我们需要重新学习下map实现的数据结构： 1234567891011121314151617type hmap struct &#123; count int flags uint8 B uint8 noverflow uint16 hash0 uint32 buckets unsafe.Pointer oldbuckets unsafe.Pointer nevacuate uintptr extra *mapextra&#125;type mapextra struct &#123; overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap&#125; hmap 是 map 实现的结构体。大部分字段在 第一节中已经学习过了。剩余的就是nevacuate 和extra 了。 首先需要了解搬迁的概念：当hash 中数据链太长，或者空的bucket 太多时，会操作数据搬迁，将数据挪到一个新的bucket 上，就的bucket数组成为了oldbuckets。bucket的搬迁不是一次就搬完的，是访问到对应的bucket时才可能会触发搬迁操作。（这一点是不是和redis 的扩容比较类似，将扩容放在多个访问上，减少了单次访问的延迟压力） nevactuate 标识的是搬迁的位置(也可以考虑为搬迁的进度）。标识目前 oldbuckets 中 （一个 array）bucket 搬迁到哪里了。 extra 是一个map 的结构体，nextOverflow 标识的是申请的空的bucket，用于之后解决冲突时使用；overflow 和 oldoverflow 标识溢出的链表中正在使用的bucket 数据。old 和非old 的区别是，old 是为搬迁的数据。 理解了大概的数据结构，我们可以学习map的 赋值操作了。 map 赋值操作map 的赋值操作写法如下： 12data := mapExample[\"hello\"] 赋值的实现，golang 为了对不同类型k做了优化，下面时一些实现方法： 123456func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123;&#125;func mapassign_fast32(t *maptype, h *hmap, key uint32) unsafe.Pointer &#123;&#125;func mapassign_fast32ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123;&#125;func mapassign_fast64(t *maptype, h *hmap, key uint64) unsafe.Pointer &#123;&#125;func mapassign_fast64ptr(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer&#123;&#125;func mapassign_faststr(t *maptype, h *hmap, s string) unsafe.Pointer &#123;&#125; 内容大同小异，我们主要学习mapassign 的实现。 mapassign 方法的实现是查找一个空的bucket，把key赋值到bucket上，然后把val的地址返回,然后直接通过汇编做内存拷贝。那我们一步步看是如何找空闲bucket的： ① 在查找key之前，会做异常检测，校验map是否未初始化，或正在并发写操作，如果存在，则抛出异常：（这就是为什么map 并发写回panic的原因） 12345678if h == nil &#123; panic(plainError(\"assignment to entry in nil map\"))&#125;// 竟态检查 和 内存扫描if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map writes\")&#125; ② 需要计算key 对应的hash 值，如果buckets 为空（初始化的时候小于一定长度的map 不会初始化数据）还需要初始化一个bucket 123456789alg := t.key.alghash := alg.hash(key, uintptr(h.hash0))// 为什么需要在hash 后设置flags，因为 alg.hash可能会panich.flags ^= hashWritingif h.buckets == nil &#123; h.buckets = newobject(t.bucket) // newarray(t.bucket, 1)&#125; ③ 通过hash 值，获取对应的bucket。如果map 还在迁移数据，还需要在oldbuckets中找对应的bucket，并搬迁到新的bucket。 123456789101112// 通过hash 计算bucket的位置偏移bucket := hash &amp; bucketMask(h.B)// 此处是搬迁逻辑，我们后续详解if h.growing() &#123; growWork(t, h, bucket)&#125;// 计算对应的bucket 位置，和top hash 值b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize)))top := tophash(hash) ④ 拿到bucket之后，还需要按照链表方式一个一个查，找到对应的key， 可能是已经存在的key，也可能需要新增。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546for &#123; for i := uintptr(0); i &lt; bucketCnt; i++ &#123; // 若 tophash 就不相等，那就取tophash 中的下一个 if b.tophash[i] != top &#123; // 若是个空位置，把kv的指针拿到。 if isEmpty(b.tophash[i]) &amp;&amp; inserti == nil &#123; inserti = &amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) val = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) &#125; // 若后续无数据，那就不用再找坑了 if b.tophash[i] == emptyRest &#123; break bucketloop &#125; continue &#125; // 若tophash匹配时 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; // 比较k不等，还需要继续找 if !alg.equal(key, k) &#123; continue &#125; // 如果key 也相等，说明之前有数据，直接更新k，并拿到v的地址就可以了 if t.needkeyupdate() &#123; typedmemmove(t.key, k, key) &#125; val = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) goto done &#125; // 取下一个overflow （链表指针） ovf := b.overflow(t) if ovf == nil &#123; break &#125; b = ovf&#125; 总结下这段程序，主要有几个部分： a. map hash 不匹配的情况，会看是否是空kv 。如果调用了delete，会出现空kv的情况，那先把地址留下，如果后面也没找到对应的k（也就是说之前map 里面没有对应的Key），那就直接用空kv的位置即可。b. 如果 map hash 是匹配的，需要判定key 的字面值是否匹配。如果不匹配，还需要查找。如果匹配了，那直接把key 更新（因为可能有引用），v的地址返回即可。c. 如果上面都没有，那就看下一个bucket ⑤ 插入数据前，会先检查数据太多了，需要扩容，如果需要扩容，那就从第③开始拿到新的bucket，并查找对应的位置。 1234if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again // Growing the table invalidates everything, so try again&#125; ⑥ 如果刚才看没有有空的位置，那就需要在链表后追加一个bucket，拿到kv。 1234567if inserti == nil &#123; // all current buckets are full, allocate a new one. newb := h.newoverflow(t, b) inserti = &amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) val = add(insertk, bucketCnt*uintptr(t.keysize))&#125; ⑦ 最后更新tophash 和 key 的字面值, 并解除hashWriting 约束 1234567891011121314151617181920212223// 如果非指针数据（也就是直接赋值的数据），还需要申请内存和拷贝if t.indirectkey() &#123; kmem := newobject(t.key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem&#125;if t.indirectvalue() &#123; vmem := newobject(t.elem) *(*unsafe.Pointer)(val) = vmem&#125;// 更新tophash, ktypedmemmove(t.key, insertk, key)*inserti = topdone:if h.flags&amp;hashWriting == 0 &#123; throw(\"concurrent map writes\") &#125; h.flags &amp;^= hashWriting if t.indirectvalue() &#123; val = *((*unsafe.Pointer)(val)) &#125; return val 到这里，map的赋值基本就介绍完了。下面学习下步骤⑤中的map的扩容。 Map 的扩容有两种情况下，需要做扩容。一种是存的kv数据太多了，已经超过了当前map的负载。还有一种是overflow的bucket过多了。这个阈值是一个定值，经验得出的结论，所以我们这里不考究。 当满足条件后，将开始扩容。如果满足条件二，扩容后的buckets 的数量和原来是一样的，说明可能是空kv占据的坑太多了，通过map扩容做内存整理。如果是因为kv 量多导致map负载过高，那就扩一倍的量。 1234567891011121314151617181920212223242526272829303132333435363738394041func hashGrow(t *maptype, h *hmap) &#123; bigger := uint8(1) // 如果是第二种情况，扩容大小为0 if !overLoadFactor(h.count+1, h.B) &#123; bigger = 0 h.flags |= sameSizeGrow &#125; oldbuckets := h.buckets // 申请一个大数组，作为新的buckets newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) flags := h.flags &amp;^ (iterator | oldIterator) if h.flags&amp;iterator != 0 &#123; flags |= oldIterator &#125; // 然后重新赋值map的结构体，oldbuckets 被填充。之后将做搬迁操作 h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 h.noverflow = 0 // extra 结构体做赋值 if h.extra != nil &amp;&amp; h.extra.overflow != nil &#123; // Promote current overflow buckets to the old generation. if h.extra.oldoverflow != nil &#123; throw(\"oldoverflow is not nil\") &#125; h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil &#125; if nextOverflow != nil &#123; if h.extra == nil &#123; h.extra = new(mapextra) &#125; h.extra.nextOverflow = nextOverflow &#125;&#125; 总结下map的扩容操作。首先拿到扩容的大小，然后申请大数组，然后做些初始化的操作，把老的buckets，以及overflow做切换即可。 map 数据的迁移扩容完成后，需要做数据的迁移。数据的迁移不是一次完成的，是使用时才会做对应bucket的迁移。也就是逐步做到的数据迁移。下面我们来学习。 在数据赋值的第③步，会看需要操作的bucket是不是在旧的buckets里面，如果在就搬迁。下面是搬迁的具体操作： 123456789func growWork(t *maptype, h *hmap, bucket uintptr) &#123; // 首先把需要操作的bucket 搬迁 evacuate(t, h, bucket&amp;h.oldbucketmask()) // 再顺带搬迁一个bucket if h.growing() &#123; evacuate(t, h, h.nevacuate) &#125;&#125; nevacuate 标识的是当前的进度，如果都搬迁完，应该和2^B的长度是一样的（这里说的B是oldbuckets 里面的B，毕竟新的buckets长度可能是2^(B+1))。 在evacuate 方法实现是把这个位置对应的bucket，以及其冲突链上的数据都转移到新的buckets上。 ① 先要判断当前bucket是不是已经转移。 (oldbucket 标识需要搬迁的bucket 对应的位置) 12345b := (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize)))// 判断if !evacuated(b) &#123; // 做转移操作&#125; 转移的判断直接通过tophash 就可以，判断tophash中第一个hash值即可 （tophash的作用可以参考第三讲） 12345func evacuated(b *bmap) bool &#123; h := b.tophash[0] // 这个区间的flag 均是已被转移 return h &gt; emptyOne &amp;&amp; h &lt; minTopHash&#125; ② 如果没有被转移，那就要迁移数据了。数据迁移时，可能是迁移到大小相同的buckets上，也可能迁移到2倍大的buckets上。这里xy 都是标记目标迁移位置的标记：x 标识的是迁移到相同的位置，y 标识的是迁移到2倍大的位置上。我们先看下目标位置的确定： 123456789101112var xy [2]evacDstx := &amp;xy[0]x.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize)))x.k = add(unsafe.Pointer(x.b), dataOffset)x.v = add(x.k, bucketCnt*uintptr(t.keysize))if !h.sameSizeGrow() &#123; // 如果是2倍的大小，就得算一次 y 的值 y := &amp;xy[1] y.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) y.k = add(unsafe.Pointer(y.b), dataOffset) y.v = add(y.k, bucketCnt*uintptr(t.keysize))&#125; ③ 确定bucket位置后，需要按照kv 一条一条做迁移。（目的就是清除空闲的kv） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// 遍历每个bucketfor ; b != nil; b = b.overflow(t) &#123; k := add(unsafe.Pointer(b), dataOffset) v := add(k, bucketCnt*uintptr(t.keysize)) // 遍历bucket 里面的每个kv for i := 0; i &lt; bucketCnt; i, k, v = i+1, add(k, uintptr(t.keysize)), add(v, uintptr(t.valuesize)) &#123; top := b.tophash[i] // 空的不做迁移 if isEmpty(top) &#123; b.tophash[i] = evacuatedEmpty continue &#125; if top &lt; minTopHash &#123; throw(\"bad map state\") &#125; k2 := k if t.indirectkey() &#123; k2 = *((*unsafe.Pointer)(k2)) &#125; var useY uint8 if !h.sameSizeGrow() &#123; // 2倍扩容的需要重新计算hash， hash := t.key.alg.hash(k2, uintptr(h.hash0)) if h.flags&amp;iterator != 0 &amp;&amp; !t.reflexivekey() &amp;&amp; !t.key.alg.equal(k2, k2) &#123; useY = top &amp; 1 top = tophash(hash) &#125; else &#123; if hash&amp;newbit != 0 &#123; useY = 1 &#125; &#125; &#125; // 这些是固定值的校验，可以忽略 if evacuatedX+1 != evacuatedY || evacuatedX^1 != evacuatedY &#123; throw(\"bad evacuatedN\") &#125; // 设置oldbucket 的tophash 为已搬迁 b.tophash[i] = evacuatedX + useY // evacuatedX + 1 == evacuatedY dst := &amp;xy[useY] // evacuation destination if dst.i == bucketCnt &#123; // 如果dst是bucket 里面的最后一个kv，则需要添加一个overflow dst.b = h.newoverflow(t, dst.b) dst.i = 0 dst.k = add(unsafe.Pointer(dst.b), dataOffset) dst.v = add(dst.k, bucketCnt*uintptr(t.keysize)) &#125; // 填充tophash值， kv 数据 dst.b.tophash[dst.i&amp;(bucketCnt-1)] = top if t.indirectkey() &#123; *(*unsafe.Pointer)(dst.k) = k2 &#125; else &#123; typedmemmove(t.key, dst.k, k) &#125; if t.indirectvalue() &#123; *(*unsafe.Pointer)(dst.v) = *(*unsafe.Pointer)(v) &#125; else &#123; typedmemmove(t.elem, dst.v, v) &#125; // 更新目标的bucket dst.i++ dst.k = add(dst.k, uintptr(t.keysize)) dst.v = add(dst.v, uintptr(t.valuesize)) &#125;&#125; 对于key 非间接使用的数据（即非指针数据），做内存回收 12345678if h.flags&amp;oldIterator == 0 &amp;&amp; t.bucket.kind&amp;kindNoPointers == 0 &#123; b := add(h.oldbuckets, oldbucket*uintptr(t.bucketsize)) ptr := add(b, dataOffset) n := uintptr(t.bucketsize) - dataOffset // ptr 是kv的位置， 前面的topmap 保留，做迁移前的校验使用 memclrHasPointers(ptr, n)&#125; ④ 如果当前搬迁的bucket 和 总体搬迁的bucket的位置是一样的，我们需要更新总体进度的标记 nevacuate 12345678910111213141516171819202122232425// newbit 是oldbuckets 的长度，也是nevacuate 的重点func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr) &#123; // 首先更新标记 h.nevacuate++ // 最多查看2^10 个bucket stop := h.nevacuate + 1024 if stop &gt; newbit &#123; stop = newbit &#125; // 如果没有搬迁就停止了，等下次搬迁 for h.nevacuate != stop &amp;&amp; bucketEvacuated(t, h, h.nevacuate) &#123; h.nevacuate++ &#125; // 如果都已经搬迁完了，oldbukets 完全搬迁成功，清空oldbuckets if h.nevacuate == newbit &#123; h.oldbuckets = nil if h.extra != nil &#123; h.extra.oldoverflow = nil &#125; h.flags &amp;^= sameSizeGrow &#125;&#125; 总结 Map 的赋值难点在于数据的扩容和数据的搬迁操作。 bucket 搬迁是逐步进行的，每进行一次赋值，会做至少一次搬迁工作。 扩容不是一定会新增空间，也有可能是只是做了内存整理。 tophash 的标志即可以判断是否为空，还会判断是否搬迁，以及搬迁的位置为X or Y。 delete map 中的key，有可能出现很多空的kv，会导致搬迁操作。如果可以避免，尽量避免。","categories":[],"tags":[]},{"title":"Golang Map 实现（三）","slug":"golang-map-3","date":"2020-04-26T01:21:23.000Z","updated":"2021-01-11T06:41:06.552Z","comments":true,"path":"passages/golang-map-3/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-map-3/","excerpt":"本文在golang map 数据结构的基础上，学习map 数据是如何访问的。","text":"本文在golang map 数据结构的基础上，学习map 数据是如何访问的。 map 创建示例在golang 中，访问 map 的方式有两种，例子如下： 123val := example1Map[key1]val, ok := example1Map[key1] 第一种方式不判断是否存在key值，直接返回val （可能是空值）第二种方式会返回一个bool 值，判断是否存在key 键值。（是不是和redis 的空值判断很类似） 那访问map 时，底层做了什么，我们一起来探究对于不同的访问方式，会使用不同的方法，下面是内部提供的几种方法，我们一起来学习： 1234567891011// 迭代器中使用func mapaccessK(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, unsafe.Pointer)&#123;&#125;// 不返回 boolfunc mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123;&#125;func mapaccess1_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) unsafe.Pointer &#123;&#125;// 返回 boolfunc mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) &#123;&#125;func mapaccess2_fat(t *maptype, h *hmap, key, zero unsafe.Pointer) (unsafe.Pointer, bool) &#123;&#125; 这些方法有很大的相关性，下面我们逐一来学习吧。 mapaccess1_fat, mapaccess2_fat这两个方法，从字面上来看多了个fat，就是个宽数据。何以为宽，我们从下面代码找到原因： 12345678//src/cmd/compile/internal/gc/walk.goif w := t.Elem().Width; w &lt;= 1024 &#123; // 1024 must match runtime/map.go:maxZero n = mkcall1(mapfn(mapaccess1[fast], t), types.NewPtr(t.Elem()), init, typename(t), map_, key)&#125; else &#123; z := zeroaddr(w) n = mkcall1(mapfn(\"mapaccess1_fat\", t), types.NewPtr(t.Elem()), init, typename(t), map_, key, z)&#125; 这是构建语法树时，mapaccess1 相关的代码（mapaccess2_fat 也类似）， 如果val 大于1024byte 的宽度，那会调用fat 后缀的方法。原因是，在map.go 文件中，定义了val 0值的数组，代码如下： 12const maxZero = 1024 // must match value in cmd/compile/internal/gc/walk.govar zeroVal [maxZero]byte 但是这个零值只能对宽度小于1024byte的宽度的数据有效，所以对于返回值（val）宽度小于1024 的，直接调用mapaccess1 方法即可，否则需要首先找一个对应的0值数据，然后调用mapaccess1_fat 方法，如果为0，传出对应的0值数据。 mapaccess1， mapaccess2mapaccess1 与 mapaccess2 的差别在于是否返回返回值，mapaccess2 将返回bool 类型作为是否不存在相应key的标识，mapaccess1 不会。所以，这里着重分析mapaccess2. 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) &#123; // 竟态分析 &amp;&amp; 内存扫描 // ... if h == nil || h.count == 0 &#123; // map 为空，或者size 为 0， 直接返回 &#125; if h.flags&amp;hashWriting != 0 &#123; // 这里会检查是否在写，如果在写直接panic throw(\"concurrent map read and map write\") &#125; // 拿到对应key 的hash，以及 bucket alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) m := bucketMask(h.B) b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + (hash&amp;m)*uintptr(t.bucketsize))) if c := h.oldbuckets; c != nil &#123; if !h.sameSizeGrow() &#123; // There used to be half as many buckets; mask down one more power of two. m &gt;&gt;= 1 &#125; oldb := (*bmap)(unsafe.Pointer(uintptr(c) + (hash&amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) &#123; b = oldb &#125; &#125; // 获取tophash 值 top := tophash(hash)bucketloop: // 遍历解决冲突的链表 for ; b != nil; b = b.overflow(t) &#123; // 遍历每个bucket 上的kv for i := uintptr(0); i &lt; bucketCnt; i++ &#123; // 先匹配 tophash // ... // 获取k k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; // 判断k是否相等,如果相等直接返回，否则继续遍历 if alg.equal(key, k) &#123; v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) if t.indirectvalue() &#123; v = *((*unsafe.Pointer)(v)) &#125; return v, true &#125; &#125; &#125; return unsafe.Pointer(&amp;zeroVal[0]), false&#125; 访问map的流程比较简单： 首先，获取key 的hash值，并取到相应的bucket 其次，遍历对应的bucket，以及bucket 的链表（冲突链） 对于每个bucket 需要先匹配tophash 数组中的值，如果不匹配，则直接过滤。 如果hash 匹配成功，还是需要匹配key 是否相等，相等就返回，不等继续遍历。 这里需要注意一点：在tophash 数组中不仅会标识是否匹配hash值，还会标识下个数组中是否还有元素，减少匹配的次数。代码如下： 123456if b.tophash[i] != top &#123; if b.tophash[i] == emptyRest &#123; break bucketloop &#125; continue&#125; tophash 的值有多种情况, 如果小于minTopHash，则作为标记使用。下面是标识含义: 123456789101112// 标记为空，且后面没有数据了 (包括overflow 和 index)emptyRest = 0 // 在被删除的时候设置为空emptyOne = 1 // kv 数据被迁移到新hash表的 x 位置evacuatedX = 2 // kv 数据被迁移到新hash表的 y 位置evacuatedY = 3 // bucket 被转移走了，数据是空的evacuatedEmpty = 4 // 阈值标识minTopHash = 5 enptyRest, enptyOne 是有利于数据遍历的，减少了对数据的访问次数evacuateX 和 evacuateY 与数据迁移有关，我们在赋值部分学习（赋值才有可能迁移） 总结 map 中，val 如果宽度比较大，0值问题也需要多分配内存。所以，这种情况，使用指针肯定是合理的。（当然，内存拷贝也是一个问题） tophash 值的含义参考第一篇 bucket章节 今天的作业就交完了。下一篇将学习golang 赋值的实现。 参考[1] 深入理解 Go map:初始化和访问","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"map","slug":"map","permalink":"http://blog.lpflpf.cn/tags/map/"}]},{"title":"字符串反转的面试题，你会吗","slug":"string-reverse","date":"2020-04-24T08:03:00.000Z","updated":"2021-01-11T06:41:06.585Z","comments":true,"path":"passages/string-reverse/","link":"","permalink":"http://blog.lpflpf.cn/passages/string-reverse/","excerpt":"字符串的面试题，看能到达那个阶段。","text":"字符串的面试题，看能到达那个阶段。 第一阶段不用申请内存空间，把一个字符串做反正操作。 比如说：str=”abcdefg”res=”gfedcba” 这个比较简单，只要做前后字符交换就可以了 12345678910func reverse(str []byte)&#123; i := 0 j := len(str) - 1 for i &lt; j &#123; str[i], str[j] = str[j], str[i] i ++ j -- &#125;&#125; 第二阶段不用申请内存，如何把每个单词做反转,假设单词中间只有一个空格 比如说：str = “php is the best programing language in the world”res = “php si eht tseb gnimargorp egaugnal ni eht dlrow” 12345678910111213141516171819func reverse(str string) &#123; i := 0 k := 0 reverse1 = func(str []byte, begin int, end int)&#123; for begin &lt; end &#123; str[begin], str[end] = str[end], str[begin] begin ++ end -- &#125; &#125; for i = 0; i &lt; len(str); i ++ &#123; if str[i] == ' ' &#123; reverse1(str, k, i - 1) k = i + 1 &#125; &#125;&#125; 第三阶段不用申请内存，如何把一组单词做反转。 比如说：str = “php is the best programing language in the world”res = “world the in language programing best the is php” 这个略有难度，但是只需要在第二阶段的接触上加一行代码就可以做到了。 1234567891011121314151617181920func reverse(str string) &#123; i := 0 k := 0 reverse1 = func(str []byte, begin int, end int)&#123; for begin &lt; end &#123; str[begin], str[end] = str[end], str[begin] begin ++ end -- &#125; &#125; reverse1 (str, 0, len(str) - 1) for i = 0; i &lt; len(str); i ++ &#123; if str[i] == ' ' &#123; reverse1(str, k, i - 1) k = i + 1 &#125; &#125;&#125;","categories":[],"tags":[{"name":"interview","slug":"interview","permalink":"http://blog.lpflpf.cn/tags/interview/"}]},{"title":"Golang Map 实现（二）","slug":"golang-map-2","date":"2020-04-23T09:45:23.000Z","updated":"2021-01-11T06:41:06.552Z","comments":true,"path":"passages/golang-map-2/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-map-2/","excerpt":"本文在golang map 数据结构的基础上，学习一个make 是如何构造的。","text":"本文在golang map 数据结构的基础上，学习一个make 是如何构造的。 map 创建示例在golang 中，初始化一个map 算是有两种方式。 12example1Map := make(map[int64]string)example2Map := make(map[int64]string, 100) 第一种方式默认不指定map的容量，第二种会指定后续map的容量估计为100，希望在创建的时候把空间就分配好。 当make创建map时，底层做了什么对于不同的初始化方式，会使用不同的方式。下面是提供的几种初始化方法： 1234// hint 就是 make 初始化map 的第二个参数func makemap(t *maptype, hint int, h *hmap) *hmapfunc makemap64(t *maptype, hint int64, h *hmap) *hmapfunc makemap_small() *hmap 区别在于：如果不指定 hint，就调用makemap_small；如果make 第二个参数为int64, 则调用makemap64；其他情况调用makemap方法。下面我们逐一学习。 makemap_small 12345func makemap_small() *hmap &#123; h := new(hmap) h.hash0 = fastrand() return h&#125; fastrand 是创建一个seed，在生成hash值时使用。所以在makemap_small 时，只是创建了一个hmap 的结构体，并没有初始化buckets. makemap64 123456func makemap64(t *maptype, hint int64, h *hmap) *hmap &#123; if int64(int(hint)) != hint &#123; hint = 0 &#125; return makemap(t, int(hint), h)&#125; makemap64 是对于传入的第二个参数为int64 的变量使用的。 如果hint的值大于int最大值，就将hint赋值为0，否则和makemap 初始化没有差别。为什么不把大于2^31 - 1 的map 直接初始化呢？因为在hmap 中 count 的值就是int，也就是说map最大就是 2^31 - 1 的大小。 makemap这个是初始化map的核心代码了，需要我们慢慢品味。 一开始，我们需要了解下maptype这个结构， maptype 标识一个map 数据类型的定义，当然还有其他的类型，比如说interfacetype，slicetype，chantype 等。maptype 的定义如下： 12345678910type maptype struct &#123; typ _type // type 类型 key *_type // key 的type elem *_type // value 的type bucket *_type // internal type representing a hash bucket keysize uint8 // key 的大小 valuesize uint8 // value 的大小 bucketsize uint16 // size of bucket flags uint32&#125; maptype 里面存储了kv的对象类型，bucket类型，以及kv占用内存的大小。以及bucketsize的大小，还有一些标记字段（flags）。在map 实现时，需要用到这些字段做偏移计算等。 下面是 makemap 的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041// hint 需要创建的 map 大小(预计要添加多少元素)func makemap(t *maptype, hint int, h *hmap) *hmap &#123; mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem &gt; maxAlloc &#123; hint = 0 &#125; // initialize Hmap if h == nil &#123; h = new(hmap) &#125; // xorshift64+ 算法, 可以研究下 h.hash0 = fastrand() // 计算B 的值 // 如果大于8，就先申请好。 // 申请规则就是刚好满足 hint &lt; 6.5 * 2 ^ B 的时候 （B 最大是63） // 其中6.5 相当于每个bucket 链表中，平均有6.5个bucket // 所以最长的map，应该是 6.5 * 2^63 (正常用肯定不会溢出) B := uint8(0) for overLoadFactor(hint, B) &#123; B++ &#125; h.B = B // 接着数据初始化, 如果 容量小于等于8的，就在用的时候初始化, B 为0 if h.B != 0 &#123; var nextOverflow *bmap // 申请一个buckets 数组 h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil &#123; h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow &#125; &#125; return h&#125; 首先，通过bucketsize 和hint 的值，计算出需要分配的内存大小mem， 以及是否会overflow （大于指针的最大地址范围），如果溢出或者申请的内存大于最大可以申请的内存时，就设置hint为0了，直接不初始化buckets了。 接着，和makemap_small 一样，初始化一个随机的种子。 然后，计算B的值. 在overLoadfactor 中，判断了hint 的大小。如果小于等于8，那B就不再赋值，直接不初始化数据。如果B大于8，那就计算B了。这里涉及到一个填充因子的概念。大概意思就是说，每个hash值（也就是pos）中，平均放多少个kv数据，默认是6.5；所以判断标准就是hint 必须满足如下的条件： 1hint &lt; 6.5 * (1 &lt;&lt; B) 通过增加B的值，直到上面的表达式满足为止。这样B就初始化好了。 最后，申请一个bucket数组，赋值给buckets，如果有多申请出来的buckets，那就赋值给extra.nextOverflow, 当溢出之后，从多申请出来的buckets 里面取（也是为了避免内存分配）。 下面就详细看下初始化一个buckets的构建。 makeBucketArraymakeBucketArray 用于初始化一个Bucket 数组。也就是hmap 中的buckets，下面是相关代码： 12345678910111213141516171819202122232425262728func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap) &#123; base := bucketShift(b) nbuckets := base // 为了防止溢出的迁移，加一点冗余的bucket if b &gt;= 4 &#123; // ... 修改nbuckets &#125; // 如果之前没有分配过，那直接分配 if dirtyalloc == nil &#123; buckets = newarray(t.bucket, int(nbuckets)) &#125; else &#123; // 使用以前分配好的 buckets = dirtyalloc size := t.bucket.size * nbuckets if t.bucket.kind&amp;kindNoPointers == 0 &#123; memclrHasPointers(buckets, size) &#125; else &#123; memclrNoHeapPointers(buckets, size) &#125; &#125; if base != nbuckets &#123; // 处理多申请出来的bucket &#125; return buckets, nextOverflow&#125; 这里用到了比较多的指针计算，需要细细品读。 首先，就是就是通过B计算一个base值，base = 1 &lt;&lt; B （2 ^ B)nbuckets 是需要申请的数组的长度，正常情况下 base 值就是数组长度。但是，如果 base 大于16时，会预分配一些需要后期做overflow的bucket。这个overflow的计算规则如下： 123456nbuckets += bucketShift(b - 4)sz := t.bucket.size * nbucketsup := roundupsize(sz)if up != sz &#123; nbuckets = up / t.bucket.size&#125; 在base 的基础上，多分配 base / 16 长度的bucket。然后根据内存的分配规则（包括了页大小和内存对齐等规则），计算出合适的分配内存的大小，然后计算出 bucket 的分配个数 nbuckets. 其次，如果有之前未分配内存，那就初始化一个数组（终于等到了这一步），如过有dirtyalloc， 那就使用dirtyalloc 的内存（其实是用来清除map中数据使用的），然后把dirtyalloc中不需要的数据清除引用。 最后，如果除了需要申请的base 长度的bucket外，还多申请了一些bucket，下面是对多申请的数据做的处理： 123456789// 上面添加了一些nbuckets 防止溢出，所以B 值取模就不太合理了，所以有一个mapextra 的数据节点// 数据分配也很有趣，从刚申请的buckets数组中，取出后面的一段分给mapextra// nextOverflow 分配给mapextranextOverflow = (*bmap)(add(buckets, base*uintptr(t.bucketsize)))// 取nextOverflow 里面的最后一个元素// 并把最后一个buckets 的末尾偏移的指针指向空闲的bucket (目前就是第一个buckets 了)last := (*bmap)(add(buckets, (nbuckets-1)*uintptr(t.bucketsize)))last.setoverflow(t, (*bmap)(buckets)) 先计算出多申请出来的内存地址 nextOverflow，然后计算出 申请的最后一块bucket的地址，然后将最后一块bucket的overflow指针（指向链表的指针）指向buckets 的首部。 原因呢，是为了将来判断是否还有空的bucket 可以让溢出的bucket空间使用。 今天的作业就交完了。下一篇将学习golang map的数据初始化实现。 参考[1] 深入理解 Go map:初始化和访问","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"map","slug":"map","permalink":"http://blog.lpflpf.cn/tags/map/"}]},{"title":"Golang Map实现（一）","slug":"golang-map-1","date":"2020-04-21T08:56:17.000Z","updated":"2021-01-11T06:41:06.551Z","comments":true,"path":"passages/golang-map-1/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-map-1/","excerpt":"本文学习 Golang 的 Map 数据结构，以及map buckets 的数据组织结构。","text":"本文学习 Golang 的 Map 数据结构，以及map buckets 的数据组织结构。 hash 表是什么从大学的课本里面，我们学到：hash 表其实就是将key 通过hash算法映射到数组的某个位置,然后把对应的val存放起来。如果出现了hash冲突（也就是说，不同的key被映射到了相同的位置上时），就需要解决hash冲突。解决hash冲突的方法还是比较多的，比如说开放定址法，再哈希法，链地址法，公共溢出区等(复习下大学的基本知识)。 其中链地址法比较常见，下面是一个链地址法的常见模式： Position 指通过Key 计算出的数组偏移量。例如当 Position = 6 的位置已经填满KV后，再次插入一条相同Position的数据将通过链表的方式插入到该条位置之后。 在php的Array 中是这么实现的，golang中也基本是这么实现。下面我们学习下Golang中map的实现。 Golang Map 实现的数据结构Golang的map中，首先把kv 分在了N个桶中，每个桶中的数据有8条（bucketCnt）。如果一个桶满了(overflow)，也会采用链地址法解决hash 的冲突。 下面是定义一个hashmap的结构体： 12345678910111213141516171819202122232425262728293031323334type hmap struct &#123; // 长度 count int // map 的标识, 下方做了定义 flags uint8 // 实际buckets 的长度为 2 ^ B B uint8 // 从bucket中溢出的数量，（存在extra 里面) noverflow uint16 // hash 种子，做key 哈希的时候会用到 hash0 uint32 // 存储 buckets 的地方 buckets unsafe.Pointer // 迁移时oldbuckets中存放部分buckets 的数据 oldbuckets unsafe.Pointer // 迁移的数量 nevacuate uintptr // 一些额外的字段，在做溢出处理以及数据增长的时候会用到 extra *mapextra&#125;const ( iterator = 1 // 有一个迭代器在使用buckets oldIterator = 2 // 有一个迭代器在使用oldbuckets hashWriting = 4 // 并发写，通过这个标识报panic sameSizeGrow = 8 // the current map growth is to a new map of the same size)type mapextra struct &#123; overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap&#125;type bmap struct &#123; tophash [bucketCnt]uint8&#125; 表中除了对基本的hash数据结构做了定义外，还对数据迁移、扩容等操作做了定义，这里我们可以忽略，等学习到时我们再深入了解。 深入 桶列表 (buckets)buckets 字段中是存储桶数据的地方。正常会一次申请至少2^N长度的数组，数组中每个元素就是一个桶。N 就是结构体中的B。这里面要注意以下几点： 为啥是2的幂次方 为了做完hash后，通过掩码的方式取到数组的偏移量, 省掉了不必要的计算。 B 这个数是怎么确定的 这个和我们map中要存放的数据量是有很大关系的。我们在创建map的时候来详述。 bucket 的偏移是怎么计算的 hash 方法有多个，在 runtime/alg.go 里面定义了。不同的类型用不同的hash算法。算出来是一个uint32的一个hash 码，通过和B取掩码，就找到了bucket的偏移了。下面是取对应bucket的例子： 1234567// 根据key的类型取相应的hash算法alg := t.key.alghash := alg.hash(key, uintptr(h.hash0))// 根据B拿到一个掩码m := bucketMask(h.B)// 通过掩码以及hash指，计算偏移得到一个bucketb := (*bmap)(add(h.buckets, (hash&amp;m)*uintptr(t.bucketsize))) 深入 桶 (bucket)一个桶的示意图如下： 每个桶里面，可以放8个k，8个v，还有一个overflow指针（就是上面的next），用来指向下一个bucket 的地址。在每个bucket的头部，还会放置一个tophash，也就是bmap 结构体。这个数组里面存放的是key的hash值，用来对比我们key生成的hash和存出的hash是否一致（当然除了这个还有其他的用途，后面讲数据访问的时候会讲到）。 tophash中的数据，是从计算的hash值里面截取的。获取bucket 是用的低bit位的hash，tophash 使用的是高bit位的hash值（8位） 为啥bucket 一次要存8个kv，而不是一个kv放一个bucket，然后链地址法做处理就OK了 据我分析，有几点原因: a， 一次分配8个kv的空间，可以减少内存的分配频次; b，减少了overflow指针的内存占用，比如说8个kv，采用一个一个存储的话，需要8 * 8B （64位机） = 64B的数据存下一个的地址，而采用go实现的这种方式，只需要 8B + 8B (bmap的大小） = 16B 的数据就可以了。 为啥需要用tophash 一般的hash 实现逻辑是直接和key比较，如果比较成功，这找到相应key的数据。但是这里用到了tophash，好处是可以减少key的比较成本（毕竟key 不一定都是整数形式存在的） 为啥是8个 8 * 8B = 64B 整好是64位机的一个最小寻址空间，不过可以通过修改源码自定义吧。 为什么key 和val 要分开放 这个也比较好理解，key 和val 都是用户可以自定义的。如果key是定长的（比如是数字，或者 指针之类的，大概率是这样。）内存是比较整齐的，利于寻址吧。 技术总结golang 实现的map比朴素的hashmap 在很多方面都有优化。 使用掩码方式获取偏移，减少判断。 bucket 存储方式的优化。 通过tophash 先进行一次比较，减少key 比较的成本。 当然，有一点是不太明白的，为啥 overflow 指针要放在 kv 后面？ 放在tophash 之后的位置岂不是更完美？ 今天的作业就交完了。下一篇将学习golang map的数据初始化实现。 参考[1] 深入理解 Go map:初始化和访问","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"map","slug":"map","permalink":"http://blog.lpflpf.cn/tags/map/"}]},{"title":"Golang-Slice","slug":"golang-slice","date":"2020-04-20T03:06:08.000Z","updated":"2021-01-11T06:41:06.578Z","comments":true,"path":"passages/golang-slice/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-slice/","excerpt":"本文从源码角度学习 golang slice 的创建、扩容，深拷贝的实现。","text":"本文从源码角度学习 golang slice 的创建、扩容，深拷贝的实现。 内部数据结构slice 仅有三个字段，其中array 是保存数据的部分，len 字段为长度，cap 为容量。 12345type slice struct &#123; array unsafe.Pointer // 数据部分 len int // 长度 cap int // 容量&#125; 通过下面代码可以输出空slice 的大小: 123456789101112131415161718192021222324252627282930package mainimport \"fmt\"import \"unsafe\"func main() &#123; data := make([]int, 0, 3) // 24 len:8, cap:8, array:8 fmt.Println(unsafe.Sizeof(data)) // 我们通过指针的方式，拿到数组内部结构的字段值 ptr := unsafe.Pointer(&amp;data) opt := (*[3]int)(ptr) // addr, 0, 3 fmt.Println(opt[0], opt[1], opt[2]) data = append(data, 123) fmt.Println(unsafe.Sizeof(data)) shallowCopy := data[:1] ptr1 := unsafe.Pointer(&amp;shallowCopy) opt1 := (*[3]int)(ptr1) fmt.Println(opt1[0])&#125; 创建创建一个slice，其实就是分配内存。cap, len 的设置在汇编中完成。 下面的代码主要是做了容量大小的判断，以及内存的分配。 1234567891011121314151617func makeslice(et *_type, len, cap int) unsafe.Pointer &#123; // 获取需要申请的内存大小 mem, overflow := math.MulUintptr(et.size, uintptr(cap)) if overflow || mem &gt; maxAlloc || len &lt; 0 || len &gt; cap &#123; mem, overflow := math.MulUintptr(et.size, uintptr(len)) if overflow || mem &gt; maxAlloc || len &lt; 0 &#123; panicmakeslicelen() &#125; panicmakeslicecap() &#125; // 分配内存 // 小对象从当前P 的cache中空闲数据中分配 // 大的对象 (size &gt; 32KB) 直接从heap中分配 // runtime/malloc.go return mallocgc(mem, et, true)&#125; append对于不需要内存扩容的slice，直接数据拷贝即可。 上面的DX 存放的就是array 指针，AX 是数据的偏移. 将 123 存入数组。而对于容量不够的情况，就需要对slice 进行扩容。这也是slice 比较关心的地方。 （因为对于大slice，grow slice会影响到内存的分配和执行的效率） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103func growslice(et *_type, old slice, cap int) slice &#123; // 静态分析, 内存扫描 // ... if cap &lt; old.cap &#123; panic(errorString(\"growslice: cap out of range\")) &#125; // 如果存储的类型空间为0， 比如说 []struct&#123;&#125;, 数据为空，长度不为空 if et.size == 0 &#123; return slice&#123;unsafe.Pointer(&amp;zerobase), old.len, cap&#125; &#125; newcap := old.cap doublecap := newcap + newcap if cap &gt; doublecap &#123; // 如果新容量大于原有容量的两倍，则直接按照新增容量大小申请 newcap = cap &#125; else &#123; if old.len &lt; 1024 &#123; // 如果原有长度小于1024，那新容量是老容量的2倍 newcap = doublecap &#125; else &#123; // 按照原有容量的1/4 增加，直到满足新容量的需要 for 0 &lt; newcap &amp;&amp; newcap &lt; cap &#123; newcap += newcap / 4 &#125; // 通过校验newcap 大于0检查容量是否溢出。 if newcap &lt;= 0 &#123; newcap = cap &#125; &#125; &#125; var overflow bool var lenmem, newlenmem, capmem uintptr // 为了加速计算（少用除法，乘法） // 对于不同的slice元素大小，选择不同的计算方法 // 获取需要申请的内存大小。 switch &#123; case et.size == 1: lenmem = uintptr(old.len) newlenmem = uintptr(cap) capmem = roundupsize(uintptr(newcap)) overflow = uintptr(newcap) &gt; maxAlloc newcap = int(capmem) case et.size == sys.PtrSize: lenmem = uintptr(old.len) * sys.PtrSize newlenmem = uintptr(cap) * sys.PtrSize capmem = roundupsize(uintptr(newcap) * sys.PtrSize) overflow = uintptr(newcap) &gt; maxAlloc/sys.PtrSize newcap = int(capmem / sys.PtrSize) case isPowerOfTwo(et.size): // 二的倍数，用位移运算 var shift uintptr if sys.PtrSize == 8 &#123; // Mask shift for better code generation. shift = uintptr(sys.Ctz64(uint64(et.size))) &amp; 63 &#125; else &#123; shift = uintptr(sys.Ctz32(uint32(et.size))) &amp; 31 &#125; lenmem = uintptr(old.len) &lt;&lt; shift newlenmem = uintptr(cap) &lt;&lt; shift capmem = roundupsize(uintptr(newcap) &lt;&lt; shift) overflow = uintptr(newcap) &gt; (maxAlloc &gt;&gt; shift) newcap = int(capmem &gt;&gt; shift) default: // 其他用除法 lenmem = uintptr(old.len) * et.size newlenmem = uintptr(cap) * et.size capmem, overflow = math.MulUintptr(et.size, uintptr(newcap)) capmem = roundupsize(capmem) newcap = int(capmem / et.size) &#125; // 判断是否会溢出 if overflow || capmem &gt; maxAlloc &#123; panic(errorString(\"growslice: cap out of range\")) &#125; // 内存分配 var p unsafe.Pointer if et.kind&amp;kindNoPointers != 0 &#123; p = mallocgc(capmem, nil, false) // 清空不需要数据拷贝的部分内存 memclrNoHeapPointers(add(p, newlenmem), capmem-newlenmem) &#125; else &#123; // Note: can't use rawmem (which avoids zeroing of memory), because then GC can scan uninitialized memory. p = mallocgc(capmem, et, true) if writeBarrier.enabled &#123; // gc 相关 // Only shade the pointers in old.array since we know the destination slice p // only contains nil pointers because it has been cleared during alloc. bulkBarrierPreWriteSrcOnly(uintptr(p), uintptr(old.array), lenmem) &#125; &#125; // 数据拷贝 memmove(p, old.array, lenmem) return slice&#123;p, old.len, newcap&#125;&#125; 切片拷贝 (copy)切片的浅拷贝 1234567shallowCopy := data[:1]ptr1 := unsafe.Pointer(&amp;shallowCopy)opt1 := (*[3]int)(ptr1)fmt.Println(opt1[0]) 下面是上述代码的汇编代码： 上面，先将 data 的成员数据拷贝到寄存器，然后从寄存器拷贝到shallowCopy的对象中。（注意到只是拷贝了指针而已, 所以是浅拷贝） 切片的深拷贝深拷贝也比较简单，只是做了一次内存的深拷贝。 123456789101112131415161718192021222324252627282930313233343536373839404142434445func slicecopy(to, fm slice, width uintptr) int &#123; if fm.len == 0 || to.len == 0 &#123; return 0 &#125; n := fm.len if to.len &lt; n &#123; n = to.len &#125; // 元素大小为0，则直接返回 if width == 0 &#123; return n &#125; // 竟态分析和内存扫描 // ... size := uintptr(n) * width // 直接内存拷贝 if size == 1 &#123; // common case worth about 2x to do here *(*byte)(to.array) = *(*byte)(fm.array) // known to be a byte pointer &#125; else &#123; memmove(to.array, fm.array, size) &#125; return n&#125;// 字符串slice的拷贝func slicestringcopy(to []byte, fm string) int &#123; if len(fm) == 0 || len(to) == 0 &#123; return 0 &#125; n := len(fm) if len(to) &lt; n &#123; n = len(to) &#125; // 竟态分析和内存扫描 // ... memmove(unsafe.Pointer(&amp;to[0]), stringStructOf(&amp;fm).str, uintptr(n)) return n&#125; 其他 汇编的生成方法 1go tool compile -N -S slice.go &gt; slice.S 需要了解unsafe.Pointer 的使用 slice.go 位于 runtime/slice.go 上述代码使用 go1.12.5 版本 还有一点需要提醒， type 长度为0的对象。比如说 struct{} 类型。(所以，很多使用chan struct{} 做channel 的传递，节省内存) 1234567891011121314package mainimport \"fmt\"import \"unsafe\"func main() &#123; var data [100000]struct&#123;&#125; var data1 [100000]int // 0 fmt.Println(unsafe.Sizeof(data)) // 800000 fmt.Println(unsafe.Sizeof(data1))&#125;","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"slice","slug":"slice","permalink":"http://blog.lpflpf.cn/tags/slice/"}]},{"title":"Golang 的内存泄漏","slug":"golang-memory-leak","date":"2020-04-17T06:24:44.000Z","updated":"2021-01-11T06:41:06.576Z","comments":true,"path":"passages/golang-memory-leak/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-memory-leak/","excerpt":"Golang 作为一个提供了GC的语言，还能有内存泄漏一说？其实不然，Go 服务宕机80%应该是因为内存泄漏的缘故了。","text":"Golang 作为一个提供了GC的语言，还能有内存泄漏一说？其实不然，Go 服务宕机80%应该是因为内存泄漏的缘故了。 导言内存泄漏 (Memory Leak) 是在计算机科学中，由于疏忽或错误造成程序未能释放已经不再使用的内存。内存泄漏并非指内存在物理上的消失，而是应用程序分配某段内存后，由于设计错误，导致在释放该段内存之前就失去了对该段内存的控制，从而造成了内存的浪费。 （维基百科） 所以，内存泄漏是一个共性的问题。虽然在Golang中提供了聪明的GC操作，但是如果操作不慎，也可能掉入内存泄漏的坑。 什么情况下会内存泄漏总结了一些经常碰到的内存泄漏的例子，以飨读者： 数据泄漏 比如，在全局变量(或者单例模式)中 （例如：map，slice 等 构成的数据池），不断添加新的数据，而不释放。 goroutine泄漏 goroutine 泄漏，应该是Golang 中经常遇到的一个问题了。由于goroutine 存在栈空间（至少会有2K）, 所以goroutine 的泄漏常常导致了golang的内存泄漏。在官方提供的方法中，如果使用不当，很容易出现goroutine泄漏。比如说： 在 Time 包中： 12345678910func After(d Duration) &lt;-chan Time &#123; return NewTimer(d).C&#125;func Tick(d Duration) &lt;-chan Time &#123; if d &lt;= 0 &#123; return nil &#125; return NewTicker(d).C&#125; 由于 NewTicker 和 NewTimer 会创建倒计时发送chan 的协程(创建方法在startTimer 中实现，/src/runtime/time.go)，所以这种方法不能多次使用。使用时建议新建 NewTimer 和NewTicker，并控制 Timer 和Ticker 的终结。 再比如说，在 Http 请求时，会返回 *http.Response 对象，Http 响应中的Body是http的响应数据，Body 需要每次读取后关闭。那为什么需要关闭呢，我们从 Body 的赋值代码查找结果： 12345678910111213141516// /src/net/http/transport.go// http 的持久化链接池，不断取需要做的请求，并做响应func (pc *persistConn) readResponse(rc requestAndChan, trace *httptrace.ClientTrace) (resp *Response, err error) &#123; //... resp.Body = newReadWriteCloserBody(pc.br, pc.conn) // pc.conn net.Conn // ...&#125;func newReadWriteCloserBody(br *bufio.Reader, rwc io.ReadWriteCloser) io.ReadWriteCloser &#123; body := &amp;readWriteCloserBody&#123;ReadWriteCloser: rwc&#125; if br.Buffered() != 0 &#123; body.br = br &#125; return body&#125; 从代码中可以看出 resp.Body 实际上仅仅是个代理，我们实际上读取和关闭的是net.Conn 对象。因此也不难看出关闭Body 的意义何在了。（如果不关闭，这个conn 应该是不能关闭的） 除了官方的一些func使用不当会导致goroutine泄漏，日常开发也会碰到各种内存泄漏的例子, 比如说：redis 从连接池取的链接没有做释放，DB 的 stmt 没有关闭等。 如何应对内存泄漏如果很悲剧，代码上线后，服务发生了内存泄漏，我们可以从几个方面去考虑： 分析goroutine 是否泄漏从 pprof 的goroutine 分析，是否 goroutine 在持续增长。如果持续增长，那 goroutine 泄漏没跑了。我们用下面的例子来举例。 12345678910111213141516171819202122232425262728293031323334353637package mainimport ( \"net/http\" _ \"net/http/pprof\" \"time\")type none struct&#123;&#125;func main() &#123; go func() &#123; ch := make(chan none) consumer(ch) producer(ch) &#125;() _ = http.ListenAndServe(\"0.0.0.0:8080\", nil)&#125;func consumer(ch chan none) &#123; for i := 0; i &lt; 1000; i++ &#123; // 此处类似协程泄漏 go func() &#123; &lt;-ch &#125;() time.Sleep(3 * time.Microsecond) &#125;&#125;func producer(ch chan none) &#123; time.Sleep(100 * time.Second) for i := 0; i &lt; 1000; i++ &#123; ch &lt;- none&#123;&#125; &#125;&#125; 上述代码中，逐步创建了1k个goroutine(假定是泄漏的)，我们可以通过http://127.0.0.1:8080/debug/pprof/ 访问查看goroutine的变化情况。a. 在debug 中观察goroutine的数量变化，如果持续增长，那可以确定是goroutine 泄漏了。 b. 之后访问 http://127.0.0.1:8080/debug/pprof/goroutine?debug=1查看各goroutine数量，查看持续增加的goroutine ,如果存在持续增长的goroutine，那从goroutine的堆栈代码短分析即可。下图中很明显可以看出1K的协程量。（当然是持续增长到达1K的） 数据泄漏怎么看数据泄漏出现的问题就比较多了，比如长的 string，slice 数据用切片的方式被引用，如果切片后的数据不释放，长的string，slice 是不会被释放的, 当然这种泄漏比较小。下面举一个前两天网友提供的一个案例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package mainimport ( \"fmt\" \"io/ioutil\" \"net\" \"net/http\" _ \"net/http/pprof\" \"time\")type None int64func main() &#123; go func() &#123; singals := []int64&#123;&#125; netListen, _ := net.Listen(\"tcp\", \":30000\") defer netListen.Close() for &#123; conn, err := netListen.Accept() if err != nil &#123; fmt.Println(\"Accept Error\") &#125; singals = append(singals, 1) go doSomething(conn) &#125; for _ = range singals &#123; fmt.Println(\"Received\") &#125; &#125;() _ = http.ListenAndServe(\"0.0.0.0:8080\", nil)&#125;func doSomething(conn net.Conn) &#123; defer conn.Close() time.Sleep(100 * time.Microsecond) buf, err := ioutil.ReadAll(conn) if err == nil &#123; fmt.Println(string(buf)) &#125;&#125; 例子比较简单，从net Accept 数据，并开启一个goroutine 做数据处理。singals 呢，用于做事件处理，每接收一个链接，给singal 推一条数据。为了从中查找内存泄漏，我们也增加了pprof。 为了能尽快发现问题，我这边用了一个简单的shell对服务施压(请求2w http 服务，不关心请求返回结果)。命令如下： 1for i in `seq 0 20000`; do curl -m 1 \"http://127.0.0.1:30000?abc=def\" &amp; done 从pprof 的 heap 中，我们能轻易的发现: 内存分配中，mem_leak文件的26行(append) 操作 申请的内存排在了top 1，仔细看代码，发现我们slice中的数据从来没有释放，所以造成了上面的问题。 如何解决这个问题呢？ 其实比较简单。只需要将slice，修改成带cache的chan（作为一个队列来使用），当数据使用过后即可销毁。不仅不会再出现内存泄漏，也保证了功能上的一致性。(当然需要重新起一个协程, 由于上面的for 是阻塞的，不会断开，所以也导致了下面的slice 不工作） 做一个小结当然，上面的例子都是精简到不能再精简的小例子，实际中遇到的问题可能会要比这个复杂的多。但是万变不离其宗，找到正确的方法解决也不是什么难事。 除了上面的一些问题，还应该注意点什么，做了下面的总结: 做一个服务进程内存监控的报警，这个很有必要，也是正常服务应该做的。 pprof 提供的是堆上的监控，栈内存很少会泄漏，也不容易被监控。 尽量在方法返回时不要让使用者去操作Close，减少goroutine泄漏的可能。 在用全局的Map,Slice 时要反复考虑导致内存泄漏。 slice 引用大切片时，考虑会不会有不释放的可能性。 才疏学浅，有问题请留言。谢谢","categories":[],"tags":[]},{"title":"Golang Trace 的一个例子","slug":"golang-trace-example","date":"2020-04-16T09:14:25.000Z","updated":"2021-01-11T06:41:06.579Z","comments":true,"path":"passages/golang-trace-example/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-trace-example/","excerpt":"今天，通过一个例子，一方面熟悉trace在自定义范围内的分析，另一方面golang 在协程调度策略上的浅析。","text":"今天，通过一个例子，一方面熟悉trace在自定义范围内的分析，另一方面golang 在协程调度策略上的浅析。 Show Code 1234567891011121314151617181920212223242526272829303132333435363738394041424344// trace_example.gopackage mainimport ( \"context\" \"fmt\" \"os\" \"runtime\" \"runtime/trace\" \"sync\")func main()&#123; // 为了看协程抢占，这里设置了一个cpu 跑 runtime.GOMAXPROCS(1) f, _ := os.Create(\"trace.dat\") defer f.Close() _ = trace.Start(f) defer trace.Stop() ctx, task := trace.NewTask(context.Background(), \"sumTask\") defer task.End() var wg sync.WaitGroup wg.Add(10) for i := 0; i &lt; 10; i ++ &#123; // 启动10个协程，只是做一个累加运算 go func(region string) &#123; defer wg.Done() // 标记region trace.WithRegion(ctx, region, func() &#123; var sum, k int64 for ; k &lt; 1000000000; k ++ &#123; sum += k &#125; fmt.Println(region, sum) &#125;) &#125;(fmt.Sprintf(\"region_%02d\", i)) &#125; wg.Wait()&#125; 首先，代码的功能非常简单，只是启动10个协程，每个协程处理的工作都是一样的，即把0 … 1000000000 做了sum 运算。 其次，代码中，添加了Task 和 Task 的Region，是我们更好的发现我们协程的位置(当然，我这里都捕获了，只是用Region 做了标识)，并将记录的 trace 数据写入trace.dat 文件中。 最后，为了更好的看到协程对cpu的抢占，所以把cpu的个数限制为1个。 编译并运行,会得到如下结果： 123456789101112# go build trace_example.go# ./trace_example region_09 499999999500000000region_00 499999999500000000region_01 499999999500000000region_02 499999999500000000region_03 499999999500000000region_04 499999999500000000region_05 499999999500000000region_06 499999999500000000region_07 499999999500000000region_08 499999999500000000 从结果中，我们可以看出，协程执行的顺序不是那么有序。但是真实是怎么执行的呢？我们从 trace.dat 中获取答案。 Trace 分析执行下面命令，打开trace 的web服务： 1234# go tool trace trace.dat2020/04/16 17:34:09 Parsing trace...2020/04/16 17:34:10 Splitting trace...2020/04/16 17:34:10 Opening browser. Trace viewer is listening on http://127.0.0.1:53426 我们先从分析整个协程入手, 从这里可以看出，我们的协程其实没有按照时间片轮询的方式跑（毕竟这是一个纯计算性的工作） 而从Task中，我们观察所有自定义的Region 和goroutine. 从图中可以看出，task 任务所关注的region 是一个一个跑的，region_09 先执行了，这个也从我们的输出中得到了验证。从图中也可以看到，我们的goroutineid(G1, G10, G12 等, 虽然我们在go编写代码时并不能拿到这个goroutineid). 总结与反思除了实操了一次 task 和 region 的自定义做trace 分析外，我们还能从这个例子中找到些什么信息。 goroutine 肯定是存在的 goroutine 的启动肯定不是有序的, 这一点从task 的图中就可以明显看出来 goroutine 如果没有阻塞的服务的话，会一直占用cpu的（所以有了 runtime.Gosched() 的存在） 所以，对于一些占用高频cpu的服务（比如说加解密，编解码服务等）如果有别的优先级比较高的goroutine在工作，可以适当的让出CPU, 保证服务正常有序工作。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"trace","slug":"trace","permalink":"http://blog.lpflpf.cn/tags/trace/"}]},{"title":"Golang 性能测试 (3) 跟踪刨析","slug":"golang-trace","date":"2020-04-14T08:09:31.000Z","updated":"2021-01-11T06:41:06.579Z","comments":true,"path":"passages/golang-trace/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-trace/","excerpt":"本文简单介绍 golang 如何做跟踪刨析。","text":"本文简单介绍 golang 如何做跟踪刨析。 简介对于绝大部分服务，跟踪刨析是用不到的。但是如果遇到了下面问题，可以不妨一试： 怀疑哪个协程慢了 系统调用有问题 协程调度问题 (chan 交互、互斥锁、信号量等) 怀疑是 gc (Garbage-Collect) 影响了服务性能 网络阻塞 等等 坦白的讲，通过跟踪刨析可以看到每个协程在某一时刻在干什么。 做跟踪刨析，首先需要获取trace 数据。可以通过代码中插入trace， 或者上节提到的通过pprof 下载即可。 ExampleCode下面通过代码直接插入的方式来获取trace. 内容会涉及到网络请求，涉及协程异步执行等。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package mainimport ( \"io/ioutil\" \"math/rand\" \"net/http\" \"os\" \"runtime/trace\" \"strconv\" \"sync\" \"time\")var wg sync.WaitGroupvar httpClient = &amp;http.Client&#123;Timeout: 30 * time.Second&#125;func SleepSomeTime() time.Duration&#123; return time.Microsecond * time.Duration(rand.Int()%1000)&#125;func create(readChan chan int) &#123; defer wg.Done() for i := 0; i &lt; 500; i++ &#123; readChan &lt;- getBodySize() SleepSomeTime() &#125; close(readChan)&#125;func convert(readChan chan int, output chan string) &#123; defer wg.Done() for readChan := range readChan &#123; output &lt;- strconv.Itoa(readChan) SleepSomeTime() &#125; close(output)&#125;func outputStr(output chan string) &#123; defer wg.Done() for _ = range output &#123; // do nothing SleepSomeTime() &#125;&#125;// 获取taobao 页面大小func getBodySize() int &#123; resp, _ := httpClient.Get(\"https://taobao.com\") res, _ := ioutil.ReadAll(resp.Body) _ = resp.Body.Close() return len(res)&#125;func run() &#123; readChan, output := make(chan int), make(chan string) wg.Add(3) go create(readChan) go convert(readChan, output) go outputStr(output)&#125;func main() &#123; f, _ := os.Create(\"trace.out\") defer f.Close() _ = trace.Start(f) defer trace.Stop() run() wg.Wait()&#125; 编译，并执行，然后启动trace; 123456[lipengfei5@localhost ~/blog]$ go build trace_example.go [lipengfei5@localhost ~/blog]$ ./trace_example[lipengfei5@localhost ~/blog]$ go tool trace -http=\":8000\" trace_example trace.out 2020/04/15 17:34:48 Parsing trace...2020/04/15 17:34:50 Splitting trace...2020/04/15 17:34:51 Opening browser. Trace viewer is listening on http://0.0.0.0:8000 然后打开浏览器，访问8000 端口即可。 Trace 功能 其中:View trace：查看跟踪 (按照时间分段，上面我的例子时间比较短，所以没有分段)Goroutine analysis：Goroutine 分析Network blocking profile：网络阻塞概况Synchronization blocking profile：同步阻塞概况Syscall blocking profile：系统调用阻塞概况Scheduler latency profile：调度延迟概况User defined tasks：用户自定义任务User defined regions：用户自定义区域Minimum mutator utilization：最低 Mutator 利用率 （主要是GC 的评价标准, 暂时没搞懂） goroutine 调度分析下图包含了两种事件： 网络相关 main.create 触发网络写的协程，网络写操作的协程 writeLoop，然后等待网络返回。 GC 相关操作 下面是web请求到数据，从epoll 中触发，然后readLoop协程响应,直接触发main.create 的协程得到执行。 当然我们也可以筛选协程做具体分析，从 Goroutine analysis 进入，选择具体的协程进行分析： 我们选择对 main.create 的协程做分析（这个协程略复杂，可以分析的东西比较多） 可以从图中看出，network 唤醒 readLoop 协程，进而readLoop 又通知了main.create 协程。 当然，我们也可以选择 main.convert 协程。可以看出协程被main.create 唤醒了（由于给chan 提供了数据） 除了可以分析goroutine 调度之外，还可以做网络阻塞分析，异步阻塞分析，系统调度阻塞分析，协程调度阻塞分析（下图） 自定义 Task 和 Region当然，还可以指定task 和 Region 做分析，下面是官方举的例子: 123456789101112131415161718192021//filepath: src/runtime/trace/trace.goctx, task := trace.NewTask(ctx, \"makeCappuccino\")trace.Log(ctx, \"orderID\", orderID)milk := make(chan bool)espresso := make(chan bool)go func() &#123; trace.WithRegion(ctx, \"steamMilk\", steamMilk) milk &lt;- true&#125;()go func() &#123; trace.WithRegion(ctx, \"extractCoffee\", extractCoffee) espresso &lt;- true&#125;()go func() &#123; defer task.End() // When assemble is done, the order is complete. &lt;-espresso &lt;-milk trace.WithRegion(ctx, \"mixMilkCoffee\", mixMilkCoffee)&#125;() MMU 图除此之外，还提供了Minimum Mutator Utilization 图 (mmu 图 ) mmu 图，数轴是服务可以占用cpu的百分比 (其他时间为gc操作) 从图中可以看出，在2ms之后，可利用的cpu逐步上升，直到接近100%.所以gc 毫无压力。 重点提醒 必须用chrome，并且高版本不行。我使用的是76. trace 的文件都比较大，几分钟可能上百兆，所以网络一定要好，或者使用本机做验证。 造作是 w 放大， s 缩小， a 左移， d 右移 gc 的mmu 图解释 （备注下，还没有来得及看）https://www.cs.cmu.edu/~guyb/papers/gc2001.pdf","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"trace","slug":"trace","permalink":"http://blog.lpflpf.cn/tags/trace/"}]},{"title":"Golang 性能测试  (2) 性能分析","slug":"golang-profile","date":"2020-04-11T02:13:52.000Z","updated":"2021-01-11T06:41:06.577Z","comments":true,"path":"passages/golang-profile/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-profile/","excerpt":"本文介绍 golang 如何做性能分析。","text":"本文介绍 golang 如何做性能分析。 对服务做了基准性能测试后，如果服务出现问题，可以通过性能分析工具，查出消耗资源的瓶颈，并做针对性的性能优化。 Golang 语言也为我们提供了方便的性能分析工具pprof，方便我们做必要的服务优化。pprof 可以做cpu分析，统计所有调用方法执行的时间片(通过采样); 可以查看内存分配，找到是否有内存泄漏，哪里泄露了（调用栈）；还可以查看Block、事件调用，互斥锁等。可谓麻雀虽小，五脏俱全。Golang 提供了两种分析的工具，一种是web工具，直接引入即可；另一种是命令行交互工具，需要抓取prof 数据，再做详细分析。 WEB 工具golang 性能分析工具主要有几种，最常用的是使用web 界面的工具。我们举个简单的例子，将一个map数据做编码，编码100w次，例子如下： 12345678910111213141516171819202122232425262728package mainimport \"encoding/json\"import _ \"net/http/pprof\"import \"net/http\"func main() &#123; mapData := mapData := map[string]string&#123; \"abcdefg1\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg2\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg3\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg4\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg5\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg6\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg7\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg8\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg9\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg10\": \"aaaaaaaaaaaaaaaaaaaa\", &#125; go func() &#123; for i := 0; i &lt; 100000000; i++ &#123; _, _ = json.Marshal(data) &#125; &#125;() http.ListenAndServe(\"0.0.0.0:8080\", nil)&#125; 引入 “net/http/pprof” 包，将自动在默认的http中添加相关 pprof 的处理方法（当然也可以自己添加了）。我们通过访问 /debug/pprof/ 就可以打开对应的web 界面。 allocs 过去所有内存分配的采样。 block 查看阻塞同步的堆栈 cmdline 当前进程的命令行 goroutine 所有协程的调用栈 heap 当前活动对象的内存分配 mutex 竞态互斥锁的调用栈 profile 获取一个30s（可以通过seconds 参数指定）的cpu 采样prof 文件 （可以用 go tool pprof 分析） threadcreate 导致创建了新系统线程的调用栈 trace 抓一个当前执行的trace包，可以捕获各种事件(可以用go tool trace 做可视化分析) 命令行交互命令行工具，需要先抓取一段采样数据，采样数据可以通过web 的 profile 链接直接下载，也可以不启动web服务，直接采样。直接采样的好处是，可以直接采样我们需要优化的代码段的数据，而web采样的数据不一定会抓到我们执行的代码段（毕竟是通过采样实现的）。下面我们写一个直接采样的例子： 123456789101112131415161718192021222324252627282930313233343536package mainimport \"encoding/json\"import \"runtime/pprof\"import \"os\"import \"log\"func main() &#123; cpuprofile := \"json_map.prof\" mapData := map[string]string&#123; \"abcdefg1\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg2\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg3\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg4\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg5\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg6\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg7\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg8\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg9\": \"aaaaaaaaaaaaaaaaaaaa\", \"abcdefg10\": \"aaaaaaaaaaaaaaaaaaaa\", &#125; if cpuprofile != \"\" &#123; f, err := os.Create(cpuprofile) if err != nil &#123; log.Fatal(err) &#125; pprof.StartCPUProfile(f) defer pprof.StopCPUProfile() &#125; for i := 0; i &lt; 1000000; i++ &#123; _, _ = json.Marshal(mapData) &#125;&#125; 然后我们通过如下命令进入交互模式： 1234567[root@localhost pprof]# go tool pprof json_map.profFile: json_map_1Type: cpuTime: Apr 11, 2020 at 6:49pm (CST)Duration: 7.38s, Total samples = 7.12s (96.46%)Entering interactive mode (type \"help\" for commands, \"o\" for options)(pprof) 交互模式，也提供了丰富的命令查看prof文件中的数据，例如如下使用top10 查看代码执行cpu占比top10 的方法。 123456789101112131415(pprof) top10Showing nodes accounting for 3470ms, 48.74% of 7120ms totalDropped 78 nodes (cum &lt;= 35.60ms)Showing top 10 nodes out of 87 flat flat% sum% cum cum% 570ms 8.01% 8.01% 1100ms 15.45% encoding/json.(*encodeState).string 550ms 7.72% 15.73% 1850ms 25.98% runtime.mallocgc 460ms 6.46% 22.19% 460ms 6.46% runtime.memmove 410ms 5.76% 27.95% 540ms 7.58% runtime.mapaccess2 320ms 4.49% 32.44% 350ms 4.92% runtime.heapBitsSetType 290ms 4.07% 36.52% 970ms 13.62% runtime.typedmemmove 230ms 3.23% 39.75% 230ms 3.23% runtime.nextFreeFast 220ms 3.09% 42.84% 220ms 3.09% runtime.memclrNoHeapPointers 210ms 2.95% 45.79% 210ms 2.95% cmpbody 210ms 2.95% 48.74% 6720ms 94.38% encoding/json.mapEncoder.encode 还有其他功能，例如绘制调用图，内存分配图等，可以通过help查看: 除此之外，go tool profile 还有另外的打开模式。例如，通过web服务查看prof 文件。执行如下命令，通过web服务查看prof文件： 1[root@localhost pprof]# go tool pprof -http=:8080 json_map.prof 可以查看进程调用图，看到各调用函数的执行事件。 可以查看火焰图，具体分析哪些方法有优化空间。 还可以查看Peek (调用者与被调用者匹配关系) 可以从源码角度查看执行时间占比。 也可以通过反汇编的代码角度查看执行时间占比。 除此之外，还可以命令行方式直接抓取web工具中的profile 数据做分析。（实际看来和自己抓取没什么区别，只是方便了而已） 其他golang 目前提供的性能分析工具已经比较齐全了。本文只是对目前已经使用的功能做简单总结，其他功能还待我们一起去探索。 备注： 本文使用的go版本为1.13 下一篇将对 go tool 的另一神器 go tool trace 做简单总结。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"profile","slug":"profile","permalink":"http://blog.lpflpf.cn/tags/profile/"}]},{"title":"Golang 性能测试 (1)","slug":"golang-benchmark","date":"2020-04-09T02:04:12.000Z","updated":"2021-01-11T06:41:06.546Z","comments":true,"path":"passages/golang-benchmark/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-benchmark/","excerpt":"本文介绍golang 如何做基准性能测试。","text":"本文介绍golang 如何做基准性能测试。 编写完代码除了跑必要的单元测试外，还需要考虑代码跑起来的性能如何。性能的衡量其实就是程序运行时候进程的内存分配，CPU消耗情况。 golang 语言在提供了功能测试的基础上，提供了丰富的性能测试功能。 SHOW CODE首先，从一个例子来讲起。 随便写一个简单的快速排序，然后和系统自带的排序做一个性能比较。 如下为简版快排的代码： 123456789101112131415161718192021222324252627282930package benchmarkimport \"sort\"func QSort(data []int) &#123; myqsort(data, 0, len(data)-1)&#125;func myqsort(data []int, s, e int) &#123; if s &gt;= e &#123; return &#125; t := data[s] i, j := s, e for i &lt; j &#123; for ; i &lt; j &amp;&amp; data[j] &gt;= t; j-- &#123; &#125; for ; i &lt; j &amp;&amp; data[i] &lt; t; i++ &#123; &#125; if i &lt; j &#123; break &#125; data[i], data[j] = data[j], data[i] i++ j-- &#125; data[i] = t myqsort(data, s, i-1) myqsort(data, i+1, e)&#125; 然后编写一个测试的test。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package benchmarkimport \"testing\"import \"math/rand\"import \"time\"import \"sort\"var ints []int// 长度为 1w 的数据使用系统自带排序func BenchmarkSort10k(t *testing.B) &#123; slice := ints[0:10000] t.ResetTimer() // 只考虑下面代码的运行事件，所以重置计时器 for i := 0; i &lt; t.N; i++ &#123; sort.Ints(slice) &#125;&#125;// 长度为 100 的数据使用系统自带排序func BenchmarkSort100(t *testing.B) &#123; slice := ints[0:100] t.ResetTimer() for i := 0; i &lt; t.N; i++ &#123; sort.Ints(slice) &#125;&#125;// 长度为 1w 的数据使用上述代码排序func BenchmarkQsort10k(t *testing.B) &#123; slice := ints[0:10000] t.ResetTimer() for i := 0; i &lt; t.N; i++ &#123; QSort(slice) &#125;&#125;// 长度为 100 的数据使用上述代码排序func BenchmarkQsort100(t *testing.B) &#123; slice := ints[0:100] t.ResetTimer() for i := 0; i &lt; t.N; i++ &#123; QSort(slice) &#125;&#125;// 数据初始化，为了保证每次数据都是一致的。func TestMain(m *testing.M) &#123; rand.Seed(time.Now().Unix()) ints = make([]int, 10000) for i := 0; i &lt; 10000; i++ &#123; ints[i] = rand.Int() &#125; m.Run()&#125; 运行命令 ： 1# go test -cover -count 3 -benchmem -bench=. 运行结果如下图： 基准测试，默认将每个方法执行1s中，然后展示执行的次数，每一次执行的耗时, 上述还展示了内存每次分配的大小，以及每次benchmark分配的次数。上述的命令行指定了运行次数为3次，显示代码覆盖率和内存分配情况。 从基准测试的结果可以分析出：对于1w数据量的排序，自带的排序比我的排序算法要快20倍左右；100数据量的排序，手撸的排序略胜一筹。从内存分析来讲，系统自带的会使用4B的数据，而我的算法无内存分配。 INTRODUCE BENCHMARK引入golang 提供的 testing 包，写需要的基准测试的方法（方法名必须以Benchmark开头, 参数必须为 *testing.B）。 若需要做一些数据初始化的工作，可以如上写一个TestMain 方法，将数据初始化的工作在这里完成。 除了这些，可以看*testing.B, *testing.M 的相关方法即可。 最后，只要运行官方提供的 go test -bench=. 命令,即可开始跑基准测试。 当然，还有其他选项可以满足我们多样的需求。例如： -cpu 1,2,4 指定运行的cpu 格式 -count n 指定运行的次数 -benchtime 每一条测试执行的时间 （默认是1s） -bench 指定执行bench的方法， . 是全部 -benchmem 显示内存分配情况 其他参数可以通过 go help testflag 查看 WHY SO SLOW 我这里选取的是第一个数作为中位数，数据越大越可能出现倾斜，排序慢的概率也大。 正常的排序包中，都会在对小于等于12 个数的数组做排序时使用希尔排序，速度也有很大提升。 除了简单的做性能测试外，golang 还自带了性能分析的工具，我们可以快速找出代码中的内存分配、cpu消耗的核心区，帮助我们解决服务的性能问题。下篇文章将做详细了解。","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"benchmark","slug":"benchmark","permalink":"http://blog.lpflpf.cn/tags/benchmark/"}]},{"title":"你不知道的空格","slug":"blank_space","date":"2020-04-08T07:23:04.000Z","updated":"2021-01-11T06:41:06.543Z","comments":true,"path":"passages/blank_space/","link":"","permalink":"http://blog.lpflpf.cn/passages/blank_space/","excerpt":"本文对了解的空格分为几个Level，看大家能达到哪个level。","text":"本文对了解的空格分为几个Level，看大家能达到哪个level。 Level1： 半角空格历史最悠久的空格，在1967年，ASCII 规范中被定义。空格在 ASCII 中编码为0x20, 占位符为一个半角字符。在日常英文书写和代码编写中使用。 Level2: 全角空格中文输入中的空格（标准说法为中日韩表意字符(CJK)中使用的宽空格）。和其他汉字一样，作为GBK的一个字符，其对应的unicode码为\\u3000.宽度是2个半角空格的大小。例如： 1国父 孙中山先生 Level3: 不间断空格 ( non-breaking space )unicode 为 \\u00A0, 在代码中可能会出现的编码错误(utf8 编码0xC2 0xA0) 就是它了。在Word中，会遇到一个有多个单词组成的词组被分割在两行文字中，这样很容易让人看不明白。这时候，不间断空格就可以上场了。输入不间断空格，会将不间断空格连着的单词在一行展示。举个例子： 上面英文使用了不间断空格，下面没有使用。所以上面的英文自动在一行展示，而下面没有。在word中输入不间断空格的方式为: (Ctrl + Shift + Space) 除了在word等文本编辑软件中使用，其实不间断空格在html 中大量使用。&amp;nbsp; 是html 中最为常见的空格。由于html页面中，如果有多个连着的半角空格，则空格只会展示一个。而使用&amp;nbsp; 空格，则会显示占位半个自宽。 Level4: 零宽度空格 (ZERO WIDTH SPACE)零宽度空格有两种 零宽度空格 unicode 编码为 \\u200B. 不可见非打印字符。有了半角空格，也有了全角空格，其实还有零宽度空格。因为宽度为零，因此该字符是一个不可见字符。这个编码虽然是不可见的，但是也是非常有用的。它可以替换html中的标签(软换行, html5 新增)。 零宽度非中断空格(ZWNBSP) unicode 编码为 \\u2060 (之前使用\\ufeff表示，unicode 3.2 开始 \\ufeff 标记unicode文档的字节序。) 该空格结合了 non-breaking space 和 零宽度空格的特点。既会自动换行，宽度又是0。 零宽度空格（软换行）举例： 一行连续的英文编码: 1&lt;p style=\"font-size:100px;\"&gt;PhpIsTheBestProgramingLanguageInTheWorld&lt;/p&gt; chrome 中将显示不换行： 而如果在每个可以换行的地方加上 &lt;wbr /&gt;, 则可以在标记的最近的地方换行。 1&lt;p style=\"font-size:100px;\"&gt;Php&lt;wbr /&gt;Is&lt;wbr /&gt;The&lt;wbr /&gt;Best&lt;wbr /&gt;Programing&lt;wbr /&gt;Language&lt;wbr /&gt;In&lt;wbr /&gt;The&lt;wbr /&gt;World&lt;/p&gt; chrome 中将显示： Level5: 其他空格字符空格虽然已经有半角空格、全角空格，但是上面的空格如果字体变化了，不会随着字体的变化而变化。因此，又有了可以随着字体的变化而变化的空格，简单罗列如下： 在html 的宽度度量中，有一种单位叫em，是按照字体大小定义的，下面的em也是字体的宽度。打印字符的空格有很多种，罗列几个： 名称 unicode 编码 html 标记 特征和用途 短空格 \\u2002 &amp;ensp; html 中占位半个字 长空格 \\u2003 &amp;emsp; html 中占位一个字 1/3em空格 \\u2004 &amp;emsp13; 占用1/3个空格 1/4em空格 \\u2005 &amp;emsp14; 占用1/4个空格 1/6em空格 \\u2006 &amp;emsp14; 占用1/6个空格 数样间距 (figure space) \\u2007 &amp;numsp; 在等宽字体中，宽度是一个字符的宽度。 行首前导空格 (punctuation space) \\u2008 &amp;puncsp; 宽度约为 0x20 的宽度。 瘦弱空格 (thin space) \\u2009 &amp;thinsp; 宽度是 全角打印空格的 1/5 或者 1/6 (宽度不定,法文设置为1/8)， 主要用在打印两个空的引号之间。 hair space \\u200a &amp;hairsp; (浏览器目前不支持), 最窄的空格，推荐标准为 (1/10, 1/16) narrow no-break space \\u202f &amp;nnbsp; 和0a 类似，不同语种中不太一样。 medium mathematical space \\u205f &amp;mediumspace; 在格式化数学公式时使用。是 4/18 的 em宽度，例如：”a + b”中，a 和+ 之间应该用 这个空格 引用链接: https://en.wikipedia.org/wiki/Zero-width_space https://en.wikipedia.org/wiki/Em_(typography) https://en.wikipedia.org/wiki/En_(typography) https://en.wikipedia.org/wiki/Zero-width_space https://en.wikipedia.org/wiki/Thin_space https://en.wikipedia.org/wiki/Whitespace_character https://www.unicode.org/charts/PDF/U2000.pdf https://web.archive.org/web/20100314135826/https://www.microsoft.com/typography/developers/fdsspec/spaces.htm https://en.wikipedia.org/wiki/Word_joiner","categories":[],"tags":[]},{"title":"Supervisor 的使用和进阶 (3)","slug":"supervisor3","date":"2020-04-06T03:46:35.000Z","updated":"2021-01-11T06:41:06.598Z","comments":true,"path":"passages/supervisor3/","link":"","permalink":"http://blog.lpflpf.cn/passages/supervisor3/","excerpt":"本文主要介绍 supervisor Event 的功能。","text":"本文主要介绍 supervisor Event 的功能。 supervisor 作为一个进程管理工具，在 3.0 版本之后，新增了 Event 的高级特性, 主要用于做(进程启动、退出、失败等)事件告警服务。 Event 特性是将监听的服务(listener)注册到supervisord中，当supervisord监听到相应事件时，将事件信息推送给监听对应事件的listener。 事件类型 Event 可以设置 27 种事件类型，可以分为如下几类： 1. 监控进程状态转移事件; 2. 监控进程状态日志变更事件; 3. 进程组中进程添加删除事件; 4. supervisord 进程本身日志变更事件; 5. supervisord 进程本身状态变更的事件; 6. 定时触发事件。 事件可以被单独监听，也可以一个listener 监听多种事件。 配置说明对于一个listener，与正常program的区别是，新增了events 参数，用于标识要监听的事件。 123[eventlistener:theeventlistenername]events=PROCESS_STATE,TICK_60 buffer_size=10 ; 事件池子大小（输入流大小） 事件类型配置多个，用逗号分割。上述配置的是子进程状态的变更，以及定时60s通知间隔60s事件通知缓冲区大小，可以自定义配置，上述配置了10个事件消息的缓冲。 Listener 的实现与supervisord 的交互由于supervisord 是 listener的父进程，所以交互方式采用最简单的 标准输入输出的方式交互。listener 通过标准输入获取事件，通过标准输出通知supervisord listener的事件处理结果，以及当前supervisord的状态 listener 的状态listener 有三种状态：ACKNOWLEDGED、READY、BUSY. ACKNOWLEDGED: listener 未就绪的状态。（发送READY之前的状态） READY: 等待事件触发的状态。（发送READY 消息后，未收到消息的状态） BUSY: 事件处理中的状态。（即输出 OK, FAIL 之前处理Event消息时的状态） 消息协议消息包括supervisord 通知给listener 的事件消息和 listener 通知给supervisord 的状态变更消息。 listener 的状态变更消息, READY 状态OK的 “READY\\n” 消息 处理成功 “RESULT 2\\nOK” 消息 处理失败 “RESULT 4\\nFAIL” 消息 supervisord 广播的事件消息, 事件消息分为 header 和 payload 两部分。 header 中采用kv的方式发送，header 中包含了 payload 的长度。 例如官网提供的header 的例子： 1ver:3.0 server:supervisor serial:21 pool:listener poolserial:10 eventname:PROCESS_COMMUNICATION_STDOUT len:54 header 含义： serial 为事件的序列号 pool 表示listener 的进程池名称(listener支持启动多个) poolserial 表示listener的进程池序列号 eventname 事件名称 len body 的长度 Listener 的基本流程listener 的处理流程如下： 1. 发送ready消息，等待事件发生。 2. 收到事件后，处理事件 3. 事件处理完成后，发送 result 消息, 从第一步开始循环进程状态转移举例我们以进程状态转移作为例子，做简单介绍。 首先，使用 golang 实现listener 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package mainimport ( \"bufio\" \"os\" \"strconv\" \"strings\")const RESP_OK = \"RESULT 2\\nOK\"const RESP_FAIL = \"RESULT 4\\nFAIL\"func main() &#123; stdin := bufio.NewReader(os.Stdin) stdout := bufio.NewWriter(os.Stdout) stderr := bufio.NewWriter(os.Stderr) for &#123; // 发送后等待接收event _, _ = stdout.WriteString(\"READY\\n\") _ = stdout.Flush() // 接收header line, _, _ := stdin.ReadLine() stderr.WriteString(\"read\" + string(line)) stderr.Flush() header, payloadSize := praseHeader(line) // 接收payload payload := make([]byte, payloadSize) stdin.Read(payload) stderr.WriteString(\"read : \" + string(payload)) stderr.Flush() result := alarm(header, payload) if result &#123; // 发送处理结果 stdout.WriteString(RESP_OK) &#125; else &#123; stdout.WriteString(RESP_FAIL) &#125; stdout.Flush() &#125;&#125;func praseHeader(data []byte) (header map[string]string, payloadSize int) &#123; pairs := strings.Split(string(data), \" \") header = make(map[string]string, len(pairs)) for _, pair := range pairs &#123; token := strings.Split(pair, \":\") header[token[0]] = token[1] &#125; payloadSize, _ = strconv.Atoi(header[\"len\"]) return header, payloadSize&#125;// 这里设置报警即可func alarm(header map[string]string, payload []byte) bool &#123; // send mail return true&#125; 这里，报警处理未填写。 其次，在supervisor 中添加配置，监听服务: 123456[eventlistener:listener]command=/root/listenerevents=PROCESS_STATE,TICK_5stdout_logfile=/var/log/tmp/listener_test_stdout.logstderr_logfile=/var/log/tmp/listener_test_stderr.loguser=root 这里监听了服务的处理状态，以及每5s的心跳消息。 最后，启动listener。 1supervisorct start listener 从stderr的日志中可以看到，简单的TICK_5 的消息(调整了格式): 12header : ver:3.0 server:supervisor serial:256 pool:listener_test poolserial:173 eventname:TICK_5 len:15read payload: when:1586258030 fastcgi 进程状态变更的消息: 12345header : ver:3.0 server:supervisor serial:291 pool:listener_test poolserial:208 eventname:PROCESS_STATE_EXITED len:87payload: processname:fastcgi_test groupname:fastcgi_test from_state:RUNNING expected:0 pid:19119header :ver:3.0 server:supervisor serial:293 pool:listener_test poolserial:210 eventname:PROCESS_STATE_STARTING len:73payload: processname:fastcgi_test groupname:fastcgi_test from_state:EXITED tries:0 events 参考","categories":[],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"http://blog.lpflpf.cn/tags/服务管理/"}]},{"title":"Supervisor 的使用和进阶 (2)","slug":"supervisor2","date":"2020-04-06T02:46:35.000Z","updated":"2021-01-11T06:41:06.597Z","comments":true,"path":"passages/supervisor2/","link":"","permalink":"http://blog.lpflpf.cn/passages/supervisor2/","excerpt":"本文主要介绍 supervisor 对 fastcgi 进程的管理","text":"本文主要介绍 supervisor 对 fastcgi 进程的管理 fastcgi 进程的管理在php 中，php-fpm 有主进程来管理和维护子进程的数量。但是并不是所有的服务都有类似的主进程来做子进程的维护。在很多其他语言中，有很多比较有名的fastcgi 服务，例如py 的flup， c++ 实现的 FastCgi++等。如果这些服务在单机中启动多个进程（极有可能），那如何管理这些进程是个比较头疼的问题。 supervisor 的fastcgi 管理的功能就是为了解决这个问题。 配置在普通进程的基础上，添加如下配置： 123456[fcgi-program:x]socket = \"tcp://10.3.2.10:9002\" // 支持 tcp ，或者 Unix socketsocket_backlog = 1024 // 2 的N次方, 根据机器配置设置, 默认是端口最大监听量socket_owner = chrism:wheel // 监听用户组socket_mode = 0700 // 监听模式 举个例子实现一个简单的fastcgi 服务通过监听127.0.0.1:9001 端口对 fastcgi 请求做处理。处理流程为：暂停1s，打印处理的进程id。(为了能看到不同进程做了响应，因此对进程暂停1s处理，并打印进程id。) 12345678910111213141516171819202122232425// fastcgi.gopackage mainimport ( \"net\" \"net/http\" \"net/http/fcgi\" \"os\" \"strconv\" \"time\")type FastCGIServer struct&#123;&#125;// 暂停1s， 打印标识的进程idfunc (s FastCGIServer) ServeHTTP(resp http.ResponseWriter, req *http.Request) &#123; time.Sleep(time.Second) resp.Write([]byte(\"ProcessId: \" + strconv.Itoa(os.Getpid()) + \"\\n\"))&#125;func main() &#123; listener, _ := net.Listen(\"tcp\", \"127.0.0.1:9001\") srv := new(FastCGIServer) fcgi.Serve(listener, srv)&#125; 通过如下命令得到一个简单的fastcgi 二进制文件。通过监听127.0.0.1:9001 端口做fastcgi 处理。处理内容为暂停1s，并打印处理的进程id。(为了能看到不同进程做了响应，因此对进程暂停1s处理，并打印进程id。) 1go build -o fastcgi fastcgi.go 生成的fastcgi 就是一个简单的fastcgi 服务。功能为暂停1s，并输出当前进程的进程ID。 修改 supervisor 的配置修改supervisor 的配置，将fastcgi 服务添加到supervisor 管理，并启动6个fastcgi 进程。 在supervisord.conf 添加如下配置： 123456789[fcgi-program:fastcgi_test]socket=tcp://127.0.0.1:9001command=/root/test/fastcgi autostart=truestopwaitsecs=1000autorestart=trueuser=rootprocess_name=%(program_name)s_%(process_num)02dnumprocs=6 修改完成后，需要刷新supervisord 的配置，并启动fastcgi。 12supervisorctl update supervisorctl start fastcgi_test:* # 因为启动的fastcgi 有多个，因此需要加 :* 修改nginx 的配置Nginx 配置如下： 1234567server &#123; listen 127.0.0.1:8080; location / &#123; include fastcgi.conf; fastcgi_pass 127.0.0.1:9001; &#125;&#125; 并通过如下命令重新加载 nginx 配置。 1nginx -s reload 做一个简单的请求实验对nginx 重新加载配置后，我们请求8080 端口，看服务的请求情况： post 10次请求： 12# for i in `seq 1 10`; do curl 'http://127.0.0.1:8080/app?helloworld' &amp; done# ProcessId: 11319ProcessId: 11299ProcessId: 11300ProcessId: 11307ProcessId: 11307ProcessId: 11311ProcessId: 11311ProcessId: 11315ProcessId: 11315ProcessId: 11319 返回结果，processId 被均匀的分到不同的fastcgi 上。 当某个 fastcgi_test 意外退出时，supervisor 可以再次启动一个fastcgi_test 做补充，这就实现了PHP-FPM master 进程的主要功能。 实现原理我们知道，正常情况下，一个端口只能被一个进程监听。但是刚刚看到的情况是，多个fastcgi同时启动，监听 9001 端口。这是因为linux 系统中，如果父进程监听端口后，fork 的子进程可以继承父进程的文件描述符，因此多个进程可以监听同一个端口。通过pstree 命令我们可以看到： 实现的功能supervisor 在管理fastcgi 的进程中，和管理普通进程的差别是，supervisord 进程会创建socket 链接，共享给 supervisor fork 的fastcgi 进程，但是非fastcgi 的进程不会被共享。","categories":[],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"http://blog.lpflpf.cn/tags/服务管理/"}]},{"title":"Supervisor 的使用和进阶 (1)","slug":"supervisor1","date":"2020-04-03T08:30:42.000Z","updated":"2021-01-11T06:41:06.597Z","comments":true,"path":"passages/supervisor1/","link":"","permalink":"http://blog.lpflpf.cn/passages/supervisor1/","excerpt":"再也不怕进程意外退出。","text":"再也不怕进程意外退出。 supervisor 是Python 开发的一套通用的进程管理程序，用于管理类Unix系统上的应用程序。可以实现对服务的命令行、WEB、XML等方式的管理，实现对服务的启动、重启、关闭等操作。 supervisor 可以干什么 管理进程，对进程进行开启、关闭、重启等服务； 守护管理的进程。当进程关闭后，可以自动重启； 管理一组进程，一组进程同时启动，关闭，重启等服务； 提供事件管理，用于管理的进程触发的事件进行报警的功能（supervisor 3.0 引入）； 提供了对监听同一个unix socket 文件的cgi服务的管理； 提供web服务做服务管理； 提供XMLPRC服务做二次开发； 服务安装以生产环境使用较多的CentOS 为例, 使用yum包管理器即可完成安装，命令如下： 1yum install supervisor 如何非centos系统，则可以使用Python 强大的包管理器pip来完成安装。 1pip install supervisor 安装完成后，生成配置文件。supervisor 提供了 echo_supervisord_conf 命令，用于生成supervisord 的配置文件。如果pip安装，echo_supervisord_conf 会安装在相应pip的目录下。 一般配置文件会保存至/etc/ 目录下，生成方式如下： 12echo_supervisord_conf &gt; /etc/supervisord.confmkdir -p /etc/supervisor.d // supervisord 支持include 的方式将多个配置放置不同文件中, 需要配置文件中指定 supervisor 提供了两个命令给用户： - supervisord supervisor 守护其他服务的进程 - supervisorctl supervisor的命令行工具 最后启动supervisor即可： 1supervisord -c /etc/supervisord.conf PS: 如果使用yum管理安装的，可以直接使用systemctl 管理启动和暂停supervisord。 服务的配置简单介绍两种比较常用的配置。 对简单进程的管理 12345678910111213141516171819202122232425262728293031;[program:theprogramname];command=/bin/cat ; 启动命令;process_name=%(program_name)s ; process_name expr (default %(program_name)s);numprocs=1 ; 同一个任务如果需要启动多次，需要配置此项，并配置process_name为类似于 %(program_name)%02d 格式;directory=/tmp ; 任务执行的当前目录;umask=022 ; 进程文件权限掩码 （默认创建文件为0644;priority=999 ; 优先级;autostart=true ; 是否自动开启。（当supervisord 启动时）;startsecs=1 ; 程序开启 startsec s内不退出;startretries=3 ; 最大尝试次数;autorestart=unexpected ; 是否退出后自动重启 （默认不重启，对于经常意外退出的服务可以开启）;exitcodes=0 ; 判断是否正常退出码;stopsignal=QUIT ; 关闭的信号 （默认时TERM， 也就是Ctrl-C）;stopwaitsecs=10 ; 服务关闭等待事件。若关闭事件超出，则发送 SIGKILL 信号 （也就是 kill -9);stopasgroup=false ; 是否为杀死子进程，默认不杀死。（将会出现未纳入管理的孤儿进程）;killasgroup=false ; 发送SIGKILL 信号的时候，是否杀死子进程;user=chrism ; 启动的用户;redirect_stderr=true ; 将进程的标准输出重定向为标准输出;stdout_logfile=/a/path ; 进程标准输出;stdout_logfile_maxbytes=1MB ; 之日滚动大小，默认50M;stdout_logfile_backups=10 ; 日志最多保留个数;stdout_capture_maxbytes=1MB ; 捕获输出的日志，当事件开启时发送给event_listener （也就是事件监听进程）;stdout_events_enabled=false ; 对于标准输出的情况是否发送event;stdout_syslog=false ; 是否发送到syslog;stderr_logfile=/a/path ; 标准错误输出路径;stderr_logfile_maxbytes=1MB ; 最大标准错误输出的日志文件大小（默认50M）;stderr_logfile_backups=10 ; 日志最多保留个数 （10个） ;stderr_capture_maxbytes=1MB ; 捕获标准错误输出的日志，当 stderr_events_enabled 开启时发送给event_listener;stderr_events_enabled=false ; 对于标准错误输出的情况是否发送event;stderr_syslog=false ; 是否发送错误输出日志到syslog;environment=A=\"1\",B=\"2\" ; 子进程的环境变量设置，可用的变量有 `group_name`, `host_node_name`, `process_num`, `program_name`, `here`。 对进程组的管理除了对单个进程（或者相同的进程）进行控制外，还可以将多个program分组进行控制。 例如有服务 bar，baz, 可以定义进程组： 其他模式如cgi 服务管理、事件监听，后面做详细讨论。 123[group:foo]programs:bar, bazpriority:999 对group做开启，暂停，则对下面的bar，baz都会生效。使用时可以用如下命令: 1supervisorctl [start | stop | restart | status] foo: 当然，可以使用通过 foo:bar 管理bar服务 常见命令行supervisorctl 是supervisor 提供的配套命令行工作，用于对supervisor做命令行控制。 本地服务的启动和暂停本地服务的启动暂停，使用的是 unix socket的方式对supervosrd 发送命令的。因此，使用本机操作命令，必须指定unix socket 的路径。配置如下： 12[supervisorctl]serverurl=unix:///var/tmp/supervisor.sock 命令行操作服务的启动和暂停 1supervisorctl [start | stop | restart | status ] jobname 远程的启动和暂停如果开启了远程操作的端口，也可以通过命令行方式操作远程服务。 1supervisorctl -s hostname:9001 [-u user] [ -p password] [ start | stop | restart | status ] jobname supervisor 配置更新和修改 supervisor 服务配置更新， 并对修改的服务做相应操作 1supervisorctl update supervisor 服务配置更新，并重启所有服务。 1supervisorctl reload 其他操作可以参考supervisor官方文档。 Web 服务supervisor 提供了简约而不简单的操作见面，可以在浏览器端对服务做远程控制。 web 服务需要做如下配置，开启服务监听 1234[inet_http_server] port=*:9001 username= test password=testpass ; 可以不需要账号密码 以下为操作界面： web 服务除了可以对服务做启动暂停等操作外，还可以远程查看应用的日志，监控服务的log 是否正常。这在普通的web 服务中还是比较常用的。 二次开发supervisor 提供了XMLRPC接口用于使用它的人可以二次开发利用。 例如，可以通过远程访问 supervisor 服务控制应用服务的启动、暂停。获取应用服务当前的服务状态等。因此可以通过supervisor 的xmlrpc 监控对supervisor 管理的服务做多服务远程监控 使用xmlrpc时，需要设置inet_http_server, 用于监听rpc和web服务的端口。(建议仅监听内网IP，并设置相应密码)对于PHP服务，我做了简单的封装,可以从Github中获取：github.com/lpflpf/supervisor_phpctl 123456789101112131415161718function monitor($host, $port, $jobname)&#123; $server = new Supervisord($host, $port); $state = $server-&gt;getState(); switch ($state['statename'])&#123; case 'RUNNING': // 服务正常 break; case 'RESTARTING': // 服务重启 break; case 'SHUTDOWN': // 服务关闭 break; case 'FATAL': // 服务出现错误退出 //alarm(); return; &#125;&#125; supervisor 原理 supervisord 管理的任务进程都是supervisord 的子进程, 通过 fork/exec 方式启动子进程。 supervisord 杀死子进程，其实就是发送给子进程一个中断信号（这个信号可以自定义, 参数为stopsignal， 默认为TERM信号） 其他需要强调的点 supervisor 不会随着系统的重启而启动，因此那些依赖supervisor的服务也不会随着系统重启而启动。(别问我是怎么知道的)解决办法也简单。只需要将supervisor 开机启动就行。不同版本操作系统不太一样。 centos 可以用如下方法： 12chkconfig --add supervisordchkconfig supervisord on supervisor 管理的进程可能存在多种状态，在做服务监控时需要注意, 如下为进程状态转移图： 需要注意backoff 状态，当服务不断进行快速关闭重启，则会进入baockoff 状态。这种状态一般也是有问题的。 对于进程组的操作 如果操作进程组中的某个进程，jobname 使用自定义的process_name。 如果操作进程组中的所有进程，使用process_name:* 即可 其他类似服务 runit launchd daemontools systemctl","categories":[],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"http://blog.lpflpf.cn/tags/服务管理/"}],"author":"李朋飞"},{"title":"Python 操作 Excel 和 Word","slug":"python-word-excel","date":"2020-04-02T03:35:00.000Z","updated":"2021-01-11T06:41:06.585Z","comments":true,"path":"passages/python-word-excel/","link":"","permalink":"http://blog.lpflpf.cn/passages/python-word-excel/","excerpt":"有重复的工作，那就尽量让这些搬砖的事情用程序来替代，省下的时间多陪陪家人。","text":"有重复的工作，那就尽量让这些搬砖的事情用程序来替代，省下的时间多陪陪家人。 Python 作为一门解释型语言，又是一种动态类型的语言，其灵活性非常适合编写日常脚本。一些日常不注重效率的需求可以用 Python 来实现。何况Python有足够的开源依赖包供我们使用。本文主要介绍通过 Python 语言实现对 Excel 和 Word 的操作，以及可能出现的坑。 几种选择 Python 对 Excel，Word 的操作选择其实不是很多。主要分类两类。 Win32Com 通过调用Win32Api实现操作Word, Excel, PowerPoint python-docx(word 读写）, python-excel (excel 操作），在Windo32Api 之上实现对Word，Excel 的操作, 文档比较齐全和丰富 Win32ComWindows 对 Excel, Word, PowerPoint 等应用程序会提供专门的Com包供开发者使用。Win32Com 是对包的简单封装，接口层基本无变化。在很多博客中头疼对Win32Com 的接口不是很了解，无法开发。其实Windows 官方提供了详细的接口文档，和 VBA 语言接口是基本一致的。因此可以参考如下文档的实现： Excel Word PowerPoint 举个例子： 在Excel 中，Sheet 有两种形式，Charts 或者 Worksheet, 下面举例从 Chart Sheet 中获取图片，并导出到本地。 1234567891011121314import win32com.client as win32# 从Excel excel_name 的sheet_name 中导出图片保存至picture_name 中def export_picture(excel_name, picture_name, sheet_name): # 获取Excel api excel = win32.gencache.EnsureDispatch(\"Excel.Application\") # 打开Excel 文档, wb 为文件句柄 wb = excel.Workbooks.Open(\"excel_name.xlsx\") # 导出图片 wb.Sheets(\"sheet_name\").Export(\"picture_name.jpg\") wb.Close() python-docx 包使用win32com 操作会有一些不方便，可以使用docx 库。 docx 库使用比较人性化。 doc 是按照回车符分割为一个一个段落、heading 等。因此如果需要插入一个回车符，那就需要插入一个paragraph。 举个例子： 1234567891011121314151617181920212223242526272829303132333435363738import docxdef edit_doc(doc_name, text): doc = docx.Document(doc_name) # 添加文字，并居中 # 此处可直接添加文字，add_paragraph 默认会调用 add_run doc.add_paragraph(text).paragraph_format.aligenment = docx.enum.text.WD_ALIGN_PARAGRAPH.CENTER # 添加空段落 doc.add_paragraph() # 添加 10x10 的表格 rows = 10 cols = 10 table = doc.add_table( rows, cols, style=\"Table Grid\") # 对表格内容进行赋值 for x,y in [(x,y) for x in range(0,9) for y in range (0, 9)]: table.cell(x,y).text = str(x * y) table.cell(x,y).paragraphs[0].paragraph_format.alignment = docx.enum.text.WD_ALIGN_PARAGRAPH.CENTER # 设置 cell 的宽度 table.cell(x,y).width = 25600 * 30 # 第一行表格的合并 table.cell(0,0).merge(table.cell(0,9)) # 插入分页符 doc.add_page_break() # 插入一张图片 # 对word 的编辑，需要通过 add_run() 来实现 doc.add_paragraph().add_run().add_picture(\"pciture_name.jpg\", 25600 * 200, 25600 * 200) # 保存文件 doc.save(\"output.docx\") python-excel 的使用excel 的操作，有两个包, xlrd 用于excel 的读取， xlwt 是用于excel 的写操作。这里只对excel 的读取简单介绍。 1234567891011121314# 获取表格的数据 [0, rows] x [0, cols]def get_table_data(excel_name, sheet_name, rows, cols): result = [] book = xlrd.open_workbook(excel_name) sheet = book.sheet_by_name(sheet_name) for row in range (0, rows): line = [] for col in range (0, cols): line.append(sheet.cell_value(row, col)) result.append(line) return result 遇到的一个问题:如何判断表格内容为空: 123if sheet.cell_type(x,y) is 0: print \"is empty\" 技术总结 操作Word，Excel 的包还是比较丰富的，以上是使用比较多的几个。对于在xlrd, xlwt, docx 中没有实现的接口，可以使用win32com 来实现。如果win32com 无法实现，则可以考虑是否应用程序没有提供相应的接口服务了。","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://blog.lpflpf.cn/tags/Python/"},{"name":"Excel","slug":"Excel","permalink":"http://blog.lpflpf.cn/tags/Excel/"},{"name":"Word","slug":"Word","permalink":"http://blog.lpflpf.cn/tags/Word/"},{"name":"win32com","slug":"win32com","permalink":"http://blog.lpflpf.cn/tags/win32com/"}]},{"title":"Golang 中的一些设计模式","slug":"golang-desgin-pattern","date":"2019-11-04T09:33:05.000Z","updated":"2021-01-11T06:41:06.549Z","comments":true,"path":"passages/golang-desgin-pattern/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-desgin-pattern/","excerpt":"无论什么代码写多了，都会发现有很多套路在里面，坦白的说，那可能就是一种设计模式了，今天也总结一两点 Golang 中常用的设计模式。","text":"无论什么代码写多了，都会发现有很多套路在里面，坦白的说，那可能就是一种设计模式了，今天也总结一两点 Golang 中常用的设计模式。 策略模式策略模式的简单定义一个服务定义一个抽象的接口，而接口可以有多种实现方式，在使用过程中，服务可以对不同的实现做替换。 操作系统中，打开一个.go 文件，可能有很多方式。vscode，sublime，vim等，我们也可以设置默认的打开方式。抽象来看，这些也可以看作是各种策略，可以指定策略来完成我们自定义的操作。 而前提是系统给我们提供了通用的接口，让我们来实现这些策略。 一个简单栗子写过golang的同学一般都会操作数据库，如果要使用 mysql 作为数据源，可能代码需要引入 Golang 的一个包数据驱动包，例子如下： 123456789import _ \"github.com/go-sql-driver/mysql\"import \"database/sql\"func doSomething()&#123; if db, err := sql.Open(\"mysql\", dsn); err == nil &#123; // do something &#125;&#125; 但是import 的时候，程序做了什么，使得驱动得以注册。我们可以从两个地方找到答案，一个是 mysql 的驱动包，一个是 golang 官方的 database/sql 包。 首先，我们可以从database/sql 包看起，官方包中提供了三个方法，用于获取或者操作驱动：要注册一个驱动，需要实现 database/sql/driver 包下 Driver 的接口。 12345678910111213141516171819202122232425262728293031323334// 注册驱动func Register(name string, driver driver.Driver) &#123; driversMu.Lock() defer driversMu.Unlock() if driver == nil &#123; panic(\"sql: Register driver is nil\") &#125; if _, dup := drivers[name]; dup &#123; panic(\"sql: Register called twice for driver \" + name) &#125; drivers[name] = driver&#125;// 删除所有注册的驱动func unregisterAllDrivers() &#123; driversMu.Lock() defer driversMu.Unlock() // For tests. drivers = make(map[string]driver.Driver)&#125;// 当前注册了哪些驱动// Drivers returns a sorted list of the names of the registered drivers.func Drivers() []string &#123; driversMu.RLock() defer driversMu.RUnlock() var list []string for name := range drivers &#123; list = append(list, name) &#125; sort.Strings(list) return list&#125; 而在MySQL的驱动包中， 选择github.com/go-sql-driver/mysql@v1.4.1 包作为例子， driver.go 文件中, 最后有这么一段启动代码： 123func init() &#123; sql.Register(\"mysql\", &amp;MySQLDriver&#123;&#125;)&#125; 从实现中可以看出 *MySQLDriver 便是一种Driver 的一个实现而已。因此，驱动就在直接引入mysql包的时候，悄无声息的被注册到了sql的私有变量 drivers 中了。当引用该驱动时，直接可以读取drivers中相应的驱动方法。 当然，抽象考虑的话，这就是一个策略模式的实现。提供了注册策略的接口，当数据源切换时，可以任意切换相应的驱动（策略）。 如果代码看的不尽兴，我们可以再看一个易懂的例子。 另一个简单的例子Kafka 是一个非常经典的消息队列，Kafka消费者可以按照消费组的方式进行消费，当多个客户端按照同一个消费组消费消费同一个主题（Topic）的消息时，需要按照一定的策略将客户端与Partition的对应关系协调好，这样多个客户端才能正常消费，这就是Consumer Group 的Reblance。 github.com/shopify/sarama 是golang 实现的kafka 客户端的一个比较常用的包。 包中balance_strategy.go文件中是分配策略的一些实现。 其中，分配策略接口定义如下： 12345678type BalanceStrategy interface &#123; // Name uniquely identifies the strategy. Name() string // Plan accepts a map of `memberID -&gt; metadata` and a map of `topic -&gt; partitions` // and returns a distribution plan. Plan(members map[string]ConsumerGroupMemberMetadata, topics map[string][]int32) (BalanceStrategyPlan, error)&#125; 而 balanceStrategy 是该接口的一个简单实现，而这里又实例化了两种策略： BalanceStrategyRange 12345678910func(plan BalanceStrategyPlan, memberIDs []string, topic string, partitions []int32) &#123; step := float64(len(partitions)) / float64(len(memberIDs)) for i, memberID := range memberIDs &#123; pos := float64(i) min := int(math.Floor(pos*step + 0.5)) max := int(math.Floor((pos+1)*step + 0.5)) plan.Add(memberID, topic, partitions[min:max]...) &#125;&#125; BalanceStrategyRoundRobin 123456func(plan BalanceStrategyPlan, memberIDs []string, topic string, partitions []int32) &#123; for i, part := range partitions &#123; memberID := memberIDs[i%len(memberIDs)] plan.Add(memberID, topic, part) &#125;&#125; 不同的策略，可以实现客户端与partition的不同对应关系。如果我们碰到这样一个棘手的问题:需要消费同一个topic，同一个消费组，需要多个服务在不同机器上同时启动，但是机器层次不齐。当流量大时，有些机器负载比较大可能会挂机，那我们可能实现一个reblance策略，将配置高的机器多分配partition，配置低的机器少分配些partition，来满足我们如此个性化（奇葩）的需求了。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.lpflpf.cn/tags/设计模式/"}]},{"title":"Golang 1.13","slug":"golang-1.13","date":"2019-09-19T07:15:00.000Z","updated":"2021-01-11T06:41:06.545Z","comments":true,"path":"passages/golang-1.13/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-1.13/","excerpt":"2019.09.03， golang 发布了新版本，一起来学习下本次修改的内容。","text":"2019.09.03， golang 发布了新版本，一起来学习下本次修改的内容。 二进制数字标识使用 0b 或者 0B 标识二进制数字。 例如： 0b1011 标识11 八进制数字标识使用 0o 或者 0O 标识8 进制整数。例如： 0o660, 目前使用的包含前导0的数字仍旧是合法的。 十六进制浮点数标识使用 0x 或者 0X 是使用十六进制标识浮点数。其中指数是在p标识后面，是2的指数倍。例如 0x1.0p-1021 ，代表 2 ^ -1021 次方 虚数的数字标识虚数虚部的标识已支持已有的所有表达方式，例如: 0i0123i // == 123i for backward-compatibility0o123i // == 0o123 * 1i == 83i0xabci // == 0xabc * 1i == 2748i0.i2.71828i1.e+0i6.67428e-11i1E6i.25i.12345E+5i0x1p-2i // == 0x1p-2 * 1i == 0.25i 数字分割符按照国外按下划线分为多个组, 下划线可以出现在仍以两个数字或者数字前缀和首个数字之间如: 1_000_000, 0b_1010_0110, 3.1415_9265 移位运算符不再需要uint 变量减少了不必要的uint 操作 工具的修改 golang 自带了很多工具命令，每次版本更迭时，可能会做相关工具的更新。 module GO111MODULE 环境继续默认值为auto, 但是auto 默认无论当前目录或者子目录包含go.mod 文件（即使当前目录在GOPATH/src 目录下）均认为开启gomodule。这个修改将简化现有使用GOPATH 的代码和使用gomodule但使用方是使用GOPATH的代码维护。 新增GOPRIVATE 环境变量, 用于标识非共有仓库的数据源 设置GOPROXY 控制代理 GOSUMDB 环境变量标识, 用于验证包的有效性的地址 , 默认为 sum.golang.org/lookup/xxx， 关闭方式 “go env -w GOSUMDB=off” 修改后， go get -u 仅下载当前目录下的依赖包，如果更新所有的依赖包，需要使用 go get -u all go get 不再支持 -m. Go Command go env -w / -u 设置或者删除用户的环境变量值, 环境变量值将被存在 os.UserConfigDir() go version [-m] [-v] [file ...] 若指定了file，则打印响应可执行文件的所使用的go版本， 若使用 -m , 打印嵌入没款的版本信息。如果是一个目录，将打印目录包含的可执行文件的信息和相关子目录的可执行文件的go编译版本。 go build flag 变更 -trimpath 移除所有编译自带的文件系统链接路径，减少编译依赖。 (这个对于服务跨版本迁移相当有帮助) -o 如果传入的是一个已存在的目录，go build 生成的可执行文件将写入此目录中。 -tags 建议使用逗号分割编译标识符,当然空格也在维护，但已经标识为即将废弃。 Compiler toolchain 编译器做了优化，对于逃逸分析更加精准。当然如果需要做回归分析，可以使用 -gcflags=all=-newscape=false 做老的逃逸分析。 Assembler 在 ARM v8.1 上增加了多个原子指令 gofmt 主要对于 数字样式变更的修改。 godoc godoc 不再在golang 包中出现，需要通过 go get golang.org/x/tools/cmd/godoc 安装 runtime defer 性能提升 30% Core Library TLS 1.3 支持。 在crypt/tls 包中，默认支持 TLS 1.3 crypto/ed25519 golang.org/x/crypt/ed25519包迁移至 crypto/ed25519 中。该包是 Ed25519 数字签名算法的实现。 Error wrapping golang 支持错误包装。 一个错误 e 可以包含另外一个错误w. e 通过调用 Unwrap 方法可以拿到错误w fmt.Errorf 可以通过 %w 创建一个wrapped 错误 errors 包提供了 errors.Unwrap, errors.Is errors.As 三个方法用于错误包含的判定。 其他类库的小修改 bytes.ToValidUTF8() 方法。 替换不合法的u8 编码为指定的字符。 `context crypto/tls包中，sslv3 再1.13 标记为即将废弃，在 go1.14 将被移除 crypto/x509 database/sql NullTime 类型代表可能为null 的time.Time；NullInt32 代表 可能为null 的 int32 类型 debug/dwarf errors 添加 As, Is, UnWrap 方法 fmt “%x %X” 支持 浮点数和复数的16进制格式化 “%0” 输出带有前导0o的8进制数 Errorf 添加 “%w”, 用于生成错误包装函数 go/scanner go/types html/template log math/big Rat 增加了函数 Rat.SetUint64(), Rat.SetString 支持非十进制浮点数表达 net net/http os 新的UserConfigDir 方法返回用户配置目录的根目录 如果File 通过 O_APPEND 标识打开， WriteAt 方法不可用，将返回错误。 os/exec windows 中，Cmd 的环境变量线性继承自 %SYSTEMROOT% 的值，除非Cmd.Env 显性赋值。 reflect 新增 Value.IsZero 方法，判断是否为0值 MakeFunc 允许在返回值的类型上做复制转换。有利于那种定义了抽象返回类型，而实现是一个具体返回值的方法调用 runtime strconv strings sync 通过编译优化，将 Mutex.Lock, Mutex.Unlock, RWMutex.Lock, RWMutex.RUnlock, Once.Do 编译优化inlining 化。对于amd64上无竞争的互斥量, Once.Do 快了一倍， Mutex/RWMutex 快10% syncall syscall/js testing text/scanner text/template time unicode go 1.13 release note","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"}]},{"title":"消息队列 NSQ 源码学习笔记 (五)","slug":"nsqd-study-5","date":"2019-09-12T03:08:31.000Z","updated":"2021-01-11T06:41:06.583Z","comments":true,"path":"passages/nsqd-study-5/","link":"","permalink":"http://blog.lpflpf.cn/passages/nsqd-study-5/","excerpt":"NSQ 的拓扑结构和生产消费端配置","text":"NSQ 的拓扑结构和生产消费端配置 单机模式部署NSQD 是可以脱离 nsqlookup 做单机部署的。由于 nsqd 足够轻量，可以把服务部署在消息发布的服务器上，加快 pub 消息的速度，也能兼顾消费端消息的分发 集群模式NSQD 是一个SPOF的系统，每个服务可以独立部署。当采用集群模式时，建议开启nsqlookup服务，用于管理多个 nsqd 的服务 一般的消息队列都会提供rebalance 的功能，nsqd 是没有的。不过可以通过nsq_to_nsq 做消息的复制，做服务的主备，当服务挂机后，可以切换到另外的服务器做消费。（中间channel 不会切换，因此可能会重复消费，或者丢一定消息）nsqd 正常情况下，如果配置合理，消息是不会落地的。如果需要落地，可以使用nsq_to_file, 新建一个channel订阅 相关topic, 把消息落地到硬盘。 在集群模式下，可以部署多个 nsqlookupd 服务, 这些服务之间是互相没有依赖的，nsqd 在做消息广播的时候，会对每一个nsqlookupd的服务遍历一次，更新服务上的信息 生产官方建议的生产方式，是通过 http 请求直接 pub 消息到nsq. 当然，大部分的nsq的客户端也实现了nsq 的消息发布功能 消费消息队列的实现，一般都是推模型、拉模型或者推拉结合。在 nsq 中，是使用推模型，因此需要使用客户端来做消息的接收。由于 nsq 消费的协议足够简单，也可以自行建立一个tcp 连接，做消息和连接的管理。 配置参数详解nsqlookupd 配置 12345678910111213141516171819202122-broadcast-address string nsqd, client 访问的地址 address of this lookupd node, (default to the OS hostname) (default \"tempt463.ops.shbt.qihoo.net\")-http-address string http 监听 &lt;addr&gt;:&lt;port&gt; to listen on for HTTP clients (default \"0.0.0.0:4161\")-tcp-address string tcp 监听 &lt;addr&gt;:&lt;port&gt; to listen on for TCP clients (default \"0.0.0.0:4160\")-config string path to config file-log-level value set log verbosity: debug, info, warn, error, or fatal (default INFO)-log-prefix string log message prefix (default \"[nsqlookupd] \")-inactive-producer-timeout duration duration of time a producer will remain in the active list since its last ping (default 5m0s)-tombstone-lifetime duration duration of time a producer will remain tombstoned if registration remains (default 45s)-version print version string nsqd 配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112-config string path to config file-data-path string path to store disk-backed messages-auth-http-address value 授权服务地址 &lt;addr&gt;:&lt;port&gt; to query auth server (may be given multiple times)-lookupd-tcp-address value nslookupd tcp 地址, 广播使用 lookupd TCP address (may be given multiple times)-broadcast-address string nslookupd 地址 address that will be registered with lookupd (defaults to the OS hostname) (default \"tempt463.ops.shbt.qihoo.net\")-tcp-address string &lt;addr&gt;:&lt;port&gt; to listen on for TCP clients (default \"0.0.0.0:4150\")// http 服务-http-address string &lt;addr&gt;:&lt;port&gt; to listen on for HTTP clients (default \"0.0.0.0:4151\")-http-client-connect-timeout duration timeout for HTTP connect (default 2s)-http-client-request-timeout duration timeout for HTTP request (default 5s)// https 服务-https-address string &lt;addr&gt;:&lt;port&gt; to listen on for HTTPS clients (default \"0.0.0.0:4152\")-tls-cert string path to certificate file-tls-client-auth-policy string client certificate auth policy ('require' or 'require-verify')-tls-key string path to key file-tls-min-version value minimum SSL/TLS version acceptable ('ssl3.0', 'tls1.0', 'tls1.1', or 'tls1.2') (default 769)-tls-required require TLS for client connections (true, false, tcp-https)-tls-root-ca-file string path to certificate authority file// 日志-log-level value set log verbosity: debug, info, warn, error, or fatal (default INFO)-log-prefix string log message prefix (default \"[nsqd] \")-msg-timeout duration default duration to wait before auto-requeing a message (default 1m0s)-max-body-size int 命令消息体大小限制 maximum size of a single command body (default 5242880)-max-msg-size int 单条消息限制 maximum size of a single message in bytes (default 1048576)-max-msg-timeout duration 消息超时时间 touch 命令也不能超过该限制 maximum duration before a message will timeout (default 15m0s)-node-id int snowflake 算法中，生成message 的一部分, 为保证消息的唯一性，多个nsqd 需要不同的nodeid unique part for message IDs, (int) in range [0,1024) (default is hash of hostname) (default 781)-max-rdy-count int 客户端可以批量处理的个数 maximum RDY count for a client (default 2500)-max-req-timeout duration 延时消息，最大可延时时间， 默认不超过1h maximum requeuing timeout for a message (default 1h0m0s)-mem-queue-size int 内存队列大小 number of messages to keep in memory (per topic/channel) (default 10000)-max-channel-consumers int 每个channel 最多有多少个消费者 maximum channel consumer connection count per nsqd instance (default 0, i.e., unlimited)-max-heartbeat-interval duration 客户端的心跳, 默认间隔最大为1分钟 maximum client configurable duration of time between client heartbeats (default 1m0s)-max-output-buffer-size int 输出最大buffer maximum client configurable size (in bytes) for a client output buffer (default 65536)-max-output-buffer-timeout duration 最大buffer 超时，如果时间超过，刷新到客户端 maximum client configurable duration of time between flushing to a client (default 30s)-min-output-buffer-timeout duration 最小buffer 超时时间, 尽量减少高频词写客户端 minimum client configurable duration of time between flushing to a client (default 25ms)-output-buffer-timeout duration 默认的客户端刷新时间， 可以通过 IDENTIFY 协议修改 default duration of time between flushing data to clients (default 250ms)stats 相关-statsd-address string UDP &lt;addr&gt;:&lt;port&gt; of a statsd daemon for pushing stats-statsd-interval duration duration between pushing to statsd (default 1m0s)-statsd-mem-stats toggle sending memory and GC stats to statsd (default true)-statsd-prefix string prefix used for keys sent to statsd (%s for host replacement) (default \"nsq.%s\")-statsd-udp-packet-size int the size in bytes of statsd UDP packets (default 508)-e2e-processing-latency-percentile value message processing time percentiles (as float (0, 1.0]) to track (can be specified multiple times or comma separated '1.0,0.99,0.95', default none)-e2e-processing-latency-window-time duration calculate end to end latency quantiles for this duration of time (ie: 60s would only show quantile calculations from the past 60 seconds) (default 10m0s)diskqueue 相关-max-bytes-per-file int // 磁盘队列单文件大小 默认 100M number of bytes per diskqueue file before rolling (default 104857600)-sync-every int // 默认不超过 2500 个消息将刷一次盘 number of messages per diskqueue fsync (default 2500)-sync-timeout duration // 默认不超过 2s 将刷一次盘 duration of time per diskqueue fsync (default 2s)消息压缩-snappy enable snappy feature negotiation (client compression) (default true)-deflate enable deflate feature negotiation (client compression) (default true)-max-deflate-level int max deflate compression level a client can negotiate (&gt; values == &gt; nsqd CPU usage) (default 6)-version print version string 总结总体来说，nsq 的优势在于足够轻量级，消费速度够快，没有单点问题。但缺点也显而易见：消息是不保序的，并且无法做自动的reblance.","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"http://blog.lpflpf.cn/tags/nsq/"},{"name":"messageQueue","slug":"messageQueue","permalink":"http://blog.lpflpf.cn/tags/messageQueue/"}]},{"title":"消息队列 NSQ 源码学习笔记 (四)","slug":"nsqd-study-4","date":"2019-09-10T10:09:55.000Z","updated":"2021-01-11T06:41:06.583Z","comments":true,"path":"passages/nsqd-study-4/","link":"","permalink":"http://blog.lpflpf.cn/passages/nsqd-study-4/","excerpt":"nsq 工具集学习","text":"nsq 工具集学习 nsq_to_nsqnsq 作为消息队列，有个优势是nsqd 各节点之间是不关联的，如果一个节点出了问题，仅仅影响该节点下的topic，channel，以及相关的生产者、消费者。 也就是官方说明的特性第一条：no SPOF ( single point of failure 单点故障)。好处不言而喻，坏处也是有的，如果节点出问题，没有备份数据无法恢复。 所以，在官方提供了 nsq_to_nsq 作为 nsqd 节点复制的工具，用于做 nsqd 节点数据的备份, 或者也可以用于数据的分发。 类似于MirrorMaker. 特性： 支持将M 个 topic 的消息 publish 到 N 个 nsqd 上, 其中 M &gt;= 1 , N &gt;= 1. 也就是copy 是支持多对多的。 多对多的模式支持两种： RoundRobin 模式 对下游的nsqd 服务做轮询。 HostPool 模式 随机获取一个host，并发送 总结 由于nsqd 本身是不保序的，因此nsq_to_nsq 也是此特性，在复制数据和分发的时候，如果有多个接收的nsqd，并不能保证消息分发到相同的nsqd，因此无法保序。 nsq_to_file除了使用nsq_to_nsq做节点备份外，也可以通过数据落地的方式，做消息的物理备份。nsq_to_file 可以将nsq接收到的数据，落地到硬盘。如需数据恢复，可以通过读取文件数据，重新生产即可。 nsq_tailtail 查看 topic 的消息，打印topic 数据到标准输出。 nsq_stat命令行打印服务的stats 12345---------------depth---------------+--------------metadata--------------- total mem disk inflt def | req t-o msgs clients 24660 24660 0 0 20 | 102688 0 132492418 1 25001 25001 0 0 20 | 102688 0 132493086 1 21132 21132 0 0 21 | 102688 0 132493729 1 to_nsq命令行 push 消息到 topic , 默认换行符分割多条消息。指定多个nsq，将同时向多个nsq 发布消息 nsq_to_http提供了一个 HTTP 推送的服务，将 TCP 消息的数据转化为 HTTP 请求，发送给消费端（支持 GET or POST 协议的web 服务）。","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"http://blog.lpflpf.cn/tags/nsq/"},{"name":"消息队列","slug":"消息队列","permalink":"http://blog.lpflpf.cn/tags/消息队列/"}]},{"title":"消息队列 NSQ 源码学习笔记 (三)","slug":"nsqd-study-3","date":"2019-09-06T02:44:36.000Z","updated":"2021-01-11T06:41:06.583Z","comments":true,"path":"passages/nsqd-study-3/","link":"","permalink":"http://blog.lpflpf.cn/passages/nsqd-study-3/","excerpt":"NSQD 源码学习","text":"NSQD 源码学习 NSQD 学习笔记特性总结 消息投放是不保序的 原因是内存队列、持久化队列、以及重新消费的数据混合在一起消费导致的 多个consumer 订阅同一个channel，消息将随机发送到不同的consumer 上 消息是可靠的 当消息发送出去之后，会进入in_flight_queue 队列 当恢复FIN 之后，才会从队列中将消费成功的消息清除 如果客户端发送REQ，消息将会重发 消息发送采用的是推模式，减少延迟 支持延迟消费的模式: DPUB, 或者 RRQ (消费未成功，延时消费) 命令 代码学习程序入口程序入口 github.com/nsq/apps/nsqd/main.go 获取配置，并从metadata 的持久化文件中读取topic、channel 信息。meta 信息格式: 12345678Topics []struct &#123; Name string `json:\"name\"` Paused bool `json:\"paused\"` Channels []struct &#123; Name string `json:\"name\"` Paused bool `json:\"paused\"` &#125; `json:\"channels\"`&#125; `json:\"topics\"` 启动nsqd.Main 程序, 端口监听TCP 服务 和 HTTP 服务（支持HTTPS）。 启动事件循环 queueScanLoop 处理 in-flight 消息 和 deferred 消息队列事件的协程 lookupLoop 处理与 nsqlookup 交互的协程。 包括消息的广播，lookup 节点的更新等。 如果配置了状态监听的地址，则会启动 statsdLoop 协程，用于定时发送(UDP)当前服务的各类状态 Topic 处理数据结构 123456789101112131415161718192021222324252627type Topic struct &#123; // 64bit atomic vars need to be first for proper alignment on 32bit platforms messageCount uint64 // 消息数量 messageBytes uint64 // 消息字节数 sync.RWMutex // 结构体读写锁 name string channelMap map[string]*Channel // 保存topic 下所有channel backend BackendQueue // 落地的消息队列 memoryMsgChan chan *Message // 内存中的消息 startChan chan int // topic 被订阅了，可以启动消费了 exitChan chan int // 协程退出channel channelUpdateChan chan int // channel 更新的消息 waitGroup util.WaitGroupWrapper exitFlag int32 // 退出标记 idFactory *guidFactory // uuid 生成器 ephemeral bool // 是否为临时topic deleteCallback func(*Topic) // 临时topic，自动删除相关channel deleter sync.Once paused int32 pauseChan chan int // 暂停的信号 ctx *context // 上下文，保存nsqd &#125; topic 的创建 初始化内存队列 初始化diskqueue 初始化topic 相应的 msg 唯一id生成器 向nsqlookup 广播，添加topic信息 等待事件处理 (consumer 和 channel 相关) 只有consumer 存在topic 订阅(Sub)之后，才会启动 Topic 的事件处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859for &#123; select &#123; // 消息可从二者中随机获取，所以topic 中消息是不保序的 case msg = &lt;-memoryMsgChan: // 内存消息 case buf = &lt;-backendChan: // 持久化文件中推送的消息 msg, err = decodeMessage(buf) if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, \"failed to decode message - %s\", err) continue &#125; case &lt;-t.channelUpdateChan: // 更新channel，则会增加topic 下发的列表 chans = chans[:0] t.RLock() for _, c := range t.channelMap &#123; chans = append(chans, c) &#125; t.RUnlock() if len(chans) == 0 || t.IsPaused() &#123; memoryMsgChan = nil backendChan = nil &#125; else &#123; memoryMsgChan = t.memoryMsgChan backendChan = t.backend.ReadChan() &#125; continue case &lt;-t.pauseChan: // 暂停topic，则所有chan 都暂停 if len(chans) == 0 || t.IsPaused() &#123; memoryMsgChan = nil backendChan = nil &#125; else &#123; memoryMsgChan = t.memoryMsgChan backendChan = t.backend.ReadChan() &#125; continue case &lt;-t.exitChan: goto exit &#125; for i, channel := range chans &#123; // 将topic 收到的消息广播到 topic 下所有的channel 中 chanMsg := msg // 考虑比较周全的是，减少一次message 的创建 if i &gt; 0 &#123; chanMsg = NewMessage(msg.ID, msg.Body) chanMsg.Timestamp = msg.Timestamp chanMsg.deferred = msg.deferred &#125; if chanMsg.deferred != 0 &#123; // 如果是defer 的消息，会添加到channel 的defer 队列中 channel.PutMessageDeferred(chanMsg, chanMsg.deferred) continue &#125; err := channel.PutMessage(chanMsg) // 正常消息，直接添加到channel 中 if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, \"TOPIC(%s) ERROR: failed to put msg(%s) to channel(%s) - %s\", t.name, msg.ID, channel.name, err) &#125; &#125;&#125; 值得关注的topic 操作 putMessage 1234567891011121314151617func (t *Topic) put(m *Message) error &#123; select &#123; case t.memoryMsgChan &lt;- m: default: b := bufferPoolGet() err := writeMessageToBackend(b, m, t.backend) bufferPoolPut(b) t.ctx.nsqd.SetHealth(err) if err != nil &#123; t.ctx.nsqd.logf(LOG_ERROR, \"TOPIC(%s) ERROR: failed to write message to backend - %s\", t.name, err) return err &#125; &#125; return nil&#125; 利用了golang chan 阻塞的原理，当 memoryMsgChan 满了之后，case t.memoryMsgChan &lt;- m 无法执行，会执行 default 操作，自动添加消息到硬盘中。 messageId 的生成，使用了业界常用的snowflake 算法。 Channel 处理channel 没有自己的事件操作，都是通过被动执行相关操作。 数据结构 12345678910111213141516171819202122232425262728293031323334353637type Channel struct &#123; // 64bit atomic vars need to be first for proper alignment on 32bit platforms requeueCount uint64 // 重新消费的message 个数 messageCount uint64 // 消息总数 timeoutCount uint64 // 消费超时的message 个数 sync.RWMutex topicName string name string ctx *context backend BackendQueue // 落地的队列 memoryMsgChan chan *Message // 内存中的消息 exitFlag int32 // 退出标识 exitMutex sync.RWMutex // state tracking clients map[int64]Consumer // 支持多个client 消费，但是一条消息仅能被某一个client 消费 paused int32 // 暂停标识 ephemeral bool // 临时 channel 标识 deleteCallback func(*Channel) // 删除的回调函数 deleter sync.Once // Stats tracking e2eProcessingLatencyStream *quantile.Quantile // TODO: these can be DRYd up deferredMessages map[MessageID]*pqueue.Item // defer 消息保存的map deferredPQ pqueue.PriorityQueue // defer 队列 (优先队列保存) deferredMutex sync.Mutex // 相关的互斥锁 inFlightMessages map[MessageID]*Message // 正在消费的消息保存的map inFlightPQ inFlightPqueue // 正在消费的消息保存在优先队列 (优先队列保存) inFlightMutex sync.Mutex // 相关的互斥锁&#125; 事件循环处理在启动nsqd 时，会启动一些事件循环的处理。 channel 队列处理 channel 有有两个重要队列： defer队列和inflight 队列, 事件处理主要是对两个队列的消息数据做处理 扫描channel 规则 更新 channels 的频率为100ms 刷新表的频率为 5s 默认随机选择20( queue-scan-selection-count ) 个channels 做消息队列调整 默认处理队列的协程数量不超过 4 ( queue-scan-worker-pool-max ) processInFlightQueue 做消息处理超时重发处理 flight 队列中，保存的是推送到消费端的消息，优先队列中，按照time排序, 消息已经发送的时间越久越靠前 定时从flight 队列中获取最久的消息，如果已超时( 超过 msg–time )，则将消息重新发送 processDeferdQueue 处理延迟队列的消息 deferd 队列中，保存的是延迟推送的消息，优先队列中，按照time排序，距离消息要发送的时间越短，越靠前 定时从deferd 队列中获取最近需要发送的消息，如果消息已达到发送时间，则pop 消息，将消息发送 lookup 事件响应 此处的事件循环，是用于和lookupd 交户使用的事件处理模块。例如Topic 增加或者删除， channel 增加或者删除 需要对所有 nslookupd 模块做消息广播等处理逻辑，均在此处实现。 主要的事件: 定时心跳操作 每隔 15s 发送 PING 到 所有 nslookupd 的节点上 topic,channel新增删除操作 发送消息到所有 nslookupd 的节点上 配置修改的操作 如果配置修改，会重新从配置中刷新一次 nslookupd 节点 消费协程事件处理 当一个客户端与nsqd 通过TCP建立连接后，将启动protocolV2.messagePump 协程，用于处理消息的交户,主协程用于做事件的响应。 messagePump: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125func (p *protocolV2) messagePump(client *clientV2, startedChan chan bool) &#123; var err error var memoryMsgChan chan *Message var backendMsgChan chan []byte var subChannel *Channel var flusherChan &lt;-chan time.Time var sampleRate int32 subEventChan := client.SubEventChan identifyEventChan := client.IdentifyEventChan outputBufferTicker := time.NewTicker(client.OutputBufferTimeout) heartbeatTicker := time.NewTicker(client.HeartbeatInterval) // 客户端超时时间的一半， 默认为30s heartbeatChan := heartbeatTicker.C msgTimeout := client.MsgTimeout flushed := true close(startedChan) for &#123; if subChannel == nil || !client.IsReadyForMessages() &#123; memoryMsgChan = nil backendMsgChan = nil flusherChan = nil client.writeLock.Lock() err = client.Flush() client.writeLock.Unlock() if err != nil &#123; goto exit &#125; flushed = true &#125; else if flushed &#123; memoryMsgChan = subChannel.memoryMsgChan backendMsgChan = subChannel.backend.ReadChan() flusherChan = nil &#125; else &#123; memoryMsgChan = subChannel.memoryMsgChan backendMsgChan = subChannel.backend.ReadChan() flusherChan = outputBufferTicker.C // 如果动态设置了flusher 的定时器，则使用这个定时器刷新 &#125; select &#123; case &lt;-flusherChan: client.writeLock.Lock() err = client.Flush() // 把writer flush client.writeLock.Unlock() if err != nil &#123; goto exit &#125; flushed = true case &lt;-client.ReadyStateChan: case subChannel = &lt;-subEventChan: // 一个consumer 同一个tcp 连接，只能订阅一个topic subEventChan = nil case identifyData := &lt;-identifyEventChan: // 客户端认证 identifyEventChan = nil outputBufferTicker.Stop() if identifyData.OutputBufferTimeout &gt; 0 &#123; outputBufferTicker = time.NewTicker(identifyData.OutputBufferTimeout) &#125; heartbeatTicker.Stop() heartbeatChan = nil if identifyData.HeartbeatInterval &gt; 0 &#123; // 设置刷新时间 heartbeatTicker = time.NewTicker(identifyData.HeartbeatInterval) heartbeatChan = heartbeatTicker.C &#125; if identifyData.SampleRate &gt; 0 &#123; // 可以设置采样数据，采样输出数据 sampleRate = identifyData.SampleRate &#125; msgTimeout = identifyData.MsgTimeout // identify 可以设置消息的超时事件 case &lt;-heartbeatChan: // 心跳消息 err = p.Send(client, frameTypeResponse, heartbeatBytes) if err != nil &#123; goto exit &#125; case b := &lt;-backendMsgChan: // 硬盘消息推送到consumer 中 if sampleRate &gt; 0 &amp;&amp; rand.Int31n(100) &gt; sampleRate &#123; continue &#125; msg, err := decodeMessage(b) // 硬盘消息保存为二进制，需要解码 if err != nil &#123; p.ctx.nsqd.logf(LOG_ERROR, \"failed to decode message - %s\", err) continue &#125; msg.Attempts++ // 设置超时事件，并将消息放入flight 队列中 subChannel.StartInFlightTimeout(msg, client.ID, msgTimeout) client.SendingMessage() err = p.SendMessage(client, msg) if err != nil &#123; goto exit &#125; flushed = false case msg := &lt;-memoryMsgChan: // 内存消息推送到consumer if sampleRate &gt; 0 &amp;&amp; rand.Int31n(100) &gt; sampleRate &#123; continue &#125; msg.Attempts++ // 设置超时事件，并将消息放入flight 队列中 subChannel.StartInFlightTimeout(msg, client.ID, msgTimeout) client.SendingMessage() err = p.SendMessage(client, msg) if err != nil &#123; goto exit &#125; flushed = false case &lt;-client.ExitChan: goto exit &#125; &#125;exit: p.ctx.nsqd.logf(LOG_INFO, \"PROTOCOL(V2): [%s] exiting messagePump\", client) heartbeatTicker.Stop() outputBufferTicker.Stop() if err != nil &#123; p.ctx.nsqd.logf(LOG_ERROR, \"PROTOCOL(V2): [%s] messagePump error - %s\", client, err) &#125;&#125; HTTP 传输协议 METHOD ROUTE PARAM INFO GET /ping - 如果服务器正常，返回 OK GET /info - 返回服务器的相关信息 POST /pub topicName, [defer] 消息发布, 可以选择 defer 发布 POST /mpub topicName, [binary] 多条消息的发布， 可以支持二进制消息的发布, 消息格式为 (msgNum + (msgSize + msg) * msgNum), 非binary 模式，则按照换行符分割消息 GET /stats format, topic, channel, include_clients 获取响应服务的状态， 可以通过topic, channel 过滤. POST /topic/create topic 创建一个 topic POST /topic/delete topic 删除一个 topic POST /topic/empty topic 清空一个 topic POST /topic/pause topic 暂停一个 topic POST /topic/unpause topic 启动一个暂停的 topic POST /channel/create topic, channel 创建一个 channel POST /channel/delete topic, channel 删除一个 channel POST /channel/empty topic, channel 清空一个channel， 包括 内存中的队列 和 硬盘中的队列 POST /channel/pause topic, channel 暂停一个 channel POST /channel/unpause topic, channel 启动一个暂停的 channel PUT /config/:opt nsqlookupd_tcp_addresses, log_level 修改 nsqlookupd 的地址，或者 日志级别 GET/POST /debug - something TCP 传输协议 PROTOCAL PARAM 解释 IDENTIFY Body (len + data) 客户端认证, body 采用 json 格式, 主要提供消息消费相关参数信息 FIN msgId 消息消费完成 RDY size 若客户端准备好接收消息，将发送RDY 命令，设置消费端可等待的消息量（类似批量消息）。设置为0，则暂停接收 REQ msgId, timeoutMs 将 in_flight_queue 队列中的消息放到 deferd 队列中，延时消费 （可以认为是消费失败的消息的一种处理方式） PUB topicName , Body (len + msg) 消息生产者发布消息到Topic 队列 MPUB TopicName, Body （len + msgNum + (msgSize + msg ) * msgNum 消息生产着发布多条消息到Topic队列 DPUB TopicNmae, timeoutMs, Body (len + msg) 消息生产者发布定时消息到Topic 队列 NOP - 空消息 TOUCH msgId 重置在 in_flight_queue 队列中的消息的超时时间 SUB topicName, channelName 消费端通过某个channel订阅某个topic 消息，订阅成功后，将通过 messagePump 推送消息到消费端 CLS - 消费端暂停接收消息, 等待关闭 AUTH body 授权 学习总结 nsqd 消息id 生成方法采用的uuid 生成算法 snowflake 算法 in_flight_queue 和 delay_queue 实现都是使用堆排序实现的优先队列 从M 个channel 中随机筛选N个channel 做队列队列扫描, 每次获取的概率相同 1234567891011121314151617181920func UniqRands(quantity int, maxval int) []int &#123; if maxval &lt; quantity &#123; quantity = maxval &#125; intSlice := make([]int, maxval) for i := 0; i &lt; maxval; i++ &#123; intSlice[i] = i &#125; // 每次从[i, maxval] 中筛选 1 个元素，放到位置 i 中 for i := 0; i &lt; quantity; i++ &#123; j := rand.Int()%maxval + i // swap intSlice[i], intSlice[j] = intSlice[j], intSlice[i] maxval-- &#125; return intSlice[0:quantity]&#125;","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"http://blog.lpflpf.cn/tags/nsq/"},{"name":"消息队列","slug":"消息队列","permalink":"http://blog.lpflpf.cn/tags/消息队列/"}]},{"title":"消息队列 NSQ 源码学习笔记 (二)","slug":"nsqd-study-2","date":"2019-09-04T03:01:39.000Z","updated":"2021-01-11T06:41:06.582Z","comments":true,"path":"passages/nsqd-study-2/","link":"","permalink":"http://blog.lpflpf.cn/passages/nsqd-study-2/","excerpt":"NSQ 消息队列实现消息落地使用的是 FIFO 队列。实现为 diskqueue , 使用包 github.com/nsqio/go-diskqueue ,本文主要对 diskqueue的实现做介绍。","text":"NSQ 消息队列实现消息落地使用的是 FIFO 队列。实现为 diskqueue , 使用包 github.com/nsqio/go-diskqueue ,本文主要对 diskqueue的实现做介绍。 功能定位 在NSQ 中， diskqueue 是一个实例化的 BackendQueue, 用于保存在内存中放不下的消息。使用场景如Topic 队列中的消息，Channel 队列中的消息 实现的功能是一个FIFO的队列，实现如下功能: 支持消息的插入、清空、删除、关闭操作 可以返回队列的长度(写和读偏移的距离) 具有读写功能，FIFO 的队列 diskqueue 的实现 BackendQueue 接口如下： 12345678type BackendQueue interface &#123; Put([]byte) error // 将一条消息插入到队列中 ReadChan() chan []byte // 返回一个无缓冲的chan Close() error // 队列关闭 Delete() error // 删除队列 （实际在实现时，数据仍保留） Depth() int64 // 返回读延迟的消息量 Empty() error // 清空消息 （实际会删除所有的记录文件）&#125; 数据结构对于需要原子操作的64bit 的字段，需要放在struct 的最前面，原因请看学习总结第一条数据结构中定义了 文件的读写位置、一些文件读写的控制变量，以及相关操作的channel. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// diskQueue implements a filesystem backed FIFO queuetype diskQueue struct &#123; // 64bit atomic vars need to be first for proper alignment on 32bit platforms // run-time state (also persisted to disk) readPos int64 // 读的位置 writePos int64 // 写的位置 readFileNum int64 // 读文件的编号 writeFileNum int64 // 写文件的编号 depth int64 // 读写文件的距离 (用于标识队列的长度) sync.RWMutex // instantiation time metadata name string // 标识队列名称，用于落地文件名的前缀 dataPath string // 落地文件的路径 maxBytesPerFile int64 // 每个文件最大字节数 minMsgSize int32 // 单条消息的最小大小 maxMsgSize int32 // 单挑消息的最大大小 syncEvery int64 // 每写多少次刷盘一次 syncTimeout time.Duration // 至少多久会刷盘一次 exitFlag int32 // 退出标识 needSync bool // 如果 needSync 为true， 则需要fsync刷新metadata 数据 // keeps track of the position where we have read // (but not yet sent over readChan) nextReadPos int64 // 下一次读的位置 nextReadFileNum int64 // 下一次读的文件number readFile *os.File // 读 fd writeFile *os.File // 写 fd reader *bufio.Reader // 读 buffer writeBuf bytes.Buffer // 写 buffer // exposed via ReadChan() readChan chan []byte // 读channel // internal channels writeChan chan []byte // 写 channel writeResponseChan chan error // 同步写完之后的 response emptyChan chan int // 清空文件的channel emptyResponseChan chan error // 同步清空文件后的channel exitChan chan int // 退出channel exitSyncChan chan int // 退出命令同步等待channel logf AppLogFunc // 日志句柄&#125; 初始化一个队列初始化一个队列，需要定义前缀名， 数据路径，每个文件的最大字节数，消息最大最小限制，以及刷盘频次和最长刷盘时间，最后还有一个日志函数 123456789101112131415161718192021222324252627282930func New(name string, dataPath string, maxBytesPerFile int64, minMsgSize int32, maxMsgSize int32, syncEvery int64, syncTimeout time.Duration, logf AppLogFunc) Interface &#123; d := diskQueue&#123; name: name, dataPath: dataPath, maxBytesPerFile: maxBytesPerFile, minMsgSize: minMsgSize, maxMsgSize: maxMsgSize, readChan: make(chan []byte), writeChan: make(chan []byte), writeResponseChan: make(chan error), emptyChan: make(chan int), emptyResponseChan: make(chan error), exitChan: make(chan int), exitSyncChan: make(chan int), syncEvery: syncEvery, syncTimeout: syncTimeout, logf: logf, &#125; // no need to lock here, nothing else could possibly be touching this instance err := d.retrieveMetaData() if err != nil &amp;&amp; !os.IsNotExist(err) &#123; d.logf(ERROR, \"DISKQUEUE(%s) failed to retrieveMetaData - %s\", d.name, err) &#125; go d.ioLoop() return &amp;d&#125; 可以看出, 队列中均使用不带cache 的chan，消息只能阻塞处理。 d.retrieveMetaData() 是从文件中恢复元数据。 d.ioLoop() 是队列的事件处理逻辑，后文详细解答 消息的读写文件格式文件名 &quot;name&quot; + .diskqueue.%06d.dat 其中， name 是 topic, 或者topic + channel 命名.数据采用二进制方式存储， 消息大小+ body 的形式存储。 消息读操作 如果readFile 文件描述符未初始化， 则需要先打开相应的文件，将偏移seek到相应位置，并初始化reader buffer 初始化后，首先读取文件的大小， 4个字节，然后通过文件大小获取相应的body 数据 更改相应的偏移。如果偏移达到文件最大值，则会关闭相应文件，读的文件编号 + 1 消息写操作 如果writeFile 文件描述符未初始化，则需要先打开相应的文件，将偏移seek到文件末尾。 验证消息的大小是否符合要求 将body 的大小和body 写入 buffer 中，并落地 depth +1， 如果文件大小大于每个文件的最大大小，则关闭当前文件，并将写文件的编号 + 1 事件循环 ioLoopioLoop 函数，做所有时间处理的操作,包括： 消息读取 写操作 清空队列数据 定时刷新的事件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273func (d *diskQueue) ioLoop() &#123; var dataRead []byte var err error var count int64 var r chan []byte // 定时器的设置 syncTicker := time.NewTicker(d.syncTimeout) for &#123; // 若到达刷盘频次，标记等待刷盘 if count == d.syncEvery &#123; d.needSync = true &#125; if d.needSync &#123; err = d.sync() if err != nil &#123; d.logf(ERROR, \"DISKQUEUE(%s) failed to sync - %s\", d.name, err) &#125; count = 0 &#125; // 有可读数据，并且当前读chan的数据已经被读走，则读取下一条数据 if (d.readFileNum &lt; d.writeFileNum) || (d.readPos &lt; d.writePos) &#123; if d.nextReadPos == d.readPos &#123; dataRead, err = d.readOne() if err != nil &#123; d.logf(ERROR, \"DISKQUEUE(%s) reading at %d of %s - %s\", d.name, d.readPos, d.fileName(d.readFileNum), err) d.handleReadError() continue &#125; &#125; r = d.readChan &#125; else &#123; // 如果无可读数据，那么设置 r 为nil, 防止将dataRead数据重复传入readChan中 r = nil &#125; select &#123; // the Go channel spec dictates that nil channel operations (read or write) // in a select are skipped, we set r to d.readChan only when there is data to read case r &lt;- dataRead: count++ // moveForward sets needSync flag if a file is removed // 如果读chan 被写入成功，则会修改读的偏移 d.moveForward() case &lt;-d.emptyChan: // 清空所有文件，并返回empty的结果 d.emptyResponseChan &lt;- d.deleteAllFiles() count = 0 case dataWrite := &lt;-d.writeChan: // 写msg count++ d.writeResponseChan &lt;- d.writeOne(dataWrite) case &lt;-syncTicker.C: // 到刷盘时间，则修改needSync = true if count == 0 &#123; // avoid sync when there's no activity continue &#125; d.needSync = true case &lt;-d.exitChan: goto exit &#125; &#125;exit: d.logf(INFO, \"DISKQUEUE(%s): closing ... ioLoop\", d.name) syncTicker.Stop() d.exitSyncChan &lt;- 1&#125; 需要注意的点： 数据会预先读出来，当发送到readChan 里面，才会通过moveForward 操作更改读的偏移。 queue 的Put 操作非操作，会等待写完成后，才会返回结果 Empty 操作会清空所有数据 数据会定时或者按照设定的同步频次调用FSync 刷盘 metadata 元数据metadata 文件格式文件名： &quot;name&quot; + .diskqueue.meta.dat 其中， name 是 topic, 或者topic + channel 命名. metadata 数据包含5个字段, 内容如下： 1depth\\nreadFileNum,readPos\\nwriteFileNum,writePos metadata 作用当服务关闭后，metadata 数据将保存在文件中。当服务再次启动时，将从文件中将相关数据恢复到内存中。 学习总结内存对齐与原子操作的问题 1// 64bit atomic vars need to be first for proper alignment on 32bit platforms 现象 nsq 在定义struct 的时候，很多会出现类似的注释 原因 原因在golang 源码 sync/atomic/doc.go 中 12345// On ARM, x86-32, and 32-bit MIPS,// it is the caller's responsibility to arrange for 64-bit// alignment of 64-bit words accessed atomically. The first word in a// variable or in an allocated struct, array, or slice can be relied upon to be// 64-bit aligned. 解释 在arm, 32 x86系统，和 32位 MIPS 指令集中，调用者需要保证对64位变量做原子操作时是64位内存对齐的(而不是32位对齐)。而将64位的变量放在struct, array, slice 的最前面，可以保证64位对齐 结论 有64bit 原子操作的变量，会定义在struct 的最前面，可以使变量使64位对齐，保证程序在32位系统中正确执行 对象池的使用 buffer_pool.go 文件中, 简单实现了 bytes.Buffer 的对象池，减少了gc 压力 使用场景，需要高频次做对象初始化和内存分配的情况，可使用sync.Pool 对象池减少gc 压力 如何将操作系统缓存中的数据主动刷新到硬盘中？ fsync 函数 (在write 函数之后，需要使用fsync 才能确保数据落盘) 本文代码来自于 github.com/nsqio/go-diskqueue","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"http://blog.lpflpf.cn/tags/nsq/"},{"name":"消息队列","slug":"消息队列","permalink":"http://blog.lpflpf.cn/tags/消息队列/"},{"name":"diskqueue","slug":"diskqueue","permalink":"http://blog.lpflpf.cn/tags/diskqueue/"}]},{"title":"消息队列 NSQ 源码学习笔记 (一)","slug":"nsqd-study-1","date":"2019-09-02T06:37:31.000Z","updated":"2021-01-11T06:41:06.581Z","comments":true,"path":"passages/nsqd-study-1/","link":"","permalink":"http://blog.lpflpf.cn/passages/nsqd-study-1/","excerpt":"nsqlookupd 用于Topic, Channel, Node 三类信息的一致性分发","text":"nsqlookupd 用于Topic, Channel, Node 三类信息的一致性分发 概要nsqlookup 知识点总结 功能定位 为node 节点和客户端节点提供一致的topic, channel, node 查询服务 Topic 主题， 和大部分消息队列的含义一致, 消息处理时，将相同主题的数据会归为一类消息 channel，可以理解为 topic 的一份数据拷贝，一个或者多个消费者对接一个channel。 node nsqd 启动的一个实例 一个channel会放置在某一个node 节点上，一个topic 下可以有多个channel. HTTP 接口 用于客户端服务发现以及admin 的交户使用 TCP 接口 用于 node 节点做消息广告使用 实现方式 数据包括了Topic, Channel, Node 等信息，全部存储于RegistrationDB中，RegistrationDB 采用读写锁和 map 实现，数据均存储于内存中 若存在多个nsqlookup 节点，各节点之间无耦合关系 nsqlookupd 源码阅读程序入口文件: /apps/nsqlookupd/main.go 为了时NSQ 在windows 良好运行，NSQ 使用了 github.com/judwhite/go-svc/svc 包，用于构建一个可实现windows 服务。 可以用windows 的服务管理插件直接管理。 svc 包使用时，只需要实现 github.com/judwhite/go-svc/svc.Service 的接口即可。接口如下: 123456789101112type Service interface &#123; // Init is called before the program/service is started and after it's // determined if the program is running as a Windows Service. Init(Environment) error // Start is called after Init. This method must be non-blocking. Start() error // Stop is called in response to os.Interrupt, os.Kill, or when a // Windows Service is stopped. Stop() error&#125; 因此，nsqlookup 只需要实现上述三个方法即可： Init 方法此方法仅针对windows 的服务做了处理。若为windows 服务，则修改当前目录为可执行文件的目录。 Stop 方法此方法做了nsqlookupd.Exit() 的处理。此处用到了sync.Once. 即调用的退出程序仅执行一次。 Exit 的具体内容为： 12345678910func (l *NSQLookupd) Exit() &#123; if l.tcpListener != nil &#123; l.tcpListener.Close() &#125; if l.httpListener != nil &#123; l.httpListener.Close() &#125; l.waitGroup.Wait()&#125; 关闭 TCP Listener 关闭 Http Listener 等待所有goroutine的退出 (此处用到了sync.WaitGroup，用于等待goroutine 的退出) Start 方法参数的初始化 NSQ 命令行参数的构造，采用了golang 自带的flag 包。参数保存于Options对象中，采用了先初始化，后赋值的方式，减少了不必要的条件判断。 可以采用–config 的方式，直接添加配置文件。配置文件采用toml格式. 配置的解析，采用github.com/mreiferson/go-options 实现，优先级由高到低为: 命令行参数 deprecated 的命令行参数名称 配置文件的值 (将命令行参数，连字符替换为下划线作为配置文件的key) 若参数实现了Getter，则使用Get() 方法 参数默认值 构造nsqlookupd 初始化一个RegistrationDB 建立 HttpListener 和 tcpListener (客户端请求) 启动服务，等待连接请求或者中断信号 RegistrationMap 的实现 123456789101112131415161718192021222324252627282930// RegistrationDB 使用读写锁做读写控制。type RegistrationDB struct &#123; sync.RWMutex registrationMap map[Registration]ProducerMap&#125;type Registration struct &#123; Category string // Category 有三种类型，Topic, Channel, Client. Key string SubKey string&#125;type ProducerMap map[string]*Producertype Producer struct &#123; peerInfo *PeerInfo //客户端的相关信息 tombstoned bool tombstonedAt time.Time&#125;type PeerInfo struct &#123; lastUpdate int64 // 上次更新的时间 id string // 使用ip标识的id RemoteAddress string `json:\"remote_address\"` Hostname string `json:\"hostname\"` BroadcastAddress string `json:\"broadcast_address\"` TCPPort int `json:\"tcp_port\"` HTTPPort int `json:\"http_port\"` Version string `json:\"version\"`&#125; 接口阅读TcpListener tcp 消息是 nsqd 与nsqlookupd 沟通的协议。 node 保存的是nsqd 的信息 Tcp Listener 是用来监听客户端发来的TCP 消息。建立连接后，发送4个byte标识连接的版本号。目前是v1. *”__V1”* (下划线用空格替代)消息之间按照换行符\\n分割。 目前客户端支持4类消息： PING 返回OK 若存在对端的信息，则更新client.peerInfo.lastUpdate &lt;上次更新时间&gt; IDENTIFY 用于消息的认证，将nsqd信息发送给nsqlookupd. 消息格式 IDENTIFY\\nBODYLEN(32bit)BODY 12|8bit |1 bit | 32bit | N bit ||IDENTIFY| 换行 | body 长度 | body | - BODY 为json格式 - 包含了如下字段： - 广播地址 - TCP 端口 - HTTP 端口 - 版本号 - 服务器地址 (通过连接直接获取) REGISTER 将nsqd 中注册的topic 和channel 信息发送到nsqlookupd 上，做信息共享 UNREGISTER 将nsqd 中注销的topic 和channel 信息发送到nsqlookupd 上，做信息共享 HTTPListener http 客户端的定位是用于服务的发现和admin的交互 在学习 http 请求时，可以先学习下 nsq/internal/http_api 包，此包是对golang 中http请求handler 的一次封装： 1234567891011121314151617181920212223242526272829303132type Decorator func(APIHandler) APIHandlertype APIHandler func(http.ResponseWriter, *http.Request, httprouter.Params) (interface&#123;&#125;, error)// f 是业务处理逻辑， ds 可以自定义多个包装器，用于对f 的输入和输出数据做处理。func Decorate(f APIHandler, ds ...Decorator) httprouter.Handle &#123; decorated := f for _, decorate := range ds &#123; decorated = decorate(decorated) &#125; return func(w http.ResponseWriter, req *http.Request, ps httprouter.Params) &#123; decorated(w, req, ps) &#125;&#125;// Decorator 的一个例子，做日志记录的处理func Log(logf lg.AppLogFunc) Decorator &#123; return func(f APIHandler) APIHandler &#123; return func(w http.ResponseWriter, req *http.Request, ps httprouter.Params) (interface&#123;&#125;, error) &#123; start := time.Now() response, err := f(w, req, ps) elapsed := time.Since(start) status := 200 if e, ok := err.(Err); ok &#123; status = e.Code &#125; logf(lg.INFO, \"%d %s %s (%s) %s\", status, req.Method, req.URL.RequestURI(), req.RemoteAddr, elapsed) return response, err &#125; &#125;&#125; 这种处理方式类似于大部分web框架HTTP 中间件的处理方式，是利用递归嵌套的方式，保留了处理的上下文, 实现服务切片编程。 http 服务，使用github.com/julienschmidt/httprouter包实现http 的路由功能。 目前HTTP 客户端支持以下的请求: Method Router Param Response GET /ping - “OK” GET /info - 返回版本信息 GET /debug - 返回 db 中所有信息 GET /lookup topic 返回topic 关联的所有的channels 和 nsqd 服务的信息 GET /topics - 返回所有topic 的值 GET /channels topic 返回topic 下所有的channels 信息 GET /nodes - 返回所有在线的nsqd 的node 信息, node 节点中包含了 topic 的信息，以及是否需要被删除 POST /topic/create topic 创建topic &lt;不超过64个字符长度&gt; POST /topic/delete topic 删除相应topic 的channel 和topic 信息 POST /channel/create topic, channel 创建 channel ， 若topic 不存在，创建topic POST /channel/delete topic, channel 删除 channel， 支持 * POST /topic/tombstone topic, node 将topic 下某个node 设置删除标识 tombstone, 给node 节点 一段空余时间用于删除相关topic 信息，并发送删除topic的命令 GET /debug/pprof - pprof 提供的信息 GET /debug/pprof/cmdline - pprof 提供的信息 GET /debug/pprof/symbol - pprof 提供的信息 POST /debug/pprof/symbol - pprof 提供的信息 GET /debug/pprof/profile - pprof 提供的信息 GET /debug/pprof/heap - pprof 提供的信息 GET /debug/pprof/goroutine - pprof 提供的信息 GET /debug/pprof/block - pprof 提供的信息 GET /debug/pprof/threadcreate - pprof 提供的信息 学习总结 sync.Once， sync.RWMutex 读写锁的使用 http 包装函数的简单实现 nsq/internal/http_api.Decorate github.com/judwhite/go-svc/svc 的使用 github.com/julienschmidt/httprouter 的使用 github.com/mreiferson/go-options 的使用","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"nsq","slug":"nsq","permalink":"http://blog.lpflpf.cn/tags/nsq/"},{"name":"messageQueue","slug":"messageQueue","permalink":"http://blog.lpflpf.cn/tags/messageQueue/"},{"name":"nsqlookupd","slug":"nsqlookupd","permalink":"http://blog.lpflpf.cn/tags/nsqlookupd/"}]},{"title":"为什么需要 noCopy","slug":"golang-noCopy","date":"2019-07-05T03:48:05.000Z","updated":"2021-01-11T06:41:06.576Z","comments":true,"path":"passages/golang-noCopy/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-noCopy/","excerpt":"本文对golang 的noCopy 机制做简要分析。","text":"本文对golang 的noCopy 机制做简要分析。 php 的noCopy 的实现php 中对象赋值是浅拷贝，即赋值都仅仅是copy了一次指向对象的指针而已。因此，php实现的noCopy 是针对深拷贝而言的。 深拷贝是使用clone 关键字实现的。 如果需要实现php的noCopy，只需要将php的魔术方法__clone 设置为私有即可。 代码示例 123456789101112131415161718192021222324252627&lt;?phpclass Copy&#123; public $property = 1;&#125;class noCopy &#123; public $property = 1; private function __clone()&#123;&#125;&#125;$data = new Copy();echo $data-&gt;property . \"\\n\"; // 1$ref = $data;$copy = clone $data;$data-&gt;property = 2;echo $ref-&gt;property . \"\\t\" . $copy-&gt;property . \"\\n\"; //1 ,2$data = new noCopy(); echo $data-&gt;property; // 1$ref = $data;$copy = clone $data; // Call to private noCopy::__clone() from context ...$data-&gt;property = 2;echo $ref-&gt;property; 对于一个互斥量mutex, 其本质是包含有一定状态的变量。如果一个对象持有mutex，且该对象通过mutex,操作持有的资源. 为什么需要nocopy 呢？对于一个互斥锁，实现是一个int值 和一个uint值构成的结构体。两个值标识了锁的状态。如果锁可以copy,那锁状态也将被copy(由于struct 是值拷贝的)，当锁状态再次更新后，copy后的值将不再有效。因此，对于实现了sync.Locker接口的类型来说，理论上其实例是不能再次被赋值的。 golang noCopy 的实现由于golang 中struct对象赋值是值拷贝，没有php类中的魔术方法。golang issue里面，golang sync 包中: - sync.Cond - sync.Pool - sync.WaitGroup - sync.Mutex - sync.RWMutex - …… 禁止拷贝，实现方式采用noCopy 的方式。 123456789101112131415161718192021package mainimport \"fmt\"type noCopy struct&#123;&#125;// Lock is a no-op used by -copylocks checker from `go vet`.func (*noCopy) Lock() &#123;&#125;func (*noCopy) Unlock() &#123;&#125;type S struct &#123; noCopy data int&#125;func main() &#123; var s S ss := s fmt.Println(ss)&#125; golang 没有禁止对实现sync.Locker接口的对象实例赋值进行报错，只是在使用go vet 做静态语法分析时，会提示错误。 123# command-line-arguments./nocopy.go:19: assignment copies lock value to ss: main.S./nocopy.go:20: call of fmt.Println copies lock value: main.S golang Issue","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"},{"name":"php","slug":"golang/php","permalink":"http://blog.lpflpf.cn/categories/golang/php/"}],"tags":[{"name":"noCopy","slug":"noCopy","permalink":"http://blog.lpflpf.cn/tags/noCopy/"}]},{"title":"Json Map 和 Struct 编码对比","slug":"golang-json-encoding","date":"2019-06-27T05:55:40.000Z","updated":"2021-01-11T06:41:06.550Z","comments":true,"path":"passages/golang-json-encoding/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-json-encoding/","excerpt":"本文对比试验采用官方包做json map 和struct 编码。","text":"本文对比试验采用官方包做json map 和struct 编码。 encoding/json 数据构造map 数据类型为map[string]string , key 长度为10, val 长度为100struct 定义如下： 123456789101112type Object struct &#123; Xvlbzgbaic string `json:\"xvlbzgbaic\"` Krbemfdzdc string `json:\"krbemfdzdc\"` Rzlntxyeuc string `json:\"rzlntxyeuc\"` Ctzkjkziva string `json:\"ctzkjkziva\"` Orsufumaps string `json:\"orsufumaps\"` Hyevwbtcml string `json:\"hyevwbtcml\"` Baatlyhdao string `json:\"baatlyhdao\"` Fkfohsvvxs string `json:\"fkfohsvvxs\"` Pqwarpxptp string `json:\"pqwarpxptp\"` Orvaukawww string `json:\"orvaukawww\"`&#125; 对比程序如下： 12345678910111213141516171819obj := Object&#123;&#125;json.Unmarshal([]byte(str), &amp;obj)start := time.Now()for i := 0; i &lt; 1000000; i++ &#123; json.Marshal(obj)&#125;fmt.Println(time.Since(start))maps := map[string]string&#123;&#125;json.Unmarshal([]byte(str), &amp;maps)start = time.Now()for i := 0; i &lt; 1000000; i++ &#123; json.Marshal(maps)&#125;// fmt.Println(time.Since(start)) 其中，str 为生成好的固定json数据, 我们对相同的数据做json 编码, 运行结果可以看出，时间差距大约为1倍，若将map的key 个数调整为100个 运行次数均为1000,000 次 type\\ keys 个数 10 100 1000 struct 3.84s 33.72s 5m42.34s map[string]string 7.59s 1m20.03s 17m21.47s no sorting map[string]string 6.40s 57.61s 10m4.39s 从上述对比中，得出如下结论：在大量使用json 编码时(尤其是map结构较大时)，请注意尽量直接用struct，而不是用map做编码。 原因探究 map 编码问题 struct 多次压缩时，encoding 中会缓存 name 信息, 以及对应val的类型，直接调用相应的encoder 即可;相反，map 则每次需要对key 做反射,根据类型判断获取key的值，val值也需要反射获取相应的encoder，时间浪费较多。 map 在做json 的解析的结果，会做排序操作。若修改源码，将排序操作屏蔽,key 越多，需要的时间越多。 map 编码 123456789101112131415161718192021222324252627282930// go/src/encoding/json/encode.go func (me *mapEncoder) encode(e *encodeState, v reflect.Value, opts encOpts) &#123; if v.IsNil() &#123; e.WriteString(\"null\") return &#125; e.WriteByte('&#123;') // Extract and sort the keys. keys := v.MapKeys() sv := make([]reflectWithString, len(keys)) for i, v := range keys &#123; sv[i].v = v if err := sv[i].resolve(); err != nil &#123; e.error(&amp;MarshalerError&#123;v.Type(), err&#125;) &#125; &#125; // 在输出前会做key 的排序，最后按照key 排序的结果做输出 sort.Slice(sv, func(i, j int) bool &#123; return sv[i].s &lt; sv[j].s &#125;) for i, kv := range sv &#123; if i &gt; 0 &#123; e.WriteByte(',') &#125; e.string(kv.s, opts.escapeHTML) e.WriteByte(':') me.elemEnc(e, v.MapIndex(kv.v), opts) &#125; e.WriteByte('&#125;')&#125; struct 编码 12345678910111213141516171819202122232425262728293031323334353637383940// go/src/encoding/json/encode.gotype structEncoder struct &#123; fields []field fieldEncs []encoderFunc&#125;func newStructEncoder(t reflect.Type) encoderFunc &#123; fields := cachedTypeFields(t) // 从cache 中获取fields se := &amp;structEncoder&#123; fields: fields, fieldEncs: make([]encoderFunc, len(fields)), &#125; for i, f := range fields &#123; se.fieldEncs[i] = typeEncoder(typeByIndex(t, f.index)) &#125; return se.encode&#125;func (se *structEncoder) encode(e *encodeState, v reflect.Value, opts encOpts) &#123; e.WriteByte('&#123;') first := true for i, f := range se.fields &#123; // fields 被缓存在structEncoder 结构体中 fv := fieldByIndex(v, f.index) if !fv.IsValid() || f.omitEmpty &amp;&amp; isEmptyValue(fv) &#123; continue &#125; if first &#123; first = false &#125; else &#123; e.WriteByte(',') &#125; e.string(f.name, opts.escapeHTML) e.WriteByte(':') opts.quoted = f.quoted se.fieldEncs[i](e, fv, opts) &#125; e.WriteByte('&#125;')&#125; json-iterator/go根据上述内容，对比github.com/json-iterator/go 与 encoding/json 的对比试验，也可以看出，iterator 对 map 的性能提升不是很明显(由于都需要做反射)，后续将做试验验证。 Env 机器环境： 1C1G golang 版本: go1.10.3 linux/amd64","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"}]},{"title":"Php 自增运算符","slug":"php-self-increasing","date":"2019-06-25T02:29:43.000Z","updated":"2021-01-11T06:41:06.584Z","comments":true,"path":"passages/php-self-increasing/","link":"","permalink":"http://blog.lpflpf.cn/passages/php-self-increasing/","excerpt":"php 的一些小众的用法，很多php老司机，使用时也会出问题。","text":"php 的一些小众的用法，很多php老司机，使用时也会出问题。 今天就聊一聊php的自增运算符。 bool 值 对于bool值无效。 12# php -r '$a=false; $a++; var_dump($a);';bool(false) null 值 null 值，自增后为整型1. 12# php -r '$a=null; $a++; var_dump($a);';int(1) 数字运算 正常范围的整数： 12# php -r '$a=1; $a++; var_dump($a);';int(2) 最大值的整数,整数直接变成浮点数: 1234# php -r '$a=9223372036854775807; $a++; var_dump($a);'float(9.2233720368548E+18)# php -r '$a=9223372036854775806; $a++; var_dump($a);'int(9223372036854775807) 浮点数的计算:若在精度范围内，则自增加1，若不在精度范围内，则忽略。 字符运算继承自perl的字符自增运算符。 以字符结尾 12345678910# php -r '$a=\"a\"; $a++; var_dump($a);';string(1) \"b\"# php -r '$a=\"z\"; $a++; var_dump($a);';string(2) \"aa\"# php -r '$a=\"A\"; $a++; var_dump($a);';string(1) \"B\"# php -r '$a=\"Z\"; $a++; var_dump($a);';string(2) \"AA\"# php -r '$a=\"zzz\"; $a++; var_dump($a);';string(4) \"aaaa\" 数字结尾 1234# php -r '$a=\"Z1\"; $a++; var_dump($a);';string(2) \"Z2\"# php -r '$a=\"Z9\"; $a++; var_dump($a);';string(3) \"AA0\" php 源码中，字符串自增运算符的算法说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#define LOWER_CASE 1#define UPPER_CASE 2#define NUMERIC 3static void ZEND_FASTCALL increment_string(zval *str) /* &#123;&#123;&#123; */&#123; int carry=0; // 标识是否需要进位 size_t pos=Z_STRLEN_P(str)-1; // 从字符串末端开始遍历 char *s; zend_string *t; int last=0; /* Shut up the compiler warning */ int ch; if (Z_STRLEN_P(str) == 0) &#123; zval_ptr_dtor_str(str); ZVAL_INTERNED_STR(str, ZSTR_CHAR('1')); return; &#125; if (!Z_REFCOUNTED_P(str)) &#123; Z_STR_P(str) = zend_string_init(Z_STRVAL_P(str), Z_STRLEN_P(str), 0); Z_TYPE_INFO_P(str) = IS_STRING_EX; &#125; else if (Z_REFCOUNT_P(str) &gt; 1) &#123; Z_DELREF_P(str); Z_STR_P(str) = zend_string_init(Z_STRVAL_P(str), Z_STRLEN_P(str), 0); &#125; else &#123; zend_string_forget_hash_val(Z_STR_P(str)); &#125; s = Z_STRVAL_P(str); do &#123; ch = s[pos]; if (ch &gt;= 'a' &amp;&amp; ch &lt;= 'z') &#123; if (ch == 'z') &#123; // 当末端是z 时，需要进位,修改为a s[pos] = 'a'; carry=1; &#125; else &#123; s[pos]++; carry=0; &#125; last=LOWER_CASE; &#125; else if (ch &gt;= 'A' &amp;&amp; ch &lt;= 'Z') &#123; if (ch == 'Z') &#123; // 同理，当末端是Z时，需要进位，修改为A s[pos] = 'A'; carry=1; &#125; else &#123; s[pos]++; carry=0; &#125; last=UPPER_CASE; &#125; else if (ch &gt;= '0' &amp;&amp; ch &lt;= '9') &#123; if (ch == '9') &#123; // 当末端时9时，需要进位 s[pos] = '0'; carry=1; &#125; else &#123; s[pos]++; carry=0; &#125; last = NUMERIC; &#125; else &#123; carry=0; break; &#125; if (carry == 0) &#123; // 若已经在当前位处理完成，则结束，否则一直处理到第一位 break; &#125; &#125; while (pos-- &gt; 0); if (carry) &#123; // 需要进位, 则需要多分配一个byte t = zend_string_alloc(Z_STRLEN_P(str)+1, 0); memcpy(ZSTR_VAL(t) + 1, Z_STRVAL_P(str), Z_STRLEN_P(str)); ZSTR_VAL(t)[Z_STRLEN_P(str) + 1] = '\\0'; switch (last) &#123; //考虑上一位last 标识的是那种类型,赋值不同数据 case NUMERIC: ZSTR_VAL(t)[0] = '1'; break; case UPPER_CASE: ZSTR_VAL(t)[0] = 'A'; break; case LOWER_CASE: ZSTR_VAL(t)[0] = 'a'; break; &#125; zend_string_free(Z_STR_P(str)); ZVAL_NEW_STR(str, t); &#125;&#125; php 自增运算符 Doc 路漫漫其修远兮，吾将上下而求索。","categories":[{"name":"php","slug":"php","permalink":"http://blog.lpflpf.cn/categories/php/"}],"tags":[{"name":"PHP 开发","slug":"PHP-开发","permalink":"http://blog.lpflpf.cn/tags/PHP-开发/"},{"name":"PHP Source Analysis","slug":"PHP-Source-Analysis","permalink":"http://blog.lpflpf.cn/tags/PHP-Source-Analysis/"}]},{"title":"Go Mod","slug":"golang-mod","date":"2019-06-20T10:21:43.000Z","updated":"2021-01-11T06:41:06.576Z","comments":true,"path":"passages/golang-mod/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-mod/","excerpt":"go mod 工具简单入门介绍。","text":"go mod 工具简单入门介绍。 简介目前 Golang 项目的包管理方式如下： 裸奔模式 配置项目目录为GOPATH路径 将依赖项目放在 src/… 目录下 dep 工具 (类似的有godep, govendor 工具) dep 工具为官方工具 将在目录下创建vendor 目录，依赖下载至vendor 目录下 mod 工具 Go 1.11 版本以后自带子命令 去掉GOPATH依赖 是否需要手动开启GO Module 模式 GO111MODULE 的取值为 off, on, or auto. off: GOPATH mode，查找vendor和GOPATH目录 on：module-aware mode，使用 go module，忽略GOPATH目录 auto：如果当前目录不在$GOPATH 并且 当前目录（或者父目录）下有go.mod文件，则使用 GO111MODULE， 否则仍旧使用 GOPATH mode。 如何使用 step 1: 初始化mod, 将go.mod 文件，保存当前目录的pkg name 1# go mod init current_pkg_name step 2: 将项目依赖的多有pkg下载；若GOPATH为空，则放置~/go 目录下，否则放置到GOPATH 目录下 1# go mod tidy step 3: 1# go build pkg_name 其他自命令 go mod [download | edit | graph | vendor | verify | why]","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"go build","slug":"go-build","permalink":"http://blog.lpflpf.cn/tags/go-build/"}]},{"title":"Go Binary Compress","slug":"golang-binary-compress","date":"2019-06-20T09:59:20.000Z","updated":"2021-01-11T06:41:06.546Z","comments":true,"path":"passages/golang-binary-compress/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-binary-compress/","excerpt":"golang 压缩的方式: 1. build 添加去除调试标识; 2. 使用upx 工具。","text":"golang 压缩的方式: 1. build 添加去除调试标识; 2. 使用upx 工具。 go 二进制文件压缩 由于git中保存二进制文件，可能会使项目过大，可以将二进制文件压缩，使程序更加便携。 去掉 gdb 调试信息和符号表 1# go build -ldflags \" -s -w\" s 去掉符号表信息 w 去掉调试信息 使用upx 工具压缩 可压缩 50% - 70% 大小 原理： 包含自解压程序，类似exe 文件 编译机器安装upx 命令;部署环境不需要安装 命令如下： （可以添加参数是文件压缩更小） 1# upx binary_filename 工具连接 upx","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"go build","slug":"go-build","permalink":"http://blog.lpflpf.cn/tags/go-build/"}]},{"title":"Golang 二进制文件中添加编译信息","slug":"golang-add-build-info-in-binary","date":"2019-06-20T08:34:38.000Z","updated":"2021-01-11T06:41:06.545Z","comments":true,"path":"passages/golang-add-build-info-in-binary/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-add-build-info-in-binary/","excerpt":"在编译二进制程序时，动态赋值程序的某些值，使程序包含了可靠的编译信息。","text":"在编译二进制程序时，动态赋值程序的某些值，使程序包含了可靠的编译信息。 Go 二进制中包含编译信息 如果服务上线后，不知道此二进制文件是哪个版本产出的二进制，那么本文可以帮助你实现相关的功能。 在二进制代码发布时，传入必要的版本信息，以便日后可查看相关信息。 可用于 git-runner 中，直接获取版本信息、分支信息等，填充相应参数。 效果展示binary 是我们例子中的二进制文件 123456# ./main -vCommit ID : 123Build Name: version testBuild Time: 20190620Build Vers: 1.1Golang Vers: go version go1.10.3 linux/amd64 实现方法 在golang 解析参数部分添加如下内容: 123456789101112131415package mainimport \"github.com/lpflpf/version\"import \"flag\"func main() &#123; var showVer bool // 为了举例，所以仅使用了-v 选项 flag.BoolVar(&amp;showVer, \"v\", false, \"show build version\") flag.Parse() if showVer &#123; version.Show() &#125;&#125; version 包如下： 123456789101112131415161718192021222324package versionimport ( \"fmt\" \"os\")// 连接过程中修改如下5个参数，可以自行添加使用var ( BuildVersion string BuildTime string BuildName string CommitID string GoVersion string)func Show() &#123; fmt.Printf(\"Commit ID : %s\\n\", CommitID) fmt.Printf(\"Build Name: %s\\n\", BuildName) fmt.Printf(\"Build Time: %s\\n\", BuildTime) fmt.Printf(\"Build Vers: %s\\n\", BuildVersion) fmt.Printf(\"Golang Vers: %s\\n\", GoVersion) os.Exit(0)&#125; 编译程序，编译脚本如下： 123456789101112BUILD_TIME=`date +%Y%m%d`BUILD_VERSION=1.1COMMIT_ID=123GO_VERSION=`go version`BUILD_NAME=\"version test\"VERSION_PKG='github.com/lpflpf/version'LD_FLAGS=\"-s -w -X '$VERSION_PKG.BuildTime=$BUILD_TIME' \\ -X '$VERSION_PKG.BuildVersion=$BUILD_VERSION' \\ -X '$VERSION_PKG.BuildName=$BUILD_NAME' \\ -X '$VERSION_PKG.CommitID=$COMMIT_ID' \\ -X '$VERSION_PKG.GoVersion=$GO_VERSION'\"go build -ldflags \"$LD_FLAGS\" main.go 执行脚本， 则看到本文开头说明的二进制版本信息 原理分析在golang 进行连接包时，允许将字符串传入包的变量中。因此，在编译时，通过ld选项添加相应变量，实现了二进制中保存编译信息的功能。 参考文档 golang 连接说明 version 包","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"go build","slug":"go-build","permalink":"http://blog.lpflpf.cn/tags/go-build/"}]},{"title":"由Ctype_digit 引起的一个问题","slug":"php-ctype","date":"2018-07-25T16:00:00.000Z","updated":"2021-01-11T06:41:06.584Z","comments":true,"path":"passages/php-ctype/","link":"","permalink":"http://blog.lpflpf.cn/passages/php-ctype/","excerpt":"ctype_digit 问题说明。","text":"ctype_digit 问题说明。 问题说明最近发现，代码中有先人使用ctype_digit函数判断缓存时间，若返回为true，则设置redis Key的生命周期，否则不设置超时时间。 为了使数据能快速更新，于是有人修改了缓存时间从300 -&gt; 50,导致了数据不再更新。原因就在于ctype_digit 函数的使用方式有问题。 ctype 的使用ctype 用于检测字符串的相关类型 ctype_alnum 是否为数字或者字母, 是返回TRUE，否则返回false ctype_alpha 做纯字符检测 ctype_cntrl 做控制字符检测, 换行、缩进、空格等 ctype_digit 做纯数字检测 ctype_graph 可打印字符串检测，空格除外 ctype_lower 做小写字符检测 ctype_print 做可打印字符检测 ctype_punct 检测可打印的字符是不是不包含空白、数字和字母 ctype_space 做空白字符检测 ctype_upper 做大写字母检测 ctype_xdigit 检测字符串是否只包含十六进制字符 此类函数若给出的是-128到255之间（含）的整数，会被解释为该值对应的ASCII（负值将加上256 以支持扩展ASCII字符）。其他将呗认为是字符串","categories":[{"name":"php","slug":"php","permalink":"http://blog.lpflpf.cn/categories/php/"}],"tags":[{"name":"PHP 开发","slug":"PHP-开发","permalink":"http://blog.lpflpf.cn/tags/PHP-开发/"},{"name":"ctype函数簇","slug":"ctype函数簇","permalink":"http://blog.lpflpf.cn/tags/ctype函数簇/"}]},{"title":"分布式发号器","slug":"distribuation-id-generator","date":"2018-06-17T16:00:00.000Z","updated":"2021-01-11T06:41:06.544Z","comments":true,"path":"passages/distribuation-id-generator/","link":"","permalink":"http://blog.lpflpf.cn/passages/distribuation-id-generator/","excerpt":"一些分布式全局唯一id 方案简介。","text":"一些分布式全局唯一id 方案简介。 基本需求 全局唯一性 粗略有序 在秒级别有序、在毫秒级别有序 可反解 可以通过id 获取相关信息 （例如时间信息、工作组、机器信息等) 可制造 可以进行手工处理 高可用 一台发生器出问题，可以使用别的替代，或者请求转移 高性能 性能要达到10000/s 单台机器 可伸缩性 业务是会增长的，需要有良好的可扩展性 可以选择的方案 UUIDuuid 可以保证 ID 的唯一性。但是，有如下缺点 时间内容缺失 长度比较长, 数据库占用空间较大 不具有有序性, 数据库插入不友好 数据库方案通过设置步长、数据库自增，保证唯一性 对数据库有一定依赖， 有性能问题 步长固定，水平扩展比较困难 不同数据库，管理困难 snowflake 项目 Scala 语言实现 需要二次开发 golang 实现","categories":[{"name":"分布式","slug":"分布式","permalink":"http://blog.lpflpf.cn/categories/分布式/"}],"tags":[{"name":"id generator","slug":"id-generator","permalink":"http://blog.lpflpf.cn/tags/id-generator/"},{"name":"distribuation","slug":"distribuation","permalink":"http://blog.lpflpf.cn/tags/distribuation/"}],"author":"李朋飞"},{"title":"Go Base Syntax","slug":"golang-basic-structure","date":"2018-06-14T16:00:00.000Z","updated":"2021-01-11T06:41:06.546Z","comments":true,"path":"passages/golang-basic-structure/","link":"","permalink":"http://blog.lpflpf.cn/passages/golang-basic-structure/","excerpt":"","text":"go程序go程序说明 1234567891011121314151617181920212223242526272829303132package main // 程序包名, 与 php namespace 类似； 和java 相同 // import 可以通过 &#123; &#125; 导入多个包。 中间加入 \".\", 可以在引用函数时，不带包名 import . \"fmt\" // 引入包名重命名 (.) 可以认为是类似的引用. import myio \"io\" // 定义常量 const PI = 3.14 // 定义一般变量 var name = \"gopher\" // 申明类型newType 为 int； 类似于C 中typedef type newType int // 申明类型gopher 为 一个空结构体 type gopher struct&#123;&#125; // 申明golang接口 type golang interface&#123;&#125; // 函数以func 开头，类似于PHP 中function func main() &#123; Println (\"Hello World\") &#125; // 大写开头的函数，可以在包外引用 func SayHello()&#123; &#125;","categories":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://blog.lpflpf.cn/tags/golang/"},{"name":"base structure","slug":"base-structure","permalink":"http://blog.lpflpf.cn/tags/base-structure/"}],"author":"李朋飞"},{"title":"PHP Cycle Life","slug":"php-cycle-life","date":"2018-06-05T16:00:00.000Z","updated":"2021-01-11T06:41:06.584Z","comments":true,"path":"passages/php-cycle-life/","link":"","permalink":"http://blog.lpflpf.cn/passages/php-cycle-life/","excerpt":"","text":"php 运行阶段 开始阶段 模块初始化 MINIT (module init) 这个阶段，将对每个扩展的PHP_MINIT_FUNCTION函数执行。一般执行如下操作： INI 配置文件的注册 REGISTER_INI_ENTRIES 定义该扩展实现的类, Interface等 定义的const变量 模块激活 RINIT (Request init) 每个请求进入时，将调用每个扩展的PHP_RINIT_FUNCTION。一般有如下需求会调用该方法: 重置之前的请求, 例如 spl 扩展。 通过请求数据，初始化模块的参数。 例如 mbstring 扩展。 运行阶段 进入PHP文件执行 结束阶段 停用模块 RSHUTDOWN ( Request shutdown)与 RINIT 相对应 关闭模块 MSHUTDOWN (module shutdown)与 MINIT 相对应 不同的PHP运行环境，PHP的生命周期不同php 命令行模式 php Multi Process 模式 PHP Multi Threaded 模式 其他一些函数 PHP_GINIT_FUNCTION 全局变量初始化 PHP_GSHUTDOWN_FUNCTION PHP_MINFO_FUNCTION 设置INI 文件中模块的信息, phpinfo 时打印的数据 CG Complier Global EG Executor Global PG PHP Core Global SG SAPI Global","categories":[{"name":"php","slug":"php","permalink":"http://blog.lpflpf.cn/categories/php/"}],"tags":[{"name":"cycle life","slug":"cycle-life","permalink":"http://blog.lpflpf.cn/tags/cycle-life/"},{"name":"Request 处理","slug":"Request-处理","permalink":"http://blog.lpflpf.cn/tags/Request-处理/"},{"name":"PHP Source Analysis","slug":"PHP-Source-Analysis","permalink":"http://blog.lpflpf.cn/tags/PHP-Source-Analysis/"}],"author":"李朋飞"},{"title":"PHP Array_merge 和 Array + Array 的区别","slug":"php-array_merge","date":"2018-06-04T16:00:00.000Z","updated":"2021-01-11T06:41:06.583Z","comments":true,"path":"passages/php-array_merge/","link":"","permalink":"http://blog.lpflpf.cn/passages/php-array_merge/","excerpt":"","text":"array_merge官网说明 将一个或多个数组的单元合并起来，一个数组中的值附加在前一个数组的后面。返回作为结果的数组。如果输入的数组中有相同的字符串键名，则该键名后面的值将覆盖前一个值。然而，如果数组包含数字键名，后面的值将不会覆盖原来的值，而是附加到后面。如果只给了一个数组并且该数组是数字索引的，则键名会以连续方式重新索引。 array + array官网说明 + 运算符把右边的数组元素附加到左边的数组后面，两个数组中都有的键名，则只用左边数组中的，右边的被忽略。 例子如下example 1: 123456789$addend = [ 'a' =&gt; 'apple', 'b' =&gt; 'banana', 'c' =&gt; 'cherry' ];$summand = [ 'a' =&gt; 'apricot', 'b' =&gt; 'banana', 'd' =&gt; 'date' ];// 非数字键key 相同不做merge，只是追加print_r(array_merge($addend, $summand));// key 相同，做覆盖print_r($addend + $summand); result: 1234567891011121314Array( [a] =&gt; apricot [b] =&gt; banana [c] =&gt; cherry [d] =&gt; date)Array( [a] =&gt; apple [b] =&gt; banana [c] =&gt; cherry [d] =&gt; date) example 2: 12345$addend = [ 0 =&gt; 'apple', 1 =&gt; 'banana', 3 =&gt; 'cherry' ];$summand = [ 0 =&gt; 'apricot', 1 =&gt; 'banana', 2 =&gt; 'date' ];print_r(array_merge($addend, $summand));print_r($addend + $summand); result: 12345678910111213141516Array( [0] =&gt; apple [1] =&gt; banana [2] =&gt; cherry [3] =&gt; apricot [4] =&gt; banana [5] =&gt; date)Array( [0] =&gt; apple [1] =&gt; banana [3] =&gt; cherry [2] =&gt; date)","categories":[{"name":"php","slug":"php","permalink":"http://blog.lpflpf.cn/categories/php/"}],"tags":[{"name":"array_merge","slug":"array-merge","permalink":"http://blog.lpflpf.cn/tags/array-merge/"}],"author":"李朋飞"},{"title":"PHP Singleton","slug":"php-singleton","date":"2018-05-29T16:00:00.000Z","updated":"2021-01-11T06:41:06.584Z","comments":true,"path":"passages/php-singleton/","link":"","permalink":"http://blog.lpflpf.cn/passages/php-singleton/","excerpt":"","text":"单例 单例模式，希望在程序上下文中，仅对对象做一次实例化。 问题 clone 会引起出现单例对象有多个实例 避免方式 clone 由于clone时，会调用对象的__clone magic method. 因此，可以将__clone 设置为私有，使clone失效。 单例模式 1234567891011121314class Singleton &#123; private static $instance = NULL; private function __construct() &#123; &#125; private function __clone() &#123; &#125; public static function getInstance() &#123; if (NULL === self::$instance) &#123; self::$instance = new self(); &#125; return self::$instance; &#125;&#125; unserialize将Singleton 将对象序列化，再进行反序列化。可以构造新的单例对象。这种情况下，鸟哥给出的解决方案是：(使用__wakeup() 方法)[http://www.laruence.com/2011/03/18/1909.html]。但是，通过对比发现，这个代码其实会有问题： 123456789101112131415161718192021222324252627&lt;?phpclass Singleton &#123; private static $instance = NULL; private $data = ''; private function __construct() &#123; &#125; private function __clone() &#123; &#125; public function __wakeup() &#123; self::$instance = $this; &#125; public static function getInstance() &#123; if (NULL === self::$instance) &#123; self::$instance = new self(); &#125; return self::$instance; &#125;&#125;$a = Singleton::getInstance();$b = unserialize(serialize($a));// falsevar_dump($b === $a); 因为unserialize 其实会实例化一个单例对象，和原来实例化的单例对象不是一个对象。因此，会引起dump 不一致的情况。这个暂时无法避免。","categories":[{"name":"php","slug":"php","permalink":"http://blog.lpflpf.cn/categories/php/"}],"tags":[{"name":"singleton","slug":"singleton","permalink":"http://blog.lpflpf.cn/tags/singleton/"},{"name":"serialize / unserialize","slug":"serialize-unserialize","permalink":"http://blog.lpflpf.cn/tags/serialize-unserialize/"},{"name":"clone","slug":"clone","permalink":"http://blog.lpflpf.cn/tags/clone/"}],"author":"李朋飞"},{"title":"Mysql Hint 学习","slug":"mysql-hint","date":"2018-05-25T16:00:00.000Z","updated":"2021-01-11T06:41:06.580Z","comments":true,"path":"passages/mysql-hint/","link":"","permalink":"http://blog.lpflpf.cn/passages/mysql-hint/","excerpt":"","text":"Index Hint 官网说明 语法说明 123456789101112131415161718192021tbl_name [[AS] alias] [index_hint_list]index_hint_list: index_hint [index_hint] ... index_hint: USE &#123; INDEX|KEY&#125; [FOR &#123; JOIN|ORDER BY|GROUP BY&#125;] ([index_list]) | IGNORE &#123; INDEX|KEY&#125; [FOR &#123; JOIN|ORDER BY|GROUP BY&#125;] (index_list) | FORCE &#123; INDEX|KEY&#125; [FOR &#123; JOIN|ORDER BY|GROUP BY&#125;] (index_list) index_list: index_name [, index_name] ... 在表明后 使用 (USE | IGNORE | FORCE ) (INDEX|KEY)`[FOR (JOIN | ORDER BY | GROUP BY)] index_name … 使用的是索引名称， 而非列名称 对于自然语言模式下的全文搜索, index hints默认不起作用，单索引仍有效。 对于boolean模式下的全文搜索，index hints 对于 For Order | For Group 模式，默认屏蔽。对于 for join 或者没有for的情况生效。 Query Cache SELECT Options 官网说明 语法说明： 12SELECT SQL_CACHE id, name FROM customer;SELECT SQL_NO_CACHE id, name FROM customer; 指定SQL 是否需要在缓存中查找结果。对于一天执行1，2次的SQL，可以使用 SQL_NO_CACHE 使其不从缓存查找 LOW_PRIORITY | HIGH_PRIORITY 官方说明 官方说明 在INSERT | SELECT 语句中说明。 在INSERT 中使用LOW_PRIORITY, 则该语句将在没有客户端读该表的时候执行。（在读频繁的表中，这个会引发饥饿现象） INSERT HIGH_PRIORITY(默认情况),除非使用了该配置:–low-priority-updates SELECT LOW_PRIORITY (默认情况), SELECT HIGH_PRIORITY. 只有查询一次，并且需要非常快的情况下才会使用该HINT. 这种情况会所表，知道查询结束. INSERT DELAYED 官方说明 在特定的存储引擎中可以使用。(MyISAM, MEMORY, ARCHIVE, and BLACKHOLE tables), Innodb 并不支持. 延时插入，异步返回。将插入的任务加入到队列中。","categories":[{"name":"mysql","slug":"mysql","permalink":"http://blog.lpflpf.cn/categories/mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://blog.lpflpf.cn/tags/Mysql/"},{"name":"Hint","slug":"Hint","permalink":"http://blog.lpflpf.cn/tags/Hint/"}],"author":"李朋飞"}]}